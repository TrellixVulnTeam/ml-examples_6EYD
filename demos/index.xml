<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Demos on Machine Learning Examples</title>
    <link>https://minh84.github.io/ml-examples/demos/index.xml</link>
    <description>Recent content in Demos on Machine Learning Examples</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Mar 2017 22:18:54 +0000</lastBuildDate>
    <atom:link href="/ml-examples/demos/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title></title>
      <link>https://minh84.github.io/ml-examples/demos/learn_tf/02-linear-classifier-softmax/</link>
      <pubDate>Fri, 10 Mar 2017 22:18:54 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/demos/learn_tf/02-linear-classifier-softmax/</guid>
      <description>Linear classifier with softmax&amp;#182;We continue from the previous notebook linear classifier with svm, in this note book we consider a different loss function the Softmax function $$ L(y, s(x)) = -\log\left(\frac{e^{s_y(x)}}{\sum_je^{s_j(x)}}\right) $$ we recall some notation $\newcommand{\real}{\mathbb{R}} \newcommand{\vi}[1]{#1^{(i)}} \newcommand{\vik}[1]{#1^{(i)}_k} \newcommand{\vij}[2]{#1^{(i)}_{#2}} $
 $x\in\real^{D\times 1}$ is input features  $s(x)$ is linear score of given by $$ s(x) = W^T\times x $$ with $W\in \real^{C\times D}$ with $C$ is number of classes.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://minh84.github.io/ml-examples/demos/learn_tf/01-linear-classifier-svm/</link>
      <pubDate>Thu, 09 Mar 2017 16:00:03 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/demos/learn_tf/01-linear-classifier-svm/</guid>
      <description>Classification Problem&amp;#182;In this notebook we work on classification of CIFAR-10 dataset. We define some notation to use later on
 $x^{(i)}$ are input-images each has shape 32x32x3 (RGB) $y^{(i)}$ are labels of above images and can take values $0,\ldots,9$ corresponding to 10 classes  To solve this classification, we try to find a function $h$ that maps from image $x$ to scores i.e $$ h: x \mapsto \left(\begin{array}{c}s_0(x)\\ \ldots\\ s_9(x)\end{array}\right) $$ where $s_i(x)$ is score of $x$ in $i-$th class.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://minh84.github.io/ml-examples/demos/linear_models/01-Linear-Regression/</link>
      <pubDate>Sun, 05 Mar 2017 21:36:46 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/demos/linear_models/01-Linear-Regression/</guid>
      <description>Linear basis function regression&amp;#182;In this note, we consider the simplest form of linear regression models where $h(\pmb{\mathrm{x}})$ is a linear function of the input variables $$ y(\pmb{\mathrm{x}},\pmb{\mathrm{w}}) = w_0 + w_1 x_1 + \ldots + w_D x_D $$ To make our problem more concrete, we consider the boston housing data. Go through this exercise, we will learn the following
 get the raw data and preprocess it to a convenient form visualize the data via plot formulate the maximum likelihood =&amp;gt; least squares problem solve least square problem with Gradient Descend/Stochastic Gradient Descend try it in TensorFlow  Let&#39;s get started</description>
    </item>
    
  </channel>
</rss>
