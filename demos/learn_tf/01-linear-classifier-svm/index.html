<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="Hugo 0.20-DEV" />
    <link rel="shortcut icon" href="/ml-examples/images/favicon.ico">
    <link href="https://minh84.github.io/ml-examples/index.xml" rel="alternate" type="application/rss+xml" title="Machine Learning Examples" />
    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css">
    
    <script src="https://apis.google.com/js/platform.js" async defer>{lang: 'ja'}</script>
    
    <link rel="stylesheet" href="https://yandex.st/highlightjs/8.0/styles/default.min.css">
    <script src="https://yandex.st/highlightjs/8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\[','\]']],
        processEscapes: true,
        processEnvironments: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
        TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
      }
    });
    </script>
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <link rel="stylesheet" type="text/css" href="/ml-examples/css/style.css">
    <link rel="stylesheet" type="text/css" href="/ml-examples/css/jupyter.css">
    <title> | Machine Learning Examples</title>
  </head>
  <body>
    <div id="wrap">
      
      <header class="site-header">
        <div class="site-header-left">
          <a class="site-header-title" href="https://minh84.github.io/ml-examples/">Machine Learning Examples</a>
        </div>
      </header>
      <div class="container">
        <div id="main">

<div class="container">
  <header>
    <div class="article-header">
      <h1></h1>
      <div class="article-meta">
        <span class="posttime">2017/03/09</span>
        

      </div>
    </div>
    

  </header>
  <div class="content">
    <div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Classification-Problem">Classification Problem<a class="anchor-link" href="#Classification-Problem">&#182;</a></h1><p>In this notebook we work on classification of <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>. We define some notation to use later on</p>
<ul>
<li>$x^{(i)}$ are input-images each has shape 32x32x3 (RGB)</li>
<li>$y^{(i)}$ are labels of above images and can take values $0,\ldots,9$ corresponding to 10 classes</li>
</ul>
<p>To solve this classification, we try to find a function $h$ that maps from image $x$ to scores i.e
$$
h: x \mapsto \left(\begin{array}{c}s_0(x)\\ \ldots\\ s_9(x)\end{array}\right)
$$
where $s_i(x)$ is score of $x$ in $i-$th class. Then we predict the label of $x$ as
$$
x\text{'s label}:=\mathrm{arg}\max_{i}s_i(x)
$$</p>
<p>The notebook is organized as follows</p>
<ul>
<li>Load CIFAR-10 dataset </li>
<li>Introduce Linear classifier</li>
<li>Pre-processing data for Linear classifier</li>
<li>Multiclass SVM loss</li>
<li>Optimize SVM loss with SGD</li>
</ul>
<p>The goal of this notebook is to learn how to implement SVM loss function in <a href="http://www.numpy.org/">numpy</a> and <a href="https://www.tensorflow.org/">TensorFlows</a>.</p>
<p>Let's start by loading some python modules</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span> <span class="c1"># set default size of plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.interpolation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.cmap&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span>

<span class="c1"># append common path</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">COMMON_PATH</span> <span class="o">=</span> <span class="s1">&#39;../common&#39;</span>
<span class="k">if</span> <span class="n">COMMON_PATH</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">COMMON_PATH</span><span class="p">)</span>
    
<span class="c1"># for auto-reloading extenrnal modules</span>
<span class="c1"># see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython</span>
<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Get-the-data">Get the data<a class="anchor-link" href="#Get-the-data">&#182;</a></h1><p>We need to download dataset from the internet and untar it, we use some helper functions in <em>common</em> directory</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">data_utils</span> <span class="k">import</span> <span class="n">download_file_to_cwd</span><span class="p">,</span> <span class="n">untar_to_cwd</span>

<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz&#39;</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;cifar-10-python.tar.gz&#39;</span>

<span class="c1"># download data to current directory</span>
<span class="n">download_file_to_cwd</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>

<span class="c1"># untar the data</span>
<span class="n">cifar10_dir</span> <span class="o">=</span> <span class="s1">&#39;cifar-10-batches-py&#39;</span>
<span class="n">untar_to_cwd</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">cifar10_dir</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz is downloaded to ./cifar-10-python.tar.gz
./cifar-10-python.tar.gz is untar to ./cifar-10-batches-py
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># load data to memory</span>
<span class="kn">from</span> <span class="nn">cifar10_input</span> <span class="k">import</span> <span class="n">load_CIFAR10</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">load_CIFAR10</span><span class="p">(</span><span class="n">cifar10_dir</span><span class="p">)</span>

<span class="c1"># let&#39;s divide train into training set (49000) + validation set (1000)</span>
<span class="n">num_training</span> <span class="o">=</span> <span class="mi">49000</span>

<span class="n">mask</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_training</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

<span class="n">mask</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_training</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Train inputs shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Train labels shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Validation inputs shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Validation labels shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Test inputs shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Test labels shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train inputs shape: (49000, 32, 32, 3)
Train labels shape: (49000,)
Validation inputs shape: (1000, 32, 32, 3)
Validation labels shape: (1000,)
Test inputs shape: (10000, 32, 32, 3)
Test labels shape: (10000,)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Linear-classifier">Linear classifier<a class="anchor-link" href="#Linear-classifier">&#182;</a></h1><p>We consider $h(x)$ as linear function of $x$
$$
h(x) = f(x,W) = w_0 + \sum_{i,j}w_{i,j}x_{i,j}
$$</p>
<p>Our goal is to find $W$ to minimize some loss function 
$$
L(y, f(x, W))
$$
where $y$ is the label of the image $x$.</p>
<h2 id="Pre-processing-training-data">Pre-processing training data<a class="anchor-link" href="#Pre-processing-training-data">&#182;</a></h2><p>To simplify our computation, we do the following preprocesing steps</p>
<ul>
<li>flatten our input image 32x32x3 =&gt; 3072 </li>
<li>normalize training data (it's always good idea to have normalized input with mean = 0.0), </li>
<li>append one (for bias term) to the end of each input.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># flatten input data</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># normalize training data</span>
<span class="n">mean_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">-=</span> <span class="n">mean_images</span>
<span class="n">X_val</span> <span class="o">-=</span> <span class="n">mean_images</span>
<span class="n">X_test</span> <span class="o">-=</span> <span class="n">mean_images</span>

<span class="c1"># append one for bias term</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_val</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Train inputs shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Validation inputs shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Test inputs shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train inputs shape: (49000, 3073)
Validation inputs shape: (1000, 3073)
Test inputs shape: (10000, 3073)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the above code, $x$ is flattened $x\in\mathbb{R}^D$ (D=3073 in our example) and we consider the input data in following form
$$
X = \left(\begin{array}{c}
(x^{(1)})^T\\
\vdots\\
(x^{(N)})^T
\end{array}\right)\in \mathbb{R}^{N\times D}
$$
so the weight matrix $W$ has shape $D\times C$ with $C$ is number of classes (in our example $C=10$) and our linear function is given by
$$
f(X,W) = X\times W
$$
where each row is score for each input.</p>
<h2 id="Multiclass-SVM-loss">Multiclass SVM loss<a class="anchor-link" href="#Multiclass-SVM-loss">&#182;</a></h2><p>Given $(x,y)$ are the image and the label,respectively, and the scores $s(x)=f(x,W)$, the SVM loss for one sample $(x,y)$ has the form
$$
L(y, s(x)) = \sum_{i\neq y}\max(0, s_i - s_y + 1)
$$
Intutively, if $s_y >= s_i + 1$ for all $i\neq y$ we then have SVM loss $=0$, so the SVM is try to maximize the chance of $s_y$ is the maximum of $s_i$. Since we predict the label of $x$ as $\mathrm{arg}\max_{i}s_i$, minimize SVM loss might help to maximize correct prediction.</p>
<p>The SVM loss for $N$ samples is the mean of SVM loss at each one sample plus a regulized form:
$$
\mathrm{loss}(W) = \frac{1}{N} \sum_{i=1}^NL\left(y^{(i)}, s(x^{(i)})\right) + \frac{1}{2}\lambda ||W||^2
$$
where $||W||^2 = \sum_{i,j}W_{i,j}^2$ is added to reduce overfitting</p>
<p>In the following we implement the SVM loss</p>
<ul>
<li>using <strong>numpy</strong> only</li>
<li>using <strong>TensorFlow</strong></li>
</ul>
<p>Note that we need to compute not only the loss function but also the gradient with respect to $W$ so that we can use with SGD to minimize the loss.</p>
<p>Let's compute the loss, we define $M = \left(M_{ij}\right)\in \mathbb{R}^{N\times D}$ where
$$
 M_{ij} = \left\{ \begin{array}{l}
 s_j\left(x^{(i)}\right) - s_{y^{(i)}}\left(x^{(i)}\right) + 1 \text{ for } j\neq y^{(i)}\\
 0 \text{ otherwise}
 \end{array}
 \right.
$$
Then the SVM-loss is given as
$$
 \sum_{i=1}^NL\left(y^{(i)}, s(x^{(i)})\right) = \sum_{ij}M_{ij}\times 1_{M_{ij} > 0}
$$</p>
<p>Let's derive the gradient, we have
$$
 \frac{\partial}{\partial W_{uv}} \max\left(0, s_j\left(x^{(i)}\right) - s_{y^{(i)}}\left(x^{(i)}\right) + 1\right)  = 1_{M_{ij} > 0}\times \left(x^{(i)}_{u}\times 1_{j=v} -x^{(i)}_{u}\times 1_{j=y^{(i)}}\right)
$$
so the we have
$$
\nabla_W\max\left(0, s_j\left(x^{(i)}\right) - s_{y^{(i)}}\left(x^{(i)}\right) + 1\right) =  1_{M_{ij} > 0} \times \left(\begin{array}{ccccccccccc}
    0 & \cdots & 0 &x^{(i)}_1 & 0 & \cdots & 0 & -x^{(i)}_1 & 0 & \cdots & 0\\
    0 & \cdots & 0 &x^{(i)}_2 & 0 & \cdots & 0 & -x^{(i)}_2 & 0 & \cdots & 0\\
    \vdots & \vdots & \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots\\
    0 & \cdots & 0 & \smash[b]{\underbrace{x^{(i)}_D}_{j-th}} & 0 & \cdots & 0 & \smash[b]{\underbrace{-x^{(i)}_D}_{y^{(i)}-th}} & 0 & \cdots & 0
\end{array}\right)
$$</p>
<p>Denote $P=(P_{ij})\in \mathbb{R}^{N\times D}$ is defined as
$$
P_{ij} = \left\{\begin{array}{ll}
1_{M_{ij} > 0} & \text{if } j\neq y^{(i)}\\
\sum_{j\neq y^{(i)}} 1_{M_{ij} > 0} & \text{otherwise}
\end{array}\right.
$$
From above equation, we can show that
$$
\nabla_W L\left(y^{(i)}, s(x^{(i)})\right) = P[i] \times x^{(i)}
$$
where $P[i]$ is i-th row of $P$, so
$$
\nabla_W\sum_{i=1}^NL\left(y^{(i)}, s(x^{(i)})\right) = \sum_{i=1}^N P[i] \times x^{(i)} = X^T\times P
$$
</li>
</ul>
<h3 id="Implentation-SVM-with-Numpy">Implentation SVM with Numpy<a class="anchor-link" href="#Implentation-SVM-with-Numpy">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">svm_np</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    we implement svm loss defined as above</span>
<span class="sd">        X: data inputs has shape (N, D)</span>
<span class="sd">        y: labels has shape (N,)</span>
<span class="sd">        W: weights has shape (D, num_classes)</span>
<span class="sd">        reg: positive real number</span>
<span class="sd">    the function return</span>
<span class="sd">        loss: svm loss</span>
<span class="sd">        dW: gradient of loss regarding to W</span>
<span class="sd">    &#39;&#39;&#39;</span>    
    <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>    <span class="c1"># N x num_classes</span>
    
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">scores</span>
    
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">M</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">scores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>    
    <span class="n">M</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="n">pos_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">pos_scores</span><span class="p">[</span><span class="n">M</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    
    <span class="c1"># svm loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">pos_scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
    
    <span class="c1"># adding reg</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
    
    <span class="c1"># implement the grad</span>
    <span class="n">sum_pos_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pos_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># sum 1_{M_{ij} &gt; 0}</span>
    <span class="n">pos_scores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_pos_scores</span>
    
    <span class="c1"># grad SVM with respect to W</span>
    <span class="n">dW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pos_scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
    
    <span class="c1"># grad L2 reg with respect to W</span>
    <span class="n">dW</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
    
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
    
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># we do some test on the loss function &amp; grad</span>
<span class="n">X_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">y_dev</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 3073</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># test the loss with W initialized very small</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0e-5</span>

<span class="n">loss</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">svm_np</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;loss for weight close to zero: </span><span class="si">{:.4f}</span><span class="s1">, we should expect loss is close to 9&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>loss for weight close to zero: 8.9786, we should expect loss is close to 9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># test the grad vs numerical grad</span>
<span class="kn">from</span> <span class="nn">gradient_check</span> <span class="k">import</span> <span class="n">grad_check_sparse</span><span class="p">,</span> <span class="n">rel_error</span>

<span class="c1"># first test with </span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">test analytics grad vs numerical grad for reg=</span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="p">))</span>
<span class="n">loss_t1</span><span class="p">,</span> <span class="n">dW_t1</span> <span class="o">=</span> <span class="n">svm_np</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;loss: </span><span class="si">{:10.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss_t1</span><span class="p">))</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">svm_np</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">reg</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">grad_num</span> <span class="o">=</span> <span class="n">grad_check_sparse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">dW_t1</span><span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="mf">1.0e5</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">test analytics grad vs numerical grad for reg=</span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="p">))</span>
<span class="n">loss_t2</span><span class="p">,</span> <span class="n">dW_t2</span> <span class="o">=</span> <span class="n">svm_np</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;loss: </span><span class="si">{:10.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss_t2</span><span class="p">))</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">svm_np</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">reg</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">grad_num</span> <span class="o">=</span> <span class="n">grad_check_sparse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">dW_t2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>
test analytics grad vs numerical grad for reg=0.00
loss:     8.9786
numerical:      -7.12 analytic:      -7.12, relative error: 2.32209e-12
numerical:     -20.58 analytic:     -20.58, relative error: 7.89882e-14
numerical:      29.44 analytic:      29.44, relative error: 2.94582e-13
numerical:      18.87 analytic:      18.87, relative error: 1.40428e-12
numerical:     -11.73 analytic:     -11.73, relative error: 2.12009e-12
numerical:      13.95 analytic:      13.95, relative error: 7.35565e-13
numerical:      32.50 analytic:      32.50, relative error: 1.02084e-12
numerical:       2.01 analytic:       2.01, relative error: 1.59684e-11
numerical:      57.43 analytic:      57.43, relative error: 4.55323e-13
numerical:       5.17 analytic:       5.17, relative error: 5.76042e-12

test analytics grad vs numerical grad for reg=100000.00
loss:     9.1337
numerical:     -18.45 analytic:     -18.45, relative error: 9.26696e-13
numerical:       2.98 analytic:       2.98, relative error: 1.27999e-11
numerical:     -23.46 analytic:     -23.46, relative error: 1.61913e-12
numerical:      16.46 analytic:      16.46, relative error: 2.60998e-12
numerical:     -17.86 analytic:     -17.86, relative error: 1.15782e-12
numerical:       6.51 analytic:       6.51, relative error: 8.82366e-13
numerical:       9.71 analytic:       9.71, relative error: 1.16765e-12
numerical:      48.95 analytic:      48.95, relative error: 1.60745e-12
numerical:       3.61 analytic:       3.61, relative error: 8.50203e-12
numerical:     -18.53 analytic:     -18.53, relative error: 1.44855e-12
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Looking at numerical gradient v.s analytics one, we are confident about our implementation. Let's try to re-implement in in TensorFlow</p>
<h3 id="Implentation-SVM-with-TensorFlow">Implentation SVM with TensorFlow<a class="anchor-link" href="#Implentation-SVM-with-TensorFlow">&#182;</a></h3><p>The main difficulty with TensorFlow is it does not support dynamics range i.e given $M$ is a 2D-tensor and we can't access $M[0:N-1,y]$ where $N$ is number of input. However, there is a work around, by fixing the batch_size $N$, we create a range $[0:N-1]$ in advance, then we can access $M[0:N,y]$.</p>
<p>The implementation is given below, and we test v.s numpy implementation:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#tf.reset_default_graph()</span>

<span class="k">def</span> <span class="nf">svm_tf</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>  <span class="n">reg</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>    
    <span class="n">scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
    
    <span class="n">coord</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">y</span><span class="p">]))</span>    
    <span class="n">correct_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">coord</span><span class="p">)</span>    
    <span class="n">M</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">correct_scores</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">correct_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_pred</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">acc</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>
<span class="n">y_dev</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>
<span class="n">batch_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>
<span class="n">vX</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>
<span class="n">vy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>
<span class="n">vreg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">vW</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;W&#39;</span><span class="p">)</span>

<span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">svm_tf</span><span class="p">(</span><span class="n">vW</span><span class="p">,</span> <span class="n">vX</span><span class="p">,</span> <span class="n">vy</span><span class="p">,</span> <span class="n">vreg</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">test analytics grad vs numerical grad for reg=</span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="p">))</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">],</span> <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">vX</span> <span class="p">:</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">vy</span> <span class="p">:</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">vreg</span> <span class="p">:</span> <span class="n">reg</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss is: </span><span class="si">{:.4f}</span><span class="s1"> rel error: </span><span class="si">{:10.5e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_t1</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;grad rel error: </span><span class="si">{:10.5e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rel_error</span><span class="p">(</span><span class="n">dW</span><span class="p">,</span> <span class="n">dW_t1</span><span class="p">)))</span>

<span class="n">reg</span> <span class="o">=</span> <span class="mf">1.0e5</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">test analytics grad vs numerical grad for reg=</span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="p">))</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost</span><span class="p">,</span> <span class="n">grad</span><span class="p">],</span> <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">vX</span> <span class="p">:</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">vy</span> <span class="p">:</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">vreg</span> <span class="p">:</span> <span class="n">reg</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss is: </span><span class="si">{:.4f}</span><span class="s1"> rel error: </span><span class="si">{:10.5e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_t2</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;grad rel error: </span><span class="si">{:10.5e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rel_error</span><span class="p">(</span><span class="n">dW</span><span class="p">,</span> <span class="n">dW_t2</span><span class="p">)))</span>    
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>
test analytics grad vs numerical grad for reg=0.00
loss is: 8.9786 rel error: 9.89213e-17
grad rel error: 5.33548e-11

test analytics grad vs numerical grad for reg=100000.00
loss is: 9.1337 rel error: 9.72417e-17
grad rel error: 3.82242e-12
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The TensorFlow and Numpy both give very similar results. Let's try to optimize it using SGD, (we will scale down the datatype float64 -&gt; float32 to reduce memory consumption), first we do it with numpy</p>
<h2 id="Optimize-with-SGD-algorithm">Optimize with SGD algorithm<a class="anchor-link" href="#Optimize-with-SGD-algorithm">&#182;</a></h2><h3 id="Implement-SGD-with-Numpy">Implement SGD with Numpy<a class="anchor-link" href="#Implement-SGD-with-Numpy">&#182;</a></h3><p>We start by implementing SGD in numpy. To simplify our task, we use the Dataset from data_utils to get batch-input</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sgd_np</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">initW</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.0e-3</span><span class="p">,</span> <span class="n">print_every</span> <span class="o">=</span> <span class="mi">20</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    sgd_np implements SGD algorithm to minimize function f</span>
<span class="sd">        f: is a function with signature f(X, y, W, reg) =&gt; loss, grad</span>
<span class="sd">        initW: is initial weights</span>
<span class="sd">        train_data: is Dataset object supports function next_batch() =&gt; X_batch, y_batch used in train-step</span>
<span class="sd">        val_data: is Dataset object supports function next_batch() =&gt; X_batch, y_batch used in val-step</span>
<span class="sd">        reg: regularization lambda</span>
<span class="sd">        learning_rate: a hyperparameter to control update-step W:= W - learning_rate * dW        </span>
<span class="sd">        print_every: log to console the loss &amp; store it in loss_history to visualize it laters</span>
<span class="sd">    &#39;&#39;&#39;</span>   
    
    <span class="c1"># downcast to float32</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">initW</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    
    <span class="c1"># get number of iteration</span>
    <span class="n">nb_iters</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">get_nb_iters</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>  
        <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        
        <span class="n">it_per_second</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="c1">#sys.stdout.write(&quot;\rProgress: {:&gt;5.2f}% Speed (it/sec): {:&gt;10.4f}&quot;.format(100 * i / nb_iters, it_per_second))</span>
                       
        <span class="c1"># sgd update for minimize loss</span>
        <span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>        
        
        <span class="c1"># log current state        </span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>        
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iter </span><span class="si">{:&gt;10d}</span><span class="s1">/</span><span class="si">{:&lt;10d}</span><span class="s1"> loss </span><span class="si">{:10.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">nb_iters</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
        
        
        <span class="n">epoch_end</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">is_epoch_end</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch_end</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># validation it here</span>
            <span class="k">if</span> <span class="n">val_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">()</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
                <span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_val</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Epoch </span><span class="si">{:&gt;3d}</span><span class="s1">/</span><span class="si">{:&lt;3d}</span><span class="s1"> val_acc = </span><span class="si">{:5.2f}</span><span class="s1">%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>
    
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Train time: </span><span class="si">{:&lt;10.2f}</span><span class="s1"> seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">loss_history</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Fit-optimal-weight-with-SGD">Fit optimal weight with SGD<a class="anchor-link" href="#Fit-optimal-weight-with-SGD">&#182;</a></h3><p>We now are ready to fit optimal weight for SVM, we should expect accuracy ~ 37%</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">data_utils</span> <span class="k">import</span> <span class="n">Dataset</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">initW</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> 

<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-7</span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">5e4</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2793</span><span class="p">)</span>

<span class="n">W</span><span class="p">,</span> <span class="n">loss_hist</span> <span class="o">=</span> <span class="n">sgd_np</span><span class="p">(</span><span class="n">svm_np</span><span class="p">,</span> <span class="n">initW</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> 
                      <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>
Epoch   0/10  val_acc = 12.40%
Iter        100/2450       loss   293.4844
Iter        200/2450       loss   110.6457

Epoch   1/10  val_acc = 31.50%
Iter        300/2450       loss    43.6644
Iter        400/2450       loss    19.4370

Epoch   2/10  val_acc = 36.70%
Iter        500/2450       loss    10.2672
Iter        600/2450       loss     6.9989
Iter        700/2450       loss     6.4587

Epoch   3/10  val_acc = 37.10%
Iter        800/2450       loss     5.1771
Iter        900/2450       loss     5.5268

Epoch   4/10  val_acc = 37.90%
Iter       1000/2450       loss     4.7950
Iter       1100/2450       loss     5.0537
Iter       1200/2450       loss     5.5000

Epoch   5/10  val_acc = 37.40%
Iter       1300/2450       loss     5.1793
Iter       1400/2450       loss     5.4389

Epoch   6/10  val_acc = 38.50%
Iter       1500/2450       loss     5.3384
Iter       1600/2450       loss     5.2760
Iter       1700/2450       loss     5.4559

Epoch   7/10  val_acc = 39.30%
Iter       1800/2450       loss     5.4878
Iter       1900/2450       loss     5.4624

Epoch   8/10  val_acc = 36.80%
Iter       2000/2450       loss     4.7789
Iter       2100/2450       loss     5.4958
Iter       2200/2450       loss     5.1713

Epoch   9/10  val_acc = 38.00%
Iter       2300/2450       loss     5.0288
Iter       2400/2450       loss     5.1771

Epoch  10/10  val_acc = 38.00%

Train time: 3.37       seconds
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Test-obtained-weights-on-test-data">Test obtained weights on test data<a class="anchor-link" href="#Test-obtained-weights-on-test-data">&#182;</a></h3><p>We can test our obtimal weight on X_test, y_test</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs number&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">svm_np</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Valuation on test-set acc = </span><span class="si">{:5.2f}</span><span class="s1">%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmgAAAHjCAYAAACXcOPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>
Valuation on test-set acc = 35.94%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="SGD-with-TensorFlow">SGD with TensorFlow<a class="anchor-link" href="#SGD-with-TensorFlow">&#182;</a></h3><p>Now, let's try SGD with TensorFlow which is very convenient since SGD is already implemented in SGD: <a href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer">tf.train.GradientDescentOptimizer</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sgd_tf</span><span class="p">(</span><span class="n">initW</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.0e-3</span><span class="p">,</span> <span class="n">print_every</span> <span class="o">=</span> <span class="mi">20</span><span class="p">):</span>
    <span class="c1"># setup input</span>
    <span class="n">train_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">batch_size</span><span class="p">()))</span>
    <span class="n">val_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">val_data</span><span class="o">.</span><span class="n">batch_size</span><span class="p">()))</span>
    <span class="n">vX</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>
    <span class="n">vy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])</span>
    <span class="n">vreg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    
    <span class="c1"># trainable variable</span>
    <span class="n">vW</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initW</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;W&#39;</span><span class="p">)</span>
    
    <span class="c1"># cost &amp; acc</span>
    <span class="n">cost</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">svm_tf</span><span class="p">(</span><span class="n">vW</span><span class="p">,</span> <span class="n">vX</span><span class="p">,</span> <span class="n">vy</span><span class="p">,</span> <span class="n">vreg</span><span class="p">,</span> <span class="n">train_idx</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">val_acc</span> <span class="o">=</span> <span class="n">svm_tf</span><span class="p">(</span><span class="n">vW</span><span class="p">,</span> <span class="n">vX</span><span class="p">,</span> <span class="n">vy</span><span class="p">,</span> <span class="n">vreg</span><span class="p">,</span> <span class="n">val_idx</span><span class="p">)</span>
    
    <span class="c1"># setup optimizer</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    
    <span class="n">nb_iters</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">get_nb_iters</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>    
    <span class="n">opt_W</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="c1"># init out variable</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
        
        <span class="c1"># training loop</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span> <span class="p">{</span><span class="n">vX</span> <span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span>
                                                             <span class="n">vy</span> <span class="p">:</span> <span class="n">y_batch</span><span class="p">,</span>
                                                             <span class="n">vreg</span> <span class="p">:</span> <span class="n">reg</span><span class="p">})</span>
            
            <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="c1"># log current state        </span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>                    
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iter </span><span class="si">{:&gt;10d}</span><span class="s1">/</span><span class="si">{:&lt;10d}</span><span class="s1"> loss </span><span class="si">{:10.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">nb_iters</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>


            <span class="n">epoch_end</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">is_epoch_end</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">epoch_end</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
                <span class="c1"># validation it here</span>
                <span class="k">if</span> <span class="n">val_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">next_batch</span><span class="p">()</span>
                    
                    <span class="n">acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">val_acc</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span> <span class="p">{</span> <span class="n">vX</span> <span class="p">:</span> <span class="n">X_val</span><span class="p">,</span>
                                                         <span class="n">vy</span> <span class="p">:</span> <span class="n">y_val</span><span class="p">,</span>
                                                         <span class="n">vreg</span> <span class="p">:</span> <span class="n">reg</span><span class="p">})</span>
                    
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Epoch </span><span class="si">{:&gt;3d}</span><span class="s1">/</span><span class="si">{:&lt;3d}</span><span class="s1"> val_acc = </span><span class="si">{:5.2f}</span><span class="s1">%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>
        
        <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Train time: </span><span class="si">{:&lt;10.2f}</span><span class="s1"> seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
        
        <span class="n">opt_W</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">vW</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt_W</span><span class="p">,</span> <span class="n">loss_history</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># test TensorFlow&#39;s SGD</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-7</span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">5e4</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2793</span><span class="p">)</span>

<span class="n">W_tf</span><span class="p">,</span> <span class="n">loss_hist_tf</span> <span class="o">=</span> <span class="n">sgd_tf</span><span class="p">(</span><span class="n">initW</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> 
                      <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>
Epoch   0/10  val_acc = 12.40%
Iter        100/2450       loss   293.4841
Iter        200/2450       loss   110.6457

Epoch   1/10  val_acc = 31.50%
Iter        300/2450       loss    43.6644
Iter        400/2450       loss    19.4370

Epoch   2/10  val_acc = 36.70%
Iter        500/2450       loss    10.2672
Iter        600/2450       loss     6.9989
Iter        700/2450       loss     6.4587

Epoch   3/10  val_acc = 37.10%
Iter        800/2450       loss     5.1771
Iter        900/2450       loss     5.5268

Epoch   4/10  val_acc = 37.90%
Iter       1000/2450       loss     4.7950
Iter       1100/2450       loss     5.0537
Iter       1200/2450       loss     5.5000

Epoch   5/10  val_acc = 37.40%
Iter       1300/2450       loss     5.1793
Iter       1400/2450       loss     5.4389

Epoch   6/10  val_acc = 38.50%
Iter       1500/2450       loss     5.3384
Iter       1600/2450       loss     5.2760
Iter       1700/2450       loss     5.4559

Epoch   7/10  val_acc = 39.30%
Iter       1800/2450       loss     5.4878
Iter       1900/2450       loss     5.4624

Epoch   8/10  val_acc = 36.80%
Iter       2000/2450       loss     4.7789
Iter       2100/2450       loss     5.4958
Iter       2200/2450       loss     5.1713

Epoch   9/10  val_acc = 38.00%
Iter       2300/2450       loss     5.0288
Iter       2400/2450       loss     5.1771

Epoch  10/10  val_acc = 38.00%

Train time: 10.51      seconds
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_hist_tf</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs number&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">svm_np</span><span class="p">(</span><span class="n">W_tf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Valuation on test-set acc = </span><span class="si">{:5.2f}</span><span class="s1">%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>
  
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmgAAAHjCAYAAACXcOPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>
Valuation on test-set acc = 35.94%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">&#182;</a></h1><p>We have implemented Multiclass SVM Linear Classifier using Numpy and TensorFlow, they both give similar result. However due to the difficulty while accessing array's index it seems that TensorFlows is 3 times slower than Numpy. Notice that the svm loss function is  implemented by us so it might not as optimal as it should be. We hope TensorFlow will improve array indexing in the next release and also implement more loss function (they seem only implement softmax).</p>

</div>
</div>
</div>
 


  </div>
  <footer>
    <div class="article-footer">
      

      
      

      
      
      <div id="pagenavigation-next-prev">
        
        <div id="pagenavigation-next">
          <span class="pagenav-label">Previous</span>
          <a href="https://minh84.github.io/ml-examples/post/learn-tensorflow-p01/">Learn TensorFlow P01</a>
        </div>
        
        
        <div id="pagenavigation-prev">
          <span class="pagenav-label">Next</span>
          <a href="https://minh84.github.io/ml-examples/demos/learn_tf/02-linear-classifier-softmax/"></a>
        </div>
        
      </div>
      
    </div>
  </footer>
</div>
        </div>        
      </div>
      <footer>
        <div id="site-footer-wrap">
          <div id="site-footer">
            <span>Powered by <a href="https://gohugo.io/">Hugo</a>.</span>
            <span>
              
              Copyright (c) 2017, <a href="https://minh84.github.io/ml-examples/">Machine Learning Examples</a>
              
            </span>
          </div>
        </div>
      </footer>
    </div>
  </body>
</html>

