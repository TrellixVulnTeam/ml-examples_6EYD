{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem\n",
    "In this notebook we work on classification of [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). We define some notation to use later on\n",
    "\n",
    "* $x^{(i)}$ are input-images each has shape 32x32x3 (RGB)\n",
    "* $y^{(i)}$ are labels of above images and can take values $0,\\ldots,9$ corresponding to 10 classes\n",
    "\n",
    "To solve this classification, we try to find a function $h$ that maps from image $x$ to scores i.e\n",
    "$$\n",
    "h: x \\mapsto \\left(\\begin{array}{c}s_0(x)\\\\ \\ldots\\\\ s_9(x)\\end{array}\\right)\n",
    "$$\n",
    "where $s_i(x)$ is score of $x$ in $i-$th class. Then we predict the label of $x$ as\n",
    "$$\n",
    "x\\text{'s label}:=\\mathrm{arg}\\max_{i}s_i(x)\n",
    "$$\n",
    "\n",
    "The notebook is organized as follows\n",
    "* Load CIFAR-10 dataset \n",
    "* Introduce Linear classifier\n",
    "* Pre-processing data for Linear classifier\n",
    "* Multiclass SVM loss\n",
    "* Optimize SVM loss with SGD\n",
    "\n",
    "The goal of this notebook is to learn how to implement SVM loss function in [numpy](http://www.numpy.org/) and [TensorFlows](https://www.tensorflow.org/).\n",
    "\n",
    "Let's start by loading some python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# append common path\n",
    "import sys\n",
    "COMMON_PATH = '../common'\n",
    "if COMMON_PATH not in sys.path:\n",
    "    sys.path.insert(0, COMMON_PATH)\n",
    "    \n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "We need to download dataset from the internet and untar it, we use some helper functions in *common* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz is downloaded to ./cifar-10-python.tar.gz\n",
      "./cifar-10-python.tar.gz is untar to ./cifar-10-batches-py\n"
     ]
    }
   ],
   "source": [
    "from data_utils import download_file_to_cwd, untar_to_cwd\n",
    "\n",
    "url = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "filename = 'cifar-10-python.tar.gz'\n",
    "\n",
    "# download data to current directory\n",
    "download_file_to_cwd(url, filename)\n",
    "\n",
    "# untar the data\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "untar_to_cwd(filename, cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs shape: (49000, 32, 32, 3)\n",
      "Train labels shape: (49000,)\n",
      "Validation inputs shape: (1000, 32, 32, 3)\n",
      "Validation labels shape: (1000,)\n",
      "Test inputs shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# load data to memory\n",
    "from cifar10_input import load_CIFAR10\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# let's divide train into training set (49000) + validation set (1000)\n",
    "num_training = 49000\n",
    "\n",
    "mask = range(num_training, X_train.shape[0])\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print ('Train inputs shape: {}'.format(X_train.shape))\n",
    "print ('Train labels shape: {}'.format(y_train.shape))\n",
    "\n",
    "print ('Validation inputs shape: {}'.format(X_val.shape))\n",
    "print ('Validation labels shape: {}'.format(y_val.shape))\n",
    "\n",
    "print ('Test inputs shape: {}'.format(X_test.shape))\n",
    "print ('Test labels shape: {}'.format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier\n",
    "We consider $h(x)$ as linear function of $x$\n",
    "$$\n",
    "h(x) = f(x,W) = w_0 + \\sum_{i,j}w_{i,j}x_{i,j}\n",
    "$$\n",
    "\n",
    "Our goal is to find $W$ to minimize some loss function \n",
    "$$\n",
    "L(y, f(x, W))\n",
    "$$\n",
    "where $y$ is the label of the image $x$.\n",
    "\n",
    "## Pre-processing training data\n",
    "To simplify our computation, we do the following preprocesing steps\n",
    "\n",
    "* flatten our input image 32x32x3 => 3072 \n",
    "* normalize training data (it's always good idea to have normalized input with mean = 0.0), \n",
    "* append one (for bias term) to the end of each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs shape: (49000, 3073)\n",
      "Validation inputs shape: (1000, 3073)\n",
      "Test inputs shape: (10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "# flatten input data\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "X_val = X_val.reshape(X_val.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "\n",
    "# normalize training data\n",
    "mean_images = np.mean(X_train, axis=0)\n",
    "X_train -= mean_images\n",
    "X_val -= mean_images\n",
    "X_test -= mean_images\n",
    "\n",
    "# append one for bias term\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "print ('Train inputs shape: {}'.format(X_train.shape))\n",
    "print ('Validation inputs shape: {}'.format(X_val.shape))\n",
    "print ('Test inputs shape: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, $x$ is flattened $x\\in\\mathbb{R}^D$ (D=3073 in our example) and we consider the input data in following form\n",
    "$$\n",
    "X = \\left(\\begin{array}{c}\n",
    "(x^{(1)})^T\\\\\n",
    "\\vdots\\\\\n",
    "(x^{(N)})^T\n",
    "\\end{array}\\right)\\in \\mathbb{R}^{N\\times D}\n",
    "$$\n",
    "so the weight matrix $W$ has shape $D\\times C$ with $C$ is number of classes (in our example $C=10$) and our linear function is given by\n",
    "$$\n",
    "f(X,W) = X\\times W\n",
    "$$\n",
    "where each row is score for each input.\n",
    "\n",
    "## Multiclass SVM loss\n",
    "Given $(x,y)$ are the image and the label,respectively, and the scores $s(x)=f(x,W)$, the SVM loss for one sample $(x,y)$ has the form\n",
    "$$\n",
    "L(y, s(x)) = \\sum_{i\\neq y}\\max(0, s_i - s_y + 1)\n",
    "$$\n",
    "Intutively, if $s_y >= s_i + 1$ for all $i\\neq y$ we then have SVM loss $=0$, so the SVM is try to maximize the chance of $s_y$ is the maximum of $s_i$. Since we predict the label of $x$ as $\\mathrm{arg}\\max_{i}s_i$, minimize SVM loss might help to maximize correct prediction.\n",
    "\n",
    "The SVM loss for $N$ samples is the mean of SVM loss at each one sample plus a regulized form:\n",
    "$$\n",
    "\\mathrm{loss}(W) = \\frac{1}{N} \\sum_{i=1}^NL\\left(y^{(i)}, s(x^{(i)})\\right) + \\frac{1}{2}\\lambda ||W||^2\n",
    "$$\n",
    "where $||W||^2 = \\sum_{i,j}W_{i,j}^2$ is added to reduce overfitting\n",
    "\n",
    "In the following we implement the SVM loss \n",
    "* using **numpy** only\n",
    "* using **TensorFlow**\n",
    "\n",
    "Note that we need to compute not only the loss function but also the gradient with respect to $W$ so that we can use with SGD to minimize the loss. \n",
    "\n",
    "Let's compute the loss, we define $M = \\left(M_{ij}\\right)\\in \\mathbb{R}^{N\\times D}$ where\n",
    "$$\n",
    " M_{ij} = \\left\\{ \\begin{array}{l}\n",
    " s_j\\left(x^{(i)}\\right) - s_{y^{(i)}}\\left(x^{(i)}\\right) + 1 \\text{ for } j\\neq y^{(i)}\\\\\n",
    " 0 \\text{ otherwise}\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$\n",
    "Then the SVM-loss is given as\n",
    "$$\n",
    " \\sum_{i=1}^NL\\left(y^{(i)}, s(x^{(i)})\\right) = \\sum_{ij}M_{ij}\\times 1_{M_{ij} > 0}\n",
    "$$\n",
    "\n",
    "Let's derive the gradient, we have\n",
    "$$\n",
    " \\frac{\\partial}{\\partial W_{uv}} \\max\\left(0, s_j\\left(x^{(i)}\\right) - s_{y^{(i)}}\\left(x^{(i)}\\right) + 1\\right)  = 1_{M_{ij} > 0}\\times \\left(x^{(i)}_{u}\\times 1_{j=v} -x^{(i)}_{u}\\times 1_{j=y^{(i)}}\\right)\n",
    "$$\n",
    "so the we have\n",
    "$$\n",
    "\\nabla_W\\max\\left(0, s_j\\left(x^{(i)}\\right) - s_{y^{(i)}}\\left(x^{(i)}\\right) + 1\\right) =  1_{M_{ij} > 0} \\times \\left(\\begin{array}{ccccccccccc}\n",
    "    0 & \\cdots & 0 &x^{(i)}_1 & 0 & \\cdots & 0 & -x^{(i)}_1 & 0 & \\cdots & 0\\\\\n",
    "    0 & \\cdots & 0 &x^{(i)}_2 & 0 & \\cdots & 0 & -x^{(i)}_2 & 0 & \\cdots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots& \\vdots& \\vdots& \\vdots& \\vdots& \\vdots& \\vdots& \\vdots& \\vdots\\\\\n",
    "    0 & \\cdots & 0 & \\smash[b]{\\underbrace{x^{(i)}_D}_{j-th}} & 0 & \\cdots & 0 & \\smash[b]{\\underbrace{-x^{(i)}_D}_{y^{(i)}-th}} & 0 & \\cdots & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Denote $P=(P_{ij})\\in \\mathbb{R}^{N\\times D}$ is defined as\n",
    "$$\n",
    "P_{ij} = \\left\\{\\begin{array}{ll}\n",
    "1_{M_{ij} > 0} & \\text{if } j\\neq y^{(i)}\\\\\n",
    "- \\sum_{j\\neq y^{(i)}} 1_{M_{ij} > 0} & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "From above equation, we can show that\n",
    "$$\n",
    "\\nabla_W L\\left(y^{(i)}, s(x^{(i)})\\right) = P[i] \\times x^{(i)}\n",
    "$$\n",
    "where $P[i]$ is i-th row of $P$, so\n",
    "$$\n",
    "\\nabla_W\\sum_{i=1}^NL\\left(y^{(i)}, s(x^{(i)})\\right) = \\sum_{i=1}^N P[i] \\times x^{(i)} = X^T\\times P\n",
    "$$\n",
    "\n",
    "### Implentation SVM with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_np(X, y, W, reg):\n",
    "    '''\n",
    "    we implement svm loss defined as above\n",
    "        X: data inputs has shape (N, D)\n",
    "        y: labels has shape (N,)\n",
    "        W: weights has shape (D, num_classes)\n",
    "        reg: positive real number\n",
    "    the function return\n",
    "        loss: svm loss\n",
    "        dW: gradient of loss regarding to W\n",
    "    '''    \n",
    "    scores = X.dot(W)    # N x num_classes\n",
    "    \n",
    "    if y is None:\n",
    "        return scores\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    M = 1 + scores - scores[range(N), y].reshape(N,1)    \n",
    "    M[range(N), y] = 0.0\n",
    "    \n",
    "    pos_scores = np.zeros_like(M)\n",
    "    pos_scores[M > 0] = 1.0\n",
    "    \n",
    "    # svm loss\n",
    "    loss = np.sum(M * pos_scores) / N\n",
    "    \n",
    "    # adding reg\n",
    "    loss += 0.5 * reg * np.sum(W*W)\n",
    "    \n",
    "    # implement the grad\n",
    "    sum_pos_scores = np.sum(pos_scores, axis=1)  # sum 1_{M_{ij} > 0}\n",
    "    pos_scores[range(N), y] = - sum_pos_scores\n",
    "    \n",
    "    # grad SVM with respect to W\n",
    "    dW = X.T.dot(pos_scores) / N\n",
    "    \n",
    "    # grad L2 reg with respect to W\n",
    "    dW += reg * W\n",
    "    \n",
    "    return loss, dW\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for weight close to zero: 9.0445, we should expect loss is close to 9\n"
     ]
    }
   ],
   "source": [
    "# we do some test on the loss function & grad\n",
    "X_dev = X_train[:100]\n",
    "y_dev = y_train[:100]\n",
    "\n",
    "D = X_train.shape[1]  # 3073\n",
    "num_classes = 10\n",
    "# test the loss with W initialized very small\n",
    "W = np.random.randn(D, num_classes) * 1.0e-5\n",
    "\n",
    "loss,_ = svm_np(X_dev, y_dev, W, 1e-5)\n",
    "print ('loss for weight close to zero: {:.4f}, we should expect loss is close to 9'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test analytics grad vs numerical grad for reg=0.00\n",
      "loss:     9.0445\n",
      "numerical:      -5.00 analytic:      -5.00, relative error: 4.10182e-12\n",
      "numerical:     -10.44 analytic:     -10.44, relative error: 1.46684e-12\n",
      "numerical:      12.66 analytic:      12.66, relative error: 1.79906e-13\n",
      "numerical:      11.14 analytic:      11.14, relative error: 1.59188e-13\n",
      "numerical:      -8.73 analytic:      -8.73, relative error: 7.41365e-13\n",
      "numerical:     -15.35 analytic:     -15.35, relative error: 1.27636e-12\n",
      "numerical:      -2.29 analytic:      -2.29, relative error: 1.21880e-11\n",
      "numerical:      11.61 analytic:      11.61, relative error: 1.34308e-12\n",
      "numerical:       1.76 analytic:       1.76, relative error: 3.07464e-12\n",
      "numerical:     -22.12 analytic:     -22.12, relative error: 4.89815e-13\n",
      "\n",
      "test analytics grad vs numerical grad for reg=100000.00\n",
      "loss:     9.2015\n",
      "numerical:       4.68 analytic:       4.68, relative error: 2.93276e-12\n",
      "numerical:      -8.91 analytic:      -8.91, relative error: 4.25314e-13\n",
      "numerical:      31.17 analytic:      31.17, relative error: 1.08906e-12\n",
      "numerical:      21.28 analytic:      21.28, relative error: 1.31870e-14\n",
      "numerical:      -3.19 analytic:      -3.19, relative error: 4.86434e-12\n",
      "numerical:     -23.17 analytic:     -23.17, relative error: 1.32475e-12\n",
      "numerical:      16.14 analytic:      16.14, relative error: 4.21103e-12\n",
      "numerical:       2.12 analytic:       2.12, relative error: 1.49841e-11\n",
      "numerical:       6.88 analytic:       6.88, relative error: 1.85558e-12\n",
      "numerical:      13.95 analytic:      13.95, relative error: 7.31985e-15\n"
     ]
    }
   ],
   "source": [
    "# test the grad vs numerical grad\n",
    "from gradient_check import grad_check_sparse, rel_error\n",
    "\n",
    "# first test with \n",
    "reg = 0.0\n",
    "print ('\\ntest analytics grad vs numerical grad for reg={:.2f}'.format(reg))\n",
    "loss_t1, dW_t1 = svm_np(X_dev, y_dev, W, reg)\n",
    "print ('loss: {:10.4f}'.format(loss_t1))\n",
    "f = lambda w: svm_np(X_dev, y_dev, w, reg)[0]\n",
    "grad_num = grad_check_sparse(f, W, dW_t1)\n",
    "\n",
    "reg = 1.0e5\n",
    "print ('\\ntest analytics grad vs numerical grad for reg={:.2f}'.format(reg))\n",
    "loss_t2, dW_t2 = svm_np(X_dev, y_dev, W, reg)\n",
    "print ('loss: {:10.4f}'.format(loss_t2))\n",
    "f = lambda w: svm_np(X_dev, y_dev, w, reg)[0]\n",
    "grad_num = grad_check_sparse(f, W, dW_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at numerical gradient v.s analytics one, we are confident about our implementation. Let's try to re-implement in in TensorFlow\n",
    "\n",
    "### Implentation SVM with TensorFlow\n",
    "The main difficulty with TensorFlow is it does not support dynamics range i.e given $M$ is a 2D-tensor and we can't access $M[0:N-1,y]$ where $N$ is number of input. However, there is a work around, by fixing the batch_size $N$, we create a range $[0:N-1]$ in advance, then we can access $M[0:N,y]$. \n",
    "\n",
    "The implementation is given below, and we test v.s numpy implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test analytics grad vs numerical grad for reg=0.00\n",
      "loss is: 9.0445 rel error: 0.00000e+00\n",
      "grad rel error: 5.33548e-11\n",
      "\n",
      "test analytics grad vs numerical grad for reg=100000.00\n",
      "loss is: 9.2015 rel error: 0.00000e+00\n",
      "grad rel error: 6.01852e-12\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "def svm_tf(X, y,  W,  reg, batch_idx):    \n",
    "    scores = tf.matmul(X, W)\n",
    "    \n",
    "    coord = tf.transpose(tf.stack([batch_idx, y]))    \n",
    "    correct_scores = tf.gather_nd(scores, coord)    \n",
    "    M = tf.nn.relu(1.0 + scores - tf.reshape(correct_scores,[-1,1]))\n",
    "    cost = tf.reduce_mean(tf.reduce_sum(M, axis=1)) - 1.0 + reg * tf.nn.l2_loss(W)\n",
    "    grad = tf.gradients(cost, [W])[0]\n",
    "    correct_pred = tf.equal(tf.argmax(scores, axis = 1), y)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return cost, grad, acc\n",
    "\n",
    "batch_size = 100\n",
    "X_dev = X_train[:batch_size]\n",
    "y_dev = y_train[:batch_size]\n",
    "batch_idx = tf.constant(np.arange(batch_size))\n",
    "vX = tf.placeholder(tf.float64, [None, D])\n",
    "vy = tf.placeholder(tf.int64, [None])\n",
    "vreg = tf.placeholder(tf.float64)\n",
    "vW = tf.Variable(W, name = 'W')\n",
    "\n",
    "cost, grad, _ = svm_tf(vX, vy, vW, vreg, batch_idx)\n",
    "\n",
    "reg = 0.0\n",
    "print ('\\ntest analytics grad vs numerical grad for reg={:.2f}'.format(reg))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss, dW = sess.run([cost, grad], feed_dict = {vX : X_dev, vy : y_dev, vreg : reg})\n",
    "    print('loss is: {:.4f} rel error: {:10.5e}'.format(loss, rel_error(loss, loss_t1)))\n",
    "    print('grad rel error: {:10.5e}'.format(rel_error(dW, dW_t1)))\n",
    "\n",
    "reg = 1.0e5\n",
    "print ('\\ntest analytics grad vs numerical grad for reg={:.2f}'.format(reg))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss, dW = sess.run([cost, grad], feed_dict = {vX : X_dev, vy : y_dev, vreg : reg})\n",
    "    print('loss is: {:.4f} rel error: {:10.5e}'.format(loss, rel_error(loss, loss_t2)))\n",
    "    print('grad rel error: {:10.5e}'.format(rel_error(dW, dW_t2)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow and Numpy both give very similar results. Let's try to optimize it using SGD, (we will scale down the datatype float64 -> float32 to reduce memory consumption), first we do it with numpy\n",
    "\n",
    "## Optimize with SGD algorithm\n",
    "\n",
    "### Implement SGD with Numpy\n",
    "We start by implementing SGD in numpy. To simplify our task, we use the Dataset from data_utils to get batch-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_np(f, initW, train_data, val_data, reg, epochs, learning_rate = 1.0e-3, print_every = 20):\n",
    "    '''\n",
    "    sgd_np implements SGD algorithm to minimize function f\n",
    "        f: is a function with signature f(X, y, W, reg) => loss, grad\n",
    "        initW: is initial weights\n",
    "        train_data: is Dataset object supports function next_batch() => X_batch, y_batch used in train-step\n",
    "        val_data: is Dataset object supports function next_batch() => X_batch, y_batch used in val-step\n",
    "        reg: regularization lambda\n",
    "        learning_rate: a hyperparameter to control update-step W:= W - learning_rate * dW        \n",
    "        print_every: log to console the loss & store it in loss_history to visualize it laters\n",
    "    '''   \n",
    "    \n",
    "    # downcast to float32\n",
    "    W = initW.astype(np.float32)\n",
    "    \n",
    "    # get number of iteration\n",
    "    nb_iters = train_data.get_nb_iters(epochs)\n",
    "    loss_history = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(1, nb_iters + 1):\n",
    "        X_batch, y_batch = train_data.next_batch()\n",
    "        loss, grad = f(X_batch, y_batch, W, reg)  \n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        it_per_second = i / (time.time() - start)\n",
    "        #sys.stdout.write(\"\\rProgress: {:>5.2f}% Speed (it/sec): {:>10.4f}\".format(100 * i / nb_iters, it_per_second))\n",
    "                       \n",
    "        # sgd update for minimize loss\n",
    "        W -= learning_rate * grad        \n",
    "        \n",
    "        # log current state        \n",
    "        if (i % print_every == 0):        \n",
    "            print('Iter {:>10d}/{:<10d} loss {:10.4f}'.format(i, nb_iters, loss))\n",
    "        \n",
    "        \n",
    "        epoch_end, epoch = train_data.is_epoch_end(i)\n",
    "        if (epoch_end) or (i == 1):\n",
    "            # validation it here\n",
    "            if val_data is not None:\n",
    "                X_val, y_val = val_data.next_batch()\n",
    "                scores = f(X_val, None, W, reg)\n",
    "                acc = np.mean(np.argmax(scores, axis=1) == y_val)\n",
    "                print('\\nEpoch {:>3d}/{:<3d} val_acc = {:5.2f}%'.format(epoch, epochs, 100 * acc))\n",
    "    \n",
    "    print ('\\nTrain time: {:<10.2f} seconds'.format(time.time() - start))\n",
    "    return W, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit optimal weight with SGD\n",
    "We now are ready to fit optimal weight for SVM, we should expect accuracy ~ 37%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   0/10  val_acc =  9.00%\n",
      "Iter        100/2450       loss   287.1925\n",
      "Iter        200/2450       loss   107.3475\n",
      "\n",
      "Epoch   1/10  val_acc = 32.40%\n",
      "Iter        300/2450       loss    42.8402\n",
      "Iter        400/2450       loss    19.0037\n",
      "\n",
      "Epoch   2/10  val_acc = 35.10%\n",
      "Iter        500/2450       loss    10.1583\n",
      "Iter        600/2450       loss     6.9216\n",
      "Iter        700/2450       loss     6.4267\n",
      "\n",
      "Epoch   3/10  val_acc = 37.50%\n",
      "Iter        800/2450       loss     5.1702\n",
      "Iter        900/2450       loss     5.5196\n",
      "\n",
      "Epoch   4/10  val_acc = 37.80%\n",
      "Iter       1000/2450       loss     4.7932\n",
      "Iter       1100/2450       loss     5.0509\n",
      "Iter       1200/2450       loss     5.5003\n",
      "\n",
      "Epoch   5/10  val_acc = 37.40%\n",
      "Iter       1300/2450       loss     5.1796\n",
      "Iter       1400/2450       loss     5.4393\n",
      "\n",
      "Epoch   6/10  val_acc = 38.50%\n",
      "Iter       1500/2450       loss     5.3376\n",
      "Iter       1600/2450       loss     5.2767\n",
      "Iter       1700/2450       loss     5.4552\n",
      "\n",
      "Epoch   7/10  val_acc = 39.10%\n",
      "Iter       1800/2450       loss     5.4867\n",
      "Iter       1900/2450       loss     5.4599\n",
      "\n",
      "Epoch   8/10  val_acc = 36.60%\n",
      "Iter       2000/2450       loss     4.7786\n",
      "Iter       2100/2450       loss     5.4999\n",
      "Iter       2200/2450       loss     5.1709\n",
      "\n",
      "Epoch   9/10  val_acc = 37.90%\n",
      "Iter       2300/2450       loss     5.0242\n",
      "Iter       2400/2450       loss     5.1785\n",
      "\n",
      "Epoch  10/10  val_acc = 38.00%\n",
      "\n",
      "Train time: 3.84       seconds\n"
     ]
    }
   ],
   "source": [
    "from data_utils import Dataset\n",
    "D = X_train.shape[1]\n",
    "initW = 0.001 * np.random.randn(D,10) \n",
    "\n",
    "lr = 1e-7\n",
    "reg = 5e4\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "\n",
    "train_data = Dataset(X_train, y_train, batch_size, dtype = np.float32)\n",
    "val_data = Dataset(X_val, y_val, 1000, dtype = np.float32)\n",
    "\n",
    "np.random.seed(2793)\n",
    "\n",
    "W, loss_hist = sgd_np(svm_np, initW, train_data, val_data, reg, epochs, \n",
    "                      learning_rate=lr, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test obtained weights on test data\n",
    "We can test our obtimal weight on X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAHjCAYAAACXcOPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XuY3dV93/v3d/ae2Xvud40uI5AAGRDYXKxgbGzH8Q0S\nJ8G9uaRNwpO6cc45pE16PaYneeqc1qduT5smPYnzlFwamjgmxI4LTRzbGMcmji8gMDZIQkggCd01\nGmk0N819nT/mJzxQhPZIsy8z8349zzz7t9f+7T1f8fO2Plrrt9aKlBKSJEmqHXXVLkCSJEmvZECT\nJEmqMQY0SZKkGmNAkyRJqjEGNEmSpBpjQJMkSaoxBjRJkqQaY0CTJEmqMQY0SZKkGpOvdgGXoqen\nJ23atKnaZUiSJF3Qk08+eTKl1FvKucs6oG3atInt27dXuwxJkqQLiogDpZ7rEKckSVKNMaBJkiTV\nGAOaJElSjTGgSZIk1ZiyBrSI+CcRsSMino2IT0dEMSK6IuKRiNiTPXYuOP/eiNgbEbsj4vZy1iZJ\nklSryhbQImID8I+BbSml64EccBfwUeDRlNIW4NHsORGxNXv9OuAO4JMRkStXfZIkSbWq3EOceaAx\nIvJAE3AEuBO4P3v9fuCD2fGdwAMppcmU0j5gL3BLmeuTJEmqOWULaCmlw8B/BF4CjgJnUkpfAvpS\nSkez044BfdnxBuDggo84lLVJkiStKuUc4uxkvldsM7AeaI6In1x4TkopAWmRn/uRiNgeEdsHBgaW\nrF5JkqRaUc4hzvcC+1JKAymlaeBPgbcBxyNiHUD2eCI7/zCwccH7+7O2V0gp3ZdS2pZS2tbbW9Ju\nCZIkSctKOQPaS8CtEdEUEQG8B9gFPAzcnZ1zN/BQdvwwcFdEFCJiM7AFeLyM9UmSJNWksu3FmVL6\ndkR8BngKmAG+A9wHtAAPRsSHgQPAh7Lzd0TEg8DO7Px7Ukqz5apPkiSpVsX8bWDL07Zt25KbpUuS\npOUgIp5MKW0r5Vx3EpAkSaoxBjRJkqQaY0CTJEmqMQa015FSYnhimrNTzlWQJEmVY0B7HUPj07zp\nY1/igSdeqnYpkiRpFTGgvY7mwvwqJGOTM1WuRJIkrSYGtNfRkK+jIV/HiAFNkiRVkAHtAloKeXvQ\nJElSRRnQLmA+oDlJQJIkVY4B7QKaC3lGJuxBkyRJlWNAu4CWQs4hTkmSVFEGtAtoKeQZmzKgSZKk\nyjGgXUBzIc+oQ5ySJKmCDGgX0FLIM+oQpyRJqiAD2gW4zIYkSao0A9oFNBfyjE3NMjeXql2KJEla\nJQxoF9BybrsnJwpIkqQKMaBdQEvx3H6cLlYrSZIqw4B2Aec2TB+dnK5yJZIkabUwoF1ASyEHwKg9\naJIkqUIMaBfQUqgHcCanJEmqGAPaBTRnPWjuxylJkirFgHYBL8/itAdNkiRViAHtAlxmQ5IkVZoB\n7QLOzeJ0iFOSJFWKAe0CCvk68nXhEKckSaoYA9oFRAQtRffjlCRJlWNAK0FzQ54RA5okSaoQA1oJ\nWgr2oEmSpMoxoJVgfojTnQQkSVJlGNBK0FxwiFOSJFWOAa0ELYWcQ5ySJKliDGglaCnkGXUdNEmS\nVCEGtBI0O0lAkiRVkAGtBC2FPKNTM6SUql2KJElaBQxoJWgp5EkJxqecySlJksrPgFaCc/txOswp\nSZIqwYBWgpYsoI0a0CRJUgUY0EpgQJMkSZVkQCtBswFNkiRVkAGtBK3Fc/egOUlAkiSVnwGtBN/v\nQZuuciWSJGk1KFtAi4irI+LpBT/DEfGLEdEVEY9ExJ7ssXPBe+6NiL0RsTsibi9XbYvVXMgBMGoP\nmiRJqoCyBbSU0u6U0o0ppRuBNwPjwOeAjwKPppS2AI9mz4mIrcBdwHXAHcAnIyJXrvoWo7VQD7jM\nhiRJqoxKDXG+B3ghpXQAuBO4P2u/H/hgdnwn8EBKaTKltA/YC9xSofpeV7G+jrrA/TglSVJFVCqg\n3QV8OjvuSykdzY6PAX3Z8Qbg4IL3HMraXiEiPhIR2yNi+8DAQLnqffXvpLmQdxanJEmqiLIHtIho\nAH4c+JNXv5bmN7dc1AaXKaX7UkrbUkrbent7l6jKC2t1w3RJklQhlehB+2HgqZTS8ez58YhYB5A9\nnsjaDwMbF7yvP2urCfagSZKkSqlEQPsJvj+8CfAwcHd2fDfw0IL2uyKiEBGbgS3A4xWoryQGNEmS\nVCn5cn54RDQD7wN+bkHzJ4AHI+LDwAHgQwAppR0R8SCwE5gB7kkp1cy6Fq1FhzglSVJllDWgpZTG\ngO5XtQ0yP6vztc7/OPDxctZ0sZob8hwfnqh2GZIkaRVwJ4ESNRfybvUkSZIqwoBWotai96BJkqTK\nMKCVqLmQY3RyhvmVQSRJksrHgFai5kKe2bnE5MxctUuRJEkrnAGtRK2F+fkUDnNKkqRyM6CVqKU4\nH9BG3I9TkiSVmQGtRG3FegDOnJ2uciWSJGmlM6CVqL3RgCZJkirDgFaicwFt2IAmSZLKzIBWInvQ\nJElSpRjQStRmQJMkSRViQCtRsT5HIV/nEKckSSo7A9oitDfW24MmSZLKzoC2CG0GNEmSVAEGtEWw\nB02SJFWCAW0R2op5dxKQJEllZ0BbhNZiPcMT9qBJkqTyMqAtQlujPWiSJKn8DGiL0FqsZ2RimpRS\ntUuRJEkrmAFtEVqLeaZnE5Mzc9UuRZIkrWAGtEVoLbofpyRJKj8D2iK0FfMADHsfmiRJKiMD2iK0\nZT1oI87klCRJZWRAW4TWrAfNmZySJKmcDGiL0PpyD5oBTZIklY8BbRFaX74HzSFOSZJUPga0RWhr\n9B40SZJUfga0RWhuyFEXDnFKkqTyMqAtQkTQUnC7J0mSVF4GtEVqLda7UK0kSSorA9oitRbzLlQr\nSZLKyoC2SG2N9U4SkCRJZWVAW6SOxnqGxg1okiSpfAxoi9TdUmBwbLLaZUiSpBXMgLZIPS0NnBqb\nYm4uVbsUSZK0QhnQFqmtWM9cgrEpJwpIkqTyMKAt0ve3ezKgSZKk8jCgLdL3N0x3ooAkSSoPA9oi\ntTXO96C5m4AkSSoXA9oinetBczcBSZJULga0RTp3D5o9aJIkqVwMaIvU5j1okiSpzMoa0CKiIyI+\nExHPRcSuiHhrRHRFxCMRsSd77Fxw/r0RsTcidkfE7eWs7WI5i1OSJJVbuXvQfh34QkrpGuAGYBfw\nUeDRlNIW4NHsORGxFbgLuA64A/hkROTKXN+iFetzNOTqGLYHTZIklUnZAlpEtAPvBH4XIKU0lVIa\nAu4E7s9Oux/4YHZ8J/BASmkypbQP2AvcUq76LkVrMe89aJIkqWzK2YO2GRgA/ltEfCcificimoG+\nlNLR7JxjQF92vAE4uOD9h7K2V4iIj0TE9ojYPjAwUMbyz6+tsd5ZnJIkqWzKGdDywM3Ab6WUbgLG\nyIYzz0kpJWBRm1qmlO5LKW1LKW3r7e1dsmIXwx40SZJUTuUMaIeAQymlb2fPP8N8YDseEesAsscT\n2euHgY0L3t+ftdWc+YBmD5okSSqPsgW0lNIx4GBEXJ01vQfYCTwM3J213Q08lB0/DNwVEYWI2Axs\nAR4vV32Xoq1YzxmHOCVJUpnky/z5/wj4VEQ0AC8CP8N8KHwwIj4MHAA+BJBS2hERDzIf4maAe1JK\ns2Wu76J0NNVz5qxDnJIkqTzKGtBSSk8D217jpfec5/yPAx8vZ01LoaOpgaHxKVJKRES1y5EkSSuM\nOwlchI7GembmEmNTNdnBJ0mSljkD2kXoaJrf7mlofKrKlUiSpJXIgHYROpoaABgad6KAJElaega0\ni9DReK4HzYAmSZKWngHtIrzcg3bWIU5JkrT0DGgXoTO7B+20PWiSJKkMDGgXoS0b4jzjJAFJklQG\nBrSLUKzP0Vif8x40SZJUFga0i9TZVM+Q2z1JkqQyMKBdpPZsNwFJkqSlZkC7SB2N9Q5xSpKksjCg\nXaTOZoc4JUlSeRjQLlJ7o0OckiSpPAxoF6mjaX6IM6VU7VIkSdIKY0C7SJ1N9czMJUYnZ6pdiiRJ\nWmEMaBepq7kAwOCow5ySJGlpGdAuUm/rfEAbGJ2sciWSJGmlMaBdpO7m+Q3TT4/ZgyZJkpaWAe0i\ntWf7cbrUhiRJWmoGtIvU0XRuw3QDmiRJWloGtIvUUsiTqwuGzjrEKUmSlpYB7SJFhNs9SZKksjCg\nXYL2JgOaJElaega0S9DZ1OAQpyRJWnIGtEvQ2VTPqTF70CRJ0tIyoF2CjiY3TJckSUvPgHYJOpvq\nOW1AkyRJS8yAdgk6mxuYmJ7j7NRstUuRJEkriAHtEnQ2Zds92YsmSZKWkAHtEnRmuwkY0CRJ0lIy\noF2Ccz1oroUmSZKWkgHtEnQ2zwe0U2P2oEmSpKVjQLsE5zZMd6kNSZK0lAxol6Cj8dwkAYc4JUnS\n0jGgXYKGfB2thbxDnJIkaUkZ0C5RR3O9Q5ySJGlJGdAuUWdTg0OckiRpSRnQLpH7cUqSpKVmQLtE\nXU31nDKgSZKkJWRAu0QdTQ2cHnOIU5IkLR0D2iXqbm5gdHKGyRk3TJckSUvDgHaJzu0m4HZPkiRp\nqZQ1oEXE/oh4JiKejojtWVtXRDwSEXuyx84F598bEXsjYndE3F7O2pZKdxbQBke9D02SJC2NSvSg\n/VBK6caU0rbs+UeBR1NKW4BHs+dExFbgLuA64A7gkxGRq0B9l+RcD9ppJwpIkqQlUo0hzjuB+7Pj\n+4EPLmh/IKU0mVLaB+wFbqlCfYvycg+auwlIkqQlUu6AloAvR8STEfGRrK0vpXQ0Oz4G9GXHG4CD\nC957KGt7hYj4SERsj4jtAwMD5aq7ZC/3oBnQJEnSEsmX+fPfnlI6HBFrgEci4rmFL6aUUkSkxXxg\nSuk+4D6Abdu2Leq95dDRWE+EPWiSJGnplLUHLaV0OHs8AXyO+SHL4xGxDiB7PJGdfhjYuODt/Vlb\nTcvn6mhvrOfU2GS1S5EkSStE2QJaRDRHROu5Y+D9wLPAw8Dd2Wl3Aw9lxw8Dd0VEISI2A1uAx8tV\n31LqanaxWkmStHTKOcTZB3wuIs79nj9KKX0hIp4AHoyIDwMHgA8BpJR2RMSDwE5gBrgnpbQsVn/t\nampg0B40SZK0RMoW0FJKLwI3vEb7IPCe87zn48DHy1VTuXQ1N3BgcLzaZUiSpBXCnQSWQFdzg5ME\nJEnSkjGgLYGelgKnx6eYmZ2rdimSJGkFMKAtgXUdRWbnEsdHvA9NkiRdOgPaEuhrLQIwYECTJElL\nwIC2BNyPU5IkLSUD2hLobKoHYMiAJkmSloABbQl0ZT1op1ysVpIkLQED2hJoK9ZTF/agSZKkpWFA\nWwJ1dUFHk2uhSZKkpWFAWyK9LQVOOotTkiQtAQPaEultLTAwakCTJEmXzoC2RHpbC66DJkmSloQB\nbYn0thY4MTJJSqnapUiSpGXOgLZEeloamJqZY3RyptqlSJKkZc6AtkS6mwsADI46k1OSJF0aA9oS\n6W6ZX6x2cMz70CRJ0qUxoC2Rnpb5HrST9qBJkqRLZEBbIue2e3KIU5IkXSoD2hJ5eYjTtdAkSdIl\nMqAtkUI+R2sxz0kDmiRJukQGtCXU21LwHjRJknTJDGhLqKfF7Z4kSdKlM6AtoZ7WBu9BkyRJl8yA\ntoS6mx3ilCRJl86AtoR6WgqcOTvN1MxctUuRJEnLmAFtCfW0upuAJEm6dCUFtIi4PCLemx03RkRr\nectans7tJuBitZIk6VJcMKBFxM8CnwH+a9bUD/yPcha1XK1tKwJweOhslSuRJEnLWSk9aPcAtwHD\nACmlPcCacha1XG3qbgbgpcHxKlciSZKWs1IC2mRK6eUxu4jIA6l8JS1fbY15GvJ17iYgSZIuSSkB\n7WsR8a+Axoh4H/AnwP8sb1nLU0TQ62K1kiTpEpUS0D4KDADPAD8HfB74pXIWtZx1tzQ4SUCSJF2S\n/IVOSCnNAb+d/egCeloKnBiZqHYZkiRpGbtgQIuIfbzGPWcppSvKUtEy193cwM4jw9UuQ5IkLWMX\nDGjAtgXHReDvAF3lKWf562ktMDg2SUqJiKh2OZIkaRm64D1oKaXBBT+HU0q/BnygArUtS93NDUzP\nJobPzlS7FEmStEyVMsR584Kndcz3qJXS87Yq9bbO7yYwMDpBe1N9lauRJEnLUSlB6z8tOJ4B9gMf\nKks1K8Ca1vndBE4MT3LVGnfEkiRJi1fKLM4fqkQhK8X3e9BcC02SJF2c8wa0iPinr/fGlNKvLn05\ny9+atvmAdmLYgCZJki7O6/WgOT53EVoLeQr5OnvQJEnSRTtvQEsp/UolC1kpIoI1bQVODLtYrSRJ\nujilzOIsAh8GrmN+HTQAUkr/oJRfEBE5YDtwOKX0oxHRBfwxsIlswkFK6XR27r3Z75oF/nFK6YuL\n+cPUCvfjlCRJl6KUvTj/AFgL3A58DegHRhbxO34B2LXg+UeBR1NKW4BHs+dExFbgLuaD4B3AJ7Nw\nt+ysaS16D5okSbpopQS0q1JKvwyMpZTuZ36R2reU8uER0Z+d/zsLmu8E7s+O7wc+uKD9gZTSZEpp\nH7AXuKWU31NrelvtQZMkSRevlIA2nT0ORcT1QDuwpsTP/zXgXwJzC9r6UkpHs+NjQF92vAE4uOC8\nQ1nbK0TERyJie0RsHxgYKLGMylrTWmBofJrJmdlqlyJJkpahUgLafRHRCfwy8DCwE/j3F3pTRPwo\ncCKl9OT5zkkpJV5jI/bXk1K6L6W0LaW0rbe3dzFvrZhza6GdHJ2qciWSJGk5KmUngf+WUppl/v6z\nKxbx2bcBPx4RP8L85IK2iPhD4HhErEspHY2IdcCJ7PzDwMYF7+/P2pad76+FNsGGjsYqVyNJkpab\nUnrQ9kXEfRHxnoiIUj84pXRvSqk/pbSJ+Zv/v5JS+knme+Huzk67G3goO34YuCsiChGxGdgCPF7q\n76sl57Z7OnrGpTYkSdLilRLQrgG+DNwD7I+I34iIt1/C7/wE8L6I2AO8N3tOSmkH8CDzQ6hfAO7J\neu6WnavWtJCrC3YdHa52KZIkaRkqZS/OceaD04PZvWi/zvxwZ8lLYKSUvgp8NTseBN5znvM+Dny8\n1M+tVcX6HL0tBY7ZgyZJki5CKT1oRMQPRsQngSeZv5/sQ2WtagXobS1wYsSlNiRJ0uKVspPAfuA7\nzPei/YuU0li5i1oJ1rQWvAdNkiRdlFJmcb4ppeTNVIvU21rgu4fOVLsMSZK0DF1wiNNwdnHWtBY4\nNTbJ7NyilnmTJEkq7R40LV5va4G5BINu+SRJkhbJgFYmfW2uhSZJki7OBQNaRPxCRLTFvN+NiKci\n4v2VKG4529A5v4PAkaGzVa5EkiQtN6X0oP2D7D609wOdwE+RLS6r8zu3xdNhA5okSVqkUgLaue2d\nfgT4g2zF/5K3fFqt2hvraWrIGdAkSdKilRLQnoyILzEf0L4YEa3AXHnLWv4igrVtRU4MO0lAkiQt\nTinroH0YuBF4MaU0HhFdwM+Ut6yVoa+tyLFhJwlIkqTFKaUH7a3A7pTSUET8JPBLgCuwlqCvrcBx\nA5okSVqkUgLabwHjEXED8M+AF4D/XtaqVoi+9vkhzpRcrFaSJJWulIA2k+YTxp3Ab6SUfhNoLW9Z\nK0Nfa5Gp2TlOj09XuxRJkrSMlBLQRiLiXuaX1/jziKgD6stb1sqwtn1+sdpjLlYrSZIWoZSA9neB\nSebXQzsG9AP/b1mrWiHO7SbgfWiSJGkxStks/RjwKaA9In4UmEgpeQ9aCda1u92TJElavFK2evoQ\n8Djwd4APAd+OiL9d7sJWgjWtBfJ14XZPkiRpUUpZB+3/An4gpXQCICJ6gS8DnylnYStBPlfH2vYi\nh06PV7sUSZK0jJRyD1rduXCWGSzxfQL6Oxs5dNoeNEmSVLpSetC+EBFfBD6dPf+7wOfLV9LK0t/Z\nxNf3nKx2GZIkaRm5YEBLKf2LiPhbwG1Z030ppc+Vt6yVo7+zkeMjE0zNzNGQt+NRkiRdWCk9aKSU\nPgt8tsy1rEgbOhpJCY6eOcvl3c3VLkeSJC0D5w1oETECvNYeRQGklFJb2apaQfo7mwA4dNqAJkmS\nSnPegJZScjunJdDf2QjgTE5JklQyb4oqs3XtRXJ14UxOSZJUMgNameVzdaxtK3LYgCZJkkpkQKuA\nDa6FJkmSFsGAVgHzi9V6D5okSSqNAa0C+jubODY8vxaaJEnShRjQKqC/s5G5BMfOTFS7FEmStAwY\n0CqgvyNbamPIYU5JknRhBrQKWLhYrSRJ0oUY0CpgbXuRujCgSZKk0hjQKqAhP78WmjM5JUlSKQxo\nFdLf2WQPmiRJKokBrUI2dDa6m4AkSSqJAa1C+jsbOTY8wcysa6FJkqTXZ0CrkP7ORmbnEkddC02S\nJF2AAa1CXGpDkiSVyoBWIf2d2WK1zuSUJEkXYECrkHXtjUTA4SF70CRJ0usrW0CLiGJEPB4R342I\nHRHxK1l7V0Q8EhF7ssfOBe+5NyL2RsTuiLi9XLVVQ0O+jr7WokOckiTpgsrZgzYJvDuldANwI3BH\nRNwKfBR4NKW0BXg0e05EbAXuAq4D7gA+GRG5MtZXcZd1N7H/5Fi1y5AkSTWubAEtzRvNntZnPwm4\nE7g/a78f+GB2fCfwQEppMqW0D9gL3FKu+qrhqjUt7B0YvfCJkiRpVSvrPWgRkYuIp4ETwCMppW8D\nfSmlo9kpx4C+7HgDcHDB2w9lba/+zI9ExPaI2D4wMFDG6pfeZV1NDI1PMzIxXe1SJElSDStrQEsp\nzaaUbgT6gVsi4vpXvZ6Y71VbzGfel1LallLa1tvbu4TVlt9Gl9qQJEklqMgszpTSEPCXzN9bdjwi\n1gFkjyey0w4DGxe8rT9rWzHOLbXx0imX2pAkSedXzlmcvRHRkR03Au8DngMeBu7OTrsbeCg7fhi4\nKyIKEbEZ2AI8Xq76qmFTTzMA+5woIEmSXke+jJ+9Drg/m4lZBzyYUvqziPgm8GBEfBg4AHwIIKW0\nIyIeBHYCM8A9KaXZMtZXce2N9fS0NPCiEwUkSdLrKFtASyl9D7jpNdoHgfec5z0fBz5erppqwRU9\nLbw4YA+aJEk6P3cSqLArepvZP2hAkyRJ52dAq7DNPc2cHJ3izFmX2pAkSa/NgFZhm7OJAu4oIEmS\nzseAVmFX9DqTU5IkvT4DWoVt7GqiLuBFA5okSToPA1qFFfI5+jub7EGTJEnnZUCrgs09zew76Vpo\nkiTptRnQqmBzTzP7BsaY34pUkiTplQxoVXBFbzNjU7MMjExWuxRJklSDDGhVsKl7fibnC+4oIEmS\nXoMBrQpeXgvNHQUkSdJrMKBVwfqORhrydc7klCRJr8mAVgW5umBTd5ObpkuSpNdkQKsSl9qQJEnn\nY0Crks09Lbx0apyZ2blqlyJJkmqMAa1KruhpZno2cWRootqlSJKkGmNAq5LN2abpLzrMKUmSXsWA\nViXnltpwJqckSXo1A1qVdDc30FrM89zRkWqXIkmSaowBrUoigrds7uLre09WuxRJklRjDGhVdM3a\nNo4NTziTU5IkvYIBrYr6OxuZnUscG3YmpyRJ+j4DWhX1dzYB8NLgeJUrkSRJtcSAVkVv3NBOBDyx\n/3S1S5EkSTXEgFZF7U31XN3XyncOGtAkSdL3GdCq7Mo1La6FJkmSXsGAVmVX9DRz8NQ4UzPO5JQk\nSfMMaFW2uaeZuQQvnXKigCRJmmdAq7JzWz7tPeGenJIkaZ4BrcquXddGvi743qGhapciSZJqhAGt\nyor1Oa5Z18rTBw1okiRpngGtBty4sYPvHTrD3FyqdimSJKkGGNBqwJv6OxidnGH/oMttSJIkA1pN\nuGpNCwAvDBjQJEmSAa0mXNkzH9BeHHAmpyRJMqDVhPameta0Fth9fKTapUiSpBpgQKsRW9e3sfPI\ncLXLkCRJNcCAViOuW9/G3hOjTM7MVrsUSZJUZQa0GnHtujZm5hJ7jnsfmiRJq50BrUZcu64NgF1H\nHeaUJGm1M6DViE3dzRTr69h11IkCkiStdga0GpGrC65e28bOo2eqXYokSaqysgW0iNgYEX8ZETsj\nYkdE/ELW3hURj0TEnuyxc8F77o2IvRGxOyJuL1dttWrrulZ2HR1xyydJkla5cvagzQD/LKW0FbgV\nuCcitgIfBR5NKW0BHs2ek712F3AdcAfwyYjIlbG+mnPrFd2cOTvN9w7biyZJ0mpWtoCWUjqaUnoq\nOx4BdgEbgDuB+7PT7gc+mB3fCTyQUppMKe0D9gK3lKu+WnTzZfOdiTuOGNAkSVrNKnIPWkRsAm4C\nvg30pZSOZi8dA/qy4w3AwQVvO5S1vfqzPhIR2yNi+8DAQNlqrob+zkZai3kXrJUkaZUre0CLiBbg\ns8AvppRekTxSSglY1A1XKaX7UkrbUkrbent7l7DS6osIrl3X5lIbkiStcmUNaBFRz3w4+1RK6U+z\n5uMRsS57fR1wIms/DGxc8Pb+rG1V2bqujeeOjTA1M1ftUiRJUpWUcxZnAL8L7Eop/eqClx4G7s6O\n7wYeWtB+V0QUImIzsAV4vFz11arbruphfGqWJ/afqnYpkiSpSsrZg3Yb8FPAuyPi6eznR4BPAO+L\niD3Ae7PnpJR2AA8CO4EvAPeklFbdxpQ3bGwHYM9xF6yVJGm1ypfrg1NKXwfiPC+/5zzv+Tjw8XLV\ntBz0thRob6znuWMGNEmSVit3EqgxEcGtV3TxtedX1gxVSZJUOgNaDfqBTV0cPTPBiZGJapciSZKq\nwIBWg97U3wHAs+4oIEnSqmRAq0HXrW8jXxd884XBapciSZKqwIBWg5oLeW69opuv7zWgSZK0GhnQ\natSbL+9k97FhRidnql2KJEmqMANajbr58k7mEnz34FC1S5EkSRVmQKtRb9wwv2CtG6dLkrT6GNBq\nVFdzA2vbiux043RJklYdA1oN27q+je8eHCKlVO1SJElSBRnQatjbr+rhxZNj9qJJkrTKGNBq2B3X\nrwXg8X2nqlyJJEmqJANaDVvf0Uh/Z6MBTZKkVcaAVuNu2dzF4/tOMTfnfWiSJK0WBrQad9uVPQyO\nTbHrmPfeLqJqAAAb9klEQVShSZK0WhjQatw7tvQA8NjzJ6tciSRJqhQDWo1b01bkmrWt/NWegWqX\nIkmSKsSAtgzcekU3Tx8cYmZ2rtqlSJKkCjCgLQM3XdbB+NQszxw+U+1SJElSBRjQloF3Xb2GYn0d\nDz19pNqlSJKkCjCgLQPtjfW8cUM7X3nuhNs+SZK0ChjQlol3X9PHS6fGefHkWLVLkSRJZWZAWybe\ne+0aAJ46cLrKlUiSpHIzoC0TV/a20FrM89RLQ9UuRZIklZkBbZmoqwtu3NjBN144yazbPkmStKIZ\n0JaRH79hPQcGx/nSjmPVLkWSJJWRAW0Z+Rs3baClkOexPW77JEnSSmZAW0byuTpuvaKbv95rQJMk\naSUzoC0z79jSM7/cxsBotUuRJEllYkBbZt63tQ+Av3jW+9AkSVqpDGjLzPqORm6+rIM/+97Rapci\nSZLKxIC2DH3gTevZdXTYYU5JklYoA9oy9IE3rqMu4HPfOVztUiRJUhkY0Jahte1F3n3NGv7wWweY\nnp2rdjmSJGmJGdCWqb/95n5Oj0/zpHtzSpK04hjQlqm3b+mlPhd8eefxapciSZKWmAFtmWop5HnX\n1Wv4H08fdm9OSZJWGAPaMvYjb1zLydEpvujenJIkrSgGtGXsh69fx2VdTfzBNw9UuxRJkrSEDGjL\nWLE+x+3X9fHkS6eZmJ6tdjmSJGmJGNCWuXds6WVqZo6Hnz5S7VIkSdISMaAtc+/Y0sOGjkbu/+b+\napciSZKWSNkCWkT8XkSciIhnF7R1RcQjEbEne+xc8Nq9EbE3InZHxO3lqmuliQjuftvl7DgyzL6T\nY9UuR5IkLYFy9qD9PnDHq9o+CjyaUtoCPJo9JyK2AncB12Xv+WRE5MpY24ryYzespz4X3PfYC9Uu\nRZIkLYGyBbSU0mPAqVc13wncnx3fD3xwQfsDKaXJlNI+YC9wS7lqW2nWtTdy1w9cxmeePMSpsalq\nlyNJki5Rpe9B60spHc2OjwF92fEG4OCC8w5lbf+LiPhIRGyPiO0DAwPlq3SZ+clbL2d6NvGnTx2q\ndimSJOkSVW2SQEopAYteAj+ldF9KaVtKaVtvb28ZKluerl7byg0bO/jMk4eY/08rSZKWq0oHtOMR\nsQ4gezyRtR8GNi44rz9r0yJ8aFs/zx0b4bE9J6tdiiRJugSVDmgPA3dnx3cDDy1ovysiChGxGdgC\nPF7h2pa9O2/cwIaORn7+U08xNTNX7XIkSdJFKucyG58GvglcHRGHIuLDwCeA90XEHuC92XNSSjuA\nB4GdwBeAe1JKLo2/SC2FPD/11ssZmZzhq7tPXPgNkiSpJsVyvl9p27Ztafv27dUuo6ZMz87x5n/z\nCO+6eg3/5SduqnY5kiQpExFPppS2lXKuOwmsMPW5Ov7Wm/v5i2ePcmJkotrlSJKki2BAW4F++q2b\nmJ5N3PLxR5me9V40SZKWGwPaCrS5p5lN3U0AfOU570WTJGm5MaCtUF/4xXfS1JDjP3zhOddFkyRp\nmTGgrVDF+hzv39rHCwNjfOvFV++4JUmSapkBbQX7lTuvpz4X/MRvf4vJGVctkSRpuTCgrWDtjfX8\nzG2bAfjnf/K9KlcjSZJKZUBb4f7F7VcD8D+/e4SRiekqVyNJkkphQFvh6nN1PHTPbQD88RMHq1yN\nJEkqhQFtFbhhYwc39Lfzb/98F489P1DtciRJ0gUY0FaJX/7RrQD89O89zrBDnZIk1TQD2iqxbVMX\nv/SBawG4/6/3V7cYSZL0ugxoq8iH376ZO65by689uofDQ2erXY4kSToPA9oqEhH80o9ey+xc4rZP\nfIUz4w51SpJUiwxoq0x/ZxM//dbLAfjf/vBJt4GSJKkGGdBWof/7zuv5l3dczTdfHOR3v76v2uVI\nkqRXMaCtUj/7jit419W9/Lu/eI4dR85UuxxJkrSAAW2Vqs/V8et33URLIc8H/svXnTQgSVINMaCt\nYu2N9fxf2dIbt33iK0zPzlW5IkmSBAa0Ve9D2zZyy+YuAH7k1/+KGUOaJElVZ0ATD/7cW7llcxd7\nTozy937n28zOObNTkqRqMqAJgE//7K3csrmLx/edYtu/fcSQJklSFRnQBECuLvjjj9xKe2M9p8en\n+fHf+Lr3pEmSVCUGNL0sIvjOL7+PLWta2HFkmB/7/77uxuqSJFWBAU2vUFcXPPJPf5Bf+fHreO7Y\nCG/62JfYc3yk2mVJkrSqGND0mu5+2yb+zZ3XAfC+//wYn/vOoSpXJEnS6mFA03n91Fs38et33QjA\nP/nj7/Ifv7jbvTslSaoAA5pe1503buCv/uUPUayv4zf+ci+b7/08u4855ClJUjkZ0HRBG7ua+PTP\n3vry89t/7TH+4Jv7XdRWkqQyMaCpJDdd1sn+T3yAf/PB6wH45Yd2cNu//wp7T9ibJknSUjOgaVF+\n6tbL+e6/fj8feNM6jg9P8t5ffYw7fu0xN1uXJGkJGdC0aO2N9fzm37uZP/qHb6GlkOe5YyPc9omv\n8B++8BwnRiaqXZ4kScteLOdZedu2bUvbt2+vdhmr3v3f2M/vf2M/+06OAbDt8k4+9uPXcf2G9ipX\nJklS7YiIJ1NK20o614CmpfIH3zrAL/+PZ19+Xqyv47d/ehvv2NJbxaokSaoNBjRVzfjUDH/4rQP8\nP59/7uW2npYC16xt5QNvWsffurmfhrwj65Kk1ceAppqw98QIn33qML//1/s5Oz37cvsd163lw+/Y\nzE0bO8jnDGuSpNXBgKaaMzo5w7/7/C4+9e2XXtF+eXcT79/ax91v20S+ro6+tgIRUaUqJUkqHwOa\natbE9Cyffvwl/viJgzx3nh0Jfvj6tbz7mjW899o+GhtyFOtzFa5SkqSlZ0DTsnF6bIq/3H2C//Sl\n58+7llpbMc/bruzh0NA4/+jdW+hubmBLXyvtjfUVrlaSpItnQNOyNT41w5MHTrPv5Bj3f2M/LwyM\nlfS+H75+LddvaKetsZ6bNnbQUsjT1lhPV3NDmSuWJKk0BjStOJMzs3z+maPsPjbKl3cdZ++JUXpa\nCpwcnSzp/Q35OqZm5vcO3dTdxPqORvafHKO/s4kNnY1s6GjkhYFR/ubN/dyyuYuh8SmeOzbCNWtb\nuby7+eXPmZub/77MpeQEB0nSohjQtKqMTs7w1IHTnB6f4os7jtHbUuCJ/ac5NjzBtetaGRydYmpm\njhdPltYbt1gdTfWsa29k19Fh6gLmEvR3NnLo9Flu2dzF0weH2LKmhQhICY4MnaWnpcDa9iIDI5Oc\nnZ7l8Omz/M2bN3Dd+na+9eIgf/HsMa5a00J/ZyMbO5v4/DNH+eBNG9hzYpQta1r49r5BNnU3Mzg6\nRXtjPd0tDXQ1N7DjyDBvvryTQr6OnUeHef74CP/7D17FqbFJhsanmZlLbOxq4uToJMeHJzh46iyX\nZ4E1F3B2eo4XBka5cWMHkzNzXNHbzM4jw6zvKPLkgdM05HLcsLGdfSfHOHN2moZcHceHJ7jzxg0c\nOj3OrmMjtDfWMzuXmJqZY3Bsirdd2U1nUz27j42yubeZXARHhs4yMDrJDf0dHB4a55nDw9yyqZNc\n3fznrWsvMjo5w4aORsamZjkxMsHuYyNs7GxieGKav/+Wy0kknjowRLG+js995zARwfu39nHw1Dj5\nXLC5p4Xmhhynxqco5nNMzMzS11pk59FhWot56nN1HBgco7mQZ117kZ6WAiMTMzTk6/jrvSd544Z2\nBkYm2dDZCEAEHBmaYGNXEz0tDXx19wBf2z3Az9y2iR+8upfnjo3w1IHTtDfW09HUwFs2d3Hw1Dgz\nc4ndx0b49r5B3n5VD2en59hzfIS3XNHFkaEJpmbn+IFNnazvaOQ/P/I8KcE73tDLXz53gmvWttLb\nWqCzqYGh8SnWdzTS3dLAwMgUX9p5jJs2drCuvZHhiWmuWtPC1MwcT+w/TSFfx9D4FD2tBda0Fpme\nnePy7ia+9eIgDbk6eluLfP7Zo7x/ax/7To6xoaORrevbODo0wcjkNE0NeZ45dIbjwxPcft1aCvV1\n7DwyzJq2As8eHubA4Bi3bO7i+PAkb+pvpy6ClmKetmKePcdHGRyb4oUTo1y5poV8XXBZVxMR0NdW\n5BsvDFIXwXcPDrGlr4WNXU3sPjbCljUtzMwlmhpynBydpKelQK4uaGrIs/fEKC2FHOs7GjlyZoLO\npvne8cmZOX77sRf54E0b+OKzx/jbb+7n7PQsubqgu7nA88dHeGFglNZiPW/oa+G7B4eYTYm+1iLT\nc4k39LUwO5dICS7rbuKZQ2f4zkuneecbehkcneLw0Nns+12kuZCnsT7H7FyiuZDn8OmzDE9Ms7mn\n+eX/Jtesa6WlUM9169sAeOnUOA888RJvvaKb267qYd/JMUYmZpicmeXomQnqIhjJrt2169qYmU2c\nnZ5lTWuBA4PjfOOFQTZ0NnJZVxPDZ6eZnp3j9Pg0b+pvZ2RimqcODPGDV/cyk/3D8fTYFH1tBXYe\nGebs9CwbOhrZ3NvCziPD1OeCuZRobMiTUuKhp49wzw9dyWPPn2RyZpYta1r53qEhbtjYQU9LgZm5\nOa7oaSFXF/zZ945SrK/j1NgUh0+f5eTYFG/Z3MUb+lr57JOHyNUFJ0cn6Wxq4N3XrKGlmGd9RyNT\nM3MU6+vo72xiYGSS3ceG+ZMnD3HnjRtY315kZHKGq9a0cPj0Wda2F/mr5wdobMgzMjHN27f0sKGj\nkT96/CUGRibZ1N1MY32OPSdGWN/RSG9rgeGzM9TngmvXtdHXVuTEyAT5ujpOj09xfHiCnUeGufmy\nTtoa8/S0FDg8dJaIoLWQZ/uBU1y9to1nD59hdi5xenyKv/+Wy3nz5Z1l+XvinGUd0CLiDuDXgRzw\nOymlT5zvXAOaFmNmdo6z07OMTc4yPTvHgcFxcnXB//zeEV4cGKWvrcjg6BTrO4p87fkBjg9Pcsum\nLg6cGmN6NnFqbOo1P/dcKJMkLV+3XdXNp/7hrWX9HYsJaPmyVrJIEZEDfhN4H3AIeCIiHk4p7axu\nZVoJ8rk6WnN1tBbnJxds7GoC4K1Xdl/0Z6Y0/6/vCDgwOM7GriYGxyYZGJnkuvXtjE7OUMjXka8L\nTo1Nfb/nKV9HENQFjE3N0lzIcXZqls09zTyx/xSbuptZ01Zk74lRruxtZvuB09zQ38HTB0+zqbuZ\nY8MT9Hc2MTk9y1yanx370qlx9pwY5cduWMdXdp2gsSHHlr5WXhoco6Npvoft6JkJCvk6To9Nsamn\nmW++OMixMxPcsrmLfQNjzKbE9evb6G4p8OSB0/S1Ffna8ycAOD48yf7BMf7Je9/AqbEpNnU3sfv4\nKD0tDVyzto0HnniJde1Frl7bxvDZaY4PT9DeWE+CrFepnpbCfG9IfW7+X+Obe5oZHJ2kUJ+juSHH\nzFziwOA4N13WwfHhSeZS4mu7BxiemObGjR30thbY0tfKsTNnWdc+P0z93UNDrG0vsmVNKx1N9Xx1\n9wAjE9O89coedhw5Q1dTA31tRV48OcbmniYGx6ZICaZm5jgydJYbNnZQrM/x0uAYEUF/ZyPffGGQ\n91zbx66jwyQSs3Pzk1Ua8nWMTc6ydX0bKSVydcHj+06xsauJkYkZeloaaC7kefqlIfrai3zrxUG2\nXd7JjRs7+MYLg0xMz7K2vchvffUF/s62fvafHCcCruxt4eCpcR597gRv6GvhnVt62dTTzMDIJIn5\nYfnDp89yWXcTM7OJhnwdL50a5/PPHGVjZxNv7G+nqSHH6MQMf7z9IAMjk/zCe7dQX1fHxPQsjQ05\nUoK2xjzNhTxTM3M88MRB6nPBpu5melsLdDc3MD2bmJqd48zZac5OzdLeWM/41AxzCToa6xkYnWRq\nZo5cXTB0dpqNnU1s7GpkLsHoxPz/1p/Yf4rhiWn6WoucGp/i9Pg0DblgQ0cj7Y31FOtzTM7M0Xau\nF/PUOE/sP8UbN7TzxP5T/Py7tzA+OcPa9iJ9bUV2HR3m+PAkHU311AV85slDvP2qXp48cIqI4Jq1\nrVy/oZ0//95RDgyOUVcXrG0r8pYrujg1NsWGjkb2nRzntqu6+eYLg7Q31jM2NcupsUnqc3W8+fJO\nupob+NKO4/z5M0f5sRvWc3VfCy8MjHFydJI1rUU29zRx9MwEl3U1sePIMBu7GjkwOE6xPkdbsZ6e\n1gbq6+robmlg74lRZucSW9e3MTkzx9d2D1CszxFB9r2HPcdHaSnmaWzIQYLmQo7G+hzPHx/lpss6\nyNUFx4cneNuVPTx/fIRCPseGzka+8txxDg9NcEN/O1vXtbH3xChNDTnamxpoyAXfO3SGW6/o5pnD\nZyjW5xifmuGbLwzytiu7aSnm+drzA1zR08L7tvbx7JEzzM4mvr73JNesbeWxPSf54I0b2D84RmND\njroIrlvfxvPHRjg+MsGtm7sZnZyZv7VkbJJdR+d7PT/71CHefFkn77m2j7PTs7w0OMbhobP82A3r\n2XlkmA2djdRF0NZYz44jZzg9NkWxPsfBU+O88w29jE7M8Miu41yztpWU4ObLO+lrK/DY8ycZnphm\nZGKGtVnv2BW9LXx9z0k6mxvY1N1EW7Gez33nMH/z5g3URRABxfocb+pvZ3RihuGJGYbGp9h1dIR3\nbOmhr63I1/cOkBI0F/J856UhZufmuHptG/2djdxx/dpL/FtmadVUD1pEvBX4WErp9uz5vQAppX/3\nWufbgyZJkpaLxfSg1dpdzhuAgwueH8raXhYRH4mI7RGxfWBgoKLFSZIkVUKtBbQLSindl1LallLa\n1tvrJtySJGnlqbWAdhjYuOB5f9YmSZK0atRaQHsC2BIRmyOiAbgLeLjKNUmSJFVUTc3iTCnNRMTP\nA19kfpmN30sp7ahyWZIkSRVVUwENIKX0eeDz1a5DkiSpWmptiFOSJGnVM6BJkiTVGAOaJElSjTGg\nSZIk1RgDmiRJUo0xoEmSJNUYA5okSVKNMaBJkiTVGAOaJElSjTGgSZIk1RgDmiRJUo0xoEmSJNWY\nSClVu4aLFhEDwIEK/Koe4GQFfo8Wz2tT27w+tctrU9u8PrXtYq/P5Sml3lJOXNYBrVIiYntKaVu1\n69D/ymtT27w+tctrU9u8PrWtEtfHIU5JkqQaY0CTJEmqMQa00txX7QJ0Xl6b2ub1qV1em9rm9alt\nZb8+3oMmSZJUY+xBkyRJqjEGNEmSpBpjQHsdEXFHROyOiL0R8dFq17NaRcT+iHgmIp6OiO1ZW1dE\nPBIRe7LHzgXn35tds90RcXv1Kl95IuL3IuJERDy7oG3R1yIi3pxd070R8V8iIir9Z1mJznN9PhYR\nh7Pvz9MR8SMLXvP6VEhEbIyIv4yInRGxIyJ+IWv3+1MDXuf6VO/7k1Ly5zV+gBzwAnAF0AB8F9ha\n7bpW4w+wH+h5Vdt/AD6aHX8U+PfZ8dbsWhWAzdk1zFX7z7BSfoB3AjcDz17KtQAeB24FAvgL4Ier\n/WdbCT/nuT4fA/75a5zr9anstVkH3JwdtwLPZ9fA708N/LzO9ana98cetPO7BdibUnoxpTQFPADc\nWeWa9H13Avdnx/cDH1zQ/kBKaTKltA/Yy/y11BJIKT0GnHpV86KuRUSsA9pSSt9K8/9v9t8XvEeX\n4DzX53y8PhWUUjqaUnoqOx4BdgEb8PtTE17n+pxP2a+PAe38NgAHFzw/xOtfLJVPAr4cEU9GxEey\ntr6U0tHs+BjQlx173SpvsddiQ3b86naVzz+KiO9lQ6DnhtC8PlUSEZuAm4Bv4/en5rzq+kCVvj8G\nNC0Hb08p3Qj8MHBPRLxz4YvZv1JcL6YGeC1q0m8xf6vGjcBR4D9Vt5zVLSJagM8Cv5hSGl74mt+f\n6nuN61O1748B7fwOAxsXPO/P2lRhKaXD2eMJ4HPMD1kez7qSyR5PZKd73SpvsdficHb86naVQUrp\neEppNqU0B/w23x/y9/pUWETUM/+X/6dSSn+aNfv9qRGvdX2q+f0xoJ3fE8CWiNgcEQ3AXcDDVa5p\n1YmI5ohoPXcMvB94lvlrcXd22t3AQ9nxw8BdEVGIiM3AFuZv2FT5LOpaZMM5wxFxaza76acXvEdL\n7Nxf/pm/wfz3B7w+FZX9t/xdYFdK6VcXvOT3pwac7/pU8/uTv5g3rQYppZmI+Hngi8zP6Py9lNKO\nKpe1GvUBn8tmKeeBP0opfSEingAejIgPAweADwGklHZExIPATmAGuCelNFud0leeiPg08C6gJyIO\nAf8a+ASLvxb/B/D7QCPzs5z+ooJ/jBXrPNfnXRFxI/NDZ/uBnwOvTxXcBvwU8ExEPJ21/Sv8/tSK\n812fn6jW98etniRJkmqMQ5ySJEk1xoAmSZJUYwxokiRJNcaAJkmSVGMMaJIkSTXGgCZpxYqId0XE\nn1W7joUiYlNEPHvhMyWtZgY0SVpGIsL1K6VVwIAmqaoi4icj4vGIeDoi/mtE5LL20Yj4zxGxIyIe\njYjerP3GiPhWtnnx585tXhwRV0XElyPiuxHxVERcmf2Kloj4TEQ8FxGfylb3JiI+ERE7s8/5j69R\n18eyzZG/GhEvRsQ/ztpf0QMWEf88Ij6WHX81q3l7ROyKiB+IiD+NiD0R8W8XfHw+q2VXVltT9v43\nR8TXIuLJiPjigi2AvhoRvxYR24FfWNorIKkWGdAkVU1EXAv8XeC2lNKNwCzw97OXm4HtKaXrgK8x\nvyo+wH8H/s+U0puAZxa0fwr4zZTSDcDbmN/YGOAm4BeBrcxvenxbRHQzv23LddnnLAxPC10D3M78\n/nv/Otur70KmUkr/f3v3E2JjFMZx/PtjmFEjIrNR2EmUxUxTFjZsLSyuzdSErZFip6yUvSjGwpSy\nkSILElkoSmOjNGIzJWNhNv7XjMn8LN5zm5sMM7d0L/P7rN573nue97x3cXt6zqmnDximavEyBOwA\nDpXnAmwFLtjeBnwCjpTY54Ga7V5gBDjTEHel7T7baXYesQSkVB4RrbQX6AWelsLWKuaaRc8C18r1\nVeCGpDXAWtsPy/gV4Hrp17rR9k0A21MAJeao7Yny+RmwBXgCTAGXyxm1+c6p3bY9DUxLmqRqPfYn\n9Z69z4Gx0psPSeNUzZU/AG9sP254t2PAXapE7n5Z93LmkkwafouIWAKSoEVEKwm4YvvkAr7bbF+6\n6Ybr70BH6bXbT5Ug1oCjwJ6FzKXqu9e4+9A1z5zZn+bPMvef+/O7mOq3GLO9a573+DrPeET8h7LF\nGRGt9ACoSeoBkLRO0uZybxlV8gQwADyy/RF4L2l3GR8EHtr+DExI2l/idNbPdf2KpG5gje07wHFg\n5yLW/A7okbReUiewbxFz6zZJqidiA8Aj4BWwoT4uaYWk7U3Ejoj/QCpoEdEytl9IOgXck7QMmKE6\ns/WaqmLUX+5PUp1VAzgIDJcEbBw4XMYHgUuSTpc4B37z6NXALUldVJWrE4tY80x5xijwFni50LkN\nXgFDkkaAF8BF298k1YBzZSu3AzgLjDURPyL+cbKb3TWIiPh7JH2x3d3qdUREtEK2OCMiIiLaTCpo\nEREREW0mFbSIiIiINpMELSIiIqLNJEGLiIiIaDNJ0CIiIiLaTBK0iIiIiDbzA9rgfekF/i+MAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c1f009da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valuation on test-set acc = 36.01%\n"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.xlabel('epochs number')\n",
    "plt.ylabel('loss value')\n",
    "plt.show()\n",
    "\n",
    "scores = svm_np(X_test, None, W, reg)\n",
    "acc = np.mean(np.argmax(scores, axis=1) == y_test)\n",
    "print('\\nValuation on test-set acc = {:5.2f}%'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with TensorFlow\n",
    "Now, let's try SGD with TensorFlow which is very convenient since SGD is already implemented in SGD: [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_tf(initW, train_data, val_data, reg, epochs, learning_rate = 1.0e-3, print_every = 20):\n",
    "    # setup input\n",
    "    train_idx = tf.constant(np.arange(train_data.batch_size()))\n",
    "    val_idx = tf.constant(np.arange(val_data.batch_size()))\n",
    "    vX = tf.placeholder(tf.float32, [None, D])\n",
    "    vy = tf.placeholder(tf.int64, [None])\n",
    "    vreg = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # trainable variable\n",
    "    vW = tf.Variable(initW.astype(np.float32), name = 'W')\n",
    "    \n",
    "    # cost & acc\n",
    "    cost, _, _ = svm_tf(vX, vy, vW, vreg, train_idx)\n",
    "    _, _, val_acc = svm_tf(vX, vy, vW, vreg, val_idx)\n",
    "    \n",
    "    # setup optimizer\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    nb_iters = train_data.get_nb_iters(epochs)\n",
    "    loss_history = []    \n",
    "    opt_W = None\n",
    "    with tf.Session() as sess:\n",
    "        # init out variable\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # training loop\n",
    "        start = time.time()\n",
    "        for i in range(1, nb_iters + 1):\n",
    "            X_batch, y_batch = train_data.next_batch()\n",
    "            loss, _ = sess.run([cost, train_op], feed_dict= {vX : X_batch,\n",
    "                                                             vy : y_batch,\n",
    "                                                             vreg : reg})\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            # log current state        \n",
    "            if (i % print_every == 0):                    \n",
    "                print('Iter {:>10d}/{:<10d} loss {:10.4f}'.format(i, nb_iters, loss))\n",
    "\n",
    "\n",
    "            epoch_end, epoch = train_data.is_epoch_end(i)\n",
    "            if (epoch_end) or (i == 1):\n",
    "                # validation it here\n",
    "                if val_data is not None:\n",
    "                    X_val, y_val = val_data.next_batch()\n",
    "                    \n",
    "                    acc = sess.run(val_acc, feed_dict= { vX : X_val,\n",
    "                                                         vy : y_val,\n",
    "                                                         vreg : reg})\n",
    "                    \n",
    "                    print('\\nEpoch {:>3d}/{:<3d} val_acc = {:5.2f}%'.format(epoch, epochs, 100 * acc))\n",
    "        \n",
    "        print ('\\nTrain time: {:<10.2f} seconds'.format(time.time() - start))\n",
    "        \n",
    "        opt_W = sess.run(vW)\n",
    "    return opt_W, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   0/10  val_acc =  9.00%\n",
      "Iter        100/2450       loss   287.1926\n",
      "Iter        200/2450       loss   107.3475\n",
      "\n",
      "Epoch   1/10  val_acc = 32.40%\n",
      "Iter        300/2450       loss    42.8403\n",
      "Iter        400/2450       loss    19.0037\n",
      "\n",
      "Epoch   2/10  val_acc = 35.10%\n",
      "Iter        500/2450       loss    10.1583\n",
      "Iter        600/2450       loss     6.9216\n",
      "Iter        700/2450       loss     6.4267\n",
      "\n",
      "Epoch   3/10  val_acc = 37.50%\n",
      "Iter        800/2450       loss     5.1702\n",
      "Iter        900/2450       loss     5.5196\n",
      "\n",
      "Epoch   4/10  val_acc = 37.80%\n",
      "Iter       1000/2450       loss     4.7932\n",
      "Iter       1100/2450       loss     5.0509\n",
      "Iter       1200/2450       loss     5.5003\n",
      "\n",
      "Epoch   5/10  val_acc = 37.40%\n",
      "Iter       1300/2450       loss     5.1796\n",
      "Iter       1400/2450       loss     5.4393\n",
      "\n",
      "Epoch   6/10  val_acc = 38.50%\n",
      "Iter       1500/2450       loss     5.3376\n",
      "Iter       1600/2450       loss     5.2767\n",
      "Iter       1700/2450       loss     5.4552\n",
      "\n",
      "Epoch   7/10  val_acc = 39.10%\n",
      "Iter       1800/2450       loss     5.4867\n",
      "Iter       1900/2450       loss     5.4599\n",
      "\n",
      "Epoch   8/10  val_acc = 36.60%\n",
      "Iter       2000/2450       loss     4.7786\n",
      "Iter       2100/2450       loss     5.4999\n",
      "Iter       2200/2450       loss     5.1709\n",
      "\n",
      "Epoch   9/10  val_acc = 37.90%\n",
      "Iter       2300/2450       loss     5.0242\n",
      "Iter       2400/2450       loss     5.1785\n",
      "\n",
      "Epoch  10/10  val_acc = 38.00%\n",
      "\n",
      "Train time: 9.46       seconds\n"
     ]
    }
   ],
   "source": [
    "# test TensorFlow's SGD\n",
    "lr = 1e-7\n",
    "reg = 5e4\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "\n",
    "train_data = Dataset(X_train, y_train, batch_size, dtype = np.float32)\n",
    "val_data = Dataset(X_val, y_val, 1000, dtype = np.float32)\n",
    "\n",
    "np.random.seed(2793)\n",
    "\n",
    "W_tf, loss_hist_tf = sgd_tf(initW, train_data, val_data, reg, epochs, \n",
    "                      learning_rate=lr, print_every=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAHjCAYAAACXcOPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c4c7e4860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valuation on test-set acc = 36.01%\n"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_hist_tf)\n",
    "plt.xlabel('epochs number')\n",
    "plt.ylabel('loss value')\n",
    "plt.show()\n",
    "\n",
    "scores = svm_np(X_test, None, W_tf, reg)\n",
    "acc = np.mean(np.argmax(scores, axis=1) == y_test)\n",
    "print('\\nValuation on test-set acc = {:5.2f}%'.format(100 * acc))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We have implemented Multiclass SVM Linear Classifier using Numpy and TensorFlow, they both give similar result. However due to the difficulty while accessing array's index it seems that TensorFlows is 3 times slower than Numpy. Notice that the svm loss function is  implemented by us so it might not as optimal as it should be. We hope TensorFlow will improve array indexing in the next release and also implement more loss function (they seem only implement softmax)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
