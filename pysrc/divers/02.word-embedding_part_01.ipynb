{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of numbers. However, natural language processing systems traditionally treat words a discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are very sparse and provide no useful information regarding the relationships that may exist between the individual symbols. \n",
    "\n",
    "Vector space models represent words in a continuous vector space where semantically similar words are mapped to nearby points (are embedded nearby each other). In this series of notebook, we look at few word embedding techniques and compare them:\n",
    "\n",
    "* Skip-gram with [Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* Glove: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) and more resource from [here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "# Skip-gram \n",
    "Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of traning words $w_1,w_2,\\ldots,w_T$, the objective of the Skip-gram model is to maximize the average log probability\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq 0}\\log p(w_{t+j}|w_t)\n",
    "$$\n",
    "The Skip-gram defines $p(w_{t+j}|w_t)$ using the softmax function\n",
    "$$\n",
    "p(w_{o}|w_{i}) = \\frac{\\exp\\left(u_{w_o}^Tv_{w_{i}}\\right)}{\\sum_{w=1}^V\\exp\\left(u_w^Tv_{w_{i}}\\right)}\n",
    "$$\n",
    "where $V$ is size of vocabulary and\n",
    "* $w_o$ is output word (outside word or surrounding word)\n",
    "* $w_i$ is input word (context word or center word)\n",
    "* $u_w$ is output vector representation\n",
    "* $v_w$ is input vector representation\n",
    "\n",
    "This formulation is impractical because the cost of computing the denominator is $O(V)$ where $V$ is often large ($10^5-10^7$).\n",
    "\n",
    "# Skip-gram with Negative sampling\n",
    "Mikolov et al. introduce one effecient technique so called Negative sampling (NEG). The NEG re-define the objective as\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}}\\right) + \\sum_{i=1}^k \\mathbb{E}_{j_i\\sim P_n(w)}\\log\\sigma\\left(-u_{j_i}^Tv_{w_{i}}\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "P_n(w) = U(w)^{3/4}/Z\n",
    "$$\n",
    "the unigram distribution $U(w)$ raised to the 3/4 power (then normalized by $Z$). The power 3/4 makes less frequent words be sampled more often.\n",
    "\n",
    "The idea here is to\n",
    "* maximize the probability that real outside word $w_o$ appears around center word $w_i$\n",
    "* minimize the probability that random words $j_i$ appears around center word $w_i$\n",
    "\n",
    "## Implementation planning\n",
    "Before doing the implementation, we list the required steps\n",
    "0. Choose dataset: \n",
    "    * which corpus to be used for training\n",
    "    * which test-set to be used for testing\n",
    "1. Pre-processing raw_tex:\n",
    "    * extract a set of all words (vocab)\n",
    "    * map vocab <-> integer id\n",
    "    * compute words-frequence (we might sub-sampling to remove some frequent words such as 'the,a,an,...e.t.c'), we also need the words-frequence to compute $P_n(w)$\n",
    "    * convert raw text to list of words-ids\n",
    "2. Ensemble a graph:\n",
    "    * Define inputs, targets: must take into account of mini-batches\n",
    "    * Define trainable variables\n",
    "    * Define a loss function with neg-sampling\n",
    "    * Define an optimizer (might need to apply some Gradient-Clipping technique)\n",
    "3. Training:\n",
    "    * How to feed inputs/targets data\n",
    "    * How to measure training performance\n",
    "    * How to tune hyper-parameters\n",
    "4. Evaluation:\n",
    "    * How to measure word2vec quality (hard)\n",
    "    \n",
    "## Choose dataset\n",
    "We use cleaned wiki-dataset from Matt Mahoney's [website](http://mattmahoney.net/dc/textdata.html):\n",
    "* [text8](http://mattmahoney.net/dc/text8.zip) is small dataset (100Mb) \n",
    "* [enwiki9](http://mattmahoney.net/dc/enwik9.zip) is bigger dataset (1Gb)\n",
    "\n",
    "We use the same script in Matt Mahoney to create text9 data from enwik9.\n",
    "\n",
    "First we load module for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, the text data is clean text (i.e no punctuation, no new line), let's view first 100 characters of our text-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "text_file = '/home/minhvu/workplaces/tf_datas/nltk/text8'\n",
    "preprocess_file = '/home/minhvu/workplaces/tf_datas/nltk/text8.pkl'\n",
    "with open(text_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    print (text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "We code pre-processing into **Word2VecInput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing took 21.26 seconds\n"
     ]
    }
   ],
   "source": [
    "from nlp.preprocess_input import Word2VecInput\n",
    "\n",
    "ts = time()\n",
    "w2v_input = Word2VecInput(text_file)\n",
    "print ('Pre-processing took {:.2f} seconds'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pre-processing took quite a long time, we dump pre-processed data into a pickled file which includes vocabs, word2id, id2word, word-frequences and trained_wordids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump pre-processing data to file\n",
    "w2v_input.dump(preprocess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble a graph\n",
    "We need to define inputs and targets, re-call that the Skip-gram model is to predict surrounding words given a center word so input will be center word and targets will be surrounding-words. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"750\"\n",
       "            src=\"./skipgram-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc0e253208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('./skipgram-demo/index.html', width=500, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the input/target can be defined by tf.placeholder of tf.int32 to represent word-id (integer), the tricky part is to \n",
    "* define embeding layer\n",
    "* define sampling procedure\n",
    "* define loss function\n",
    "\n",
    "### Embedding layer\n",
    "For each word $w$, we have two embedding layers $u_w$ and $v_w$ with embedding-dimension $D$, we can model it as follow\n",
    "* $v_w$ is input embedding-weight\n",
    "* $u_w$ is output softmax-weight\n",
    "\n",
    "We can define embedding-weight and softmax-weight as [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) with shape $[V,D]$. \n",
    "\n",
    "Note that for embedding-weight $u_w$ we often initialized by random-uniform between [-1,1], while $v_w$ is initialized by truncated-normal with $\\sigma=\\frac{1.0}{\\sqrt{D}}$.\n",
    "\n",
    "Note that since $V$ can be very large, we need a way to look-up $u_w, v_w$, this can be done via [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup).\n",
    "\n",
    "### Sampling procedure\n",
    "The sampling method is tricky since $V$ can be very large. Fortunately, Tensorflow has implemented various [candidate-sampling](https://www.tensorflow.org/api_guides/python/nn#Candidate_Sampling). Here we will use\n",
    "* [tf.nn.fixed_unigram_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/nn/fixed_unigram_candidate_sampler): to sample $P_n(w)$ as described above\n",
    "* [https://www.tensorflow.org/api_docs/python/tf/nn/log_uniform_candidate_sampler]: to sample log-uniform, note this should be used only our words is sorted with decreasing frequence\n",
    "\n",
    "### Loss function\n",
    "Tensorflow has already implemented (see [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py) for implementation)\n",
    "* [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss): sampled softmax training loss\n",
    "* [tf.nn.nce_loss](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss): sampled logistic training loss\n",
    "\n",
    "Let's review the implementation of the two above loss function\n",
    "* Sampled-softmax compute\n",
    "$$\n",
    "-\\log\\left(\\frac{\\exp(u_{w_o}^Tv_{w_{i}})}{\\exp(u_{w_o}^Tv_{w_{i}}) + \\sum_{i=1}^k \\exp(u_{j_i}^Tv_{w_{i}})} \\right) \n",
    "$$\n",
    "via [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* Sampled-logistic compute\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}})\\right) + \\sum_{i=1}^k \\log \\sigma(-u_{j_i}^Tv_{w_{i}})\n",
    "$$\n",
    "via [tf.nn.sigmoid_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "We implement above steps inside **`Word2vecSamping`** object and add training to it.\n",
    "\n",
    "## Training\n",
    "Let build a word2vec model using **`Word2vecSampling`**\n",
    "and training it with pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocabs, word2id, id2word, freqs, train_wordids = pickle.load(open(preprocess_file, 'rb'))\n",
    "from nlp.word2vec import Word2vecSampling\n",
    "\n",
    "w2v_model = Word2vecSampling(vocabs, word2id, id2word, freqs, train_wordids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "settings = {'embed_dim'       : 200,\n",
    "            'nb_neg_sample'   : 100,\n",
    "            'learning_rate'   : 0.01,\n",
    "            'sampling_method' : 'neg',\n",
    "            'loss_func'       : 'neg'}\n",
    "\n",
    "w2v_model.build_graph(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create checkpoints to save training-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) Batch (  100/4520 ) Iteration:      100 Avg. Training loss: 492.3835 0.0718 sec/batch\n",
      "Epoch (1/1) Batch (  200/4520 ) Iteration:      200 Avg. Training loss: 426.0508 0.0709 sec/batch\n",
      "Epoch (1/1) Batch (  300/4520 ) Iteration:      300 Avg. Training loss: 364.2723 0.0716 sec/batch\n",
      "Epoch (1/1) Batch (  400/4520 ) Iteration:      400 Avg. Training loss: 322.1343 0.0710 sec/batch\n",
      "Epoch (1/1) Batch (  500/4520 ) Iteration:      500 Avg. Training loss: 294.0232 0.0707 sec/batch\n",
      "Epoch (1/1) Batch (  600/4520 ) Iteration:      600 Avg. Training loss: 262.0385 0.0710 sec/batch\n",
      "Epoch (1/1) Batch (  700/4520 ) Iteration:      700 Avg. Training loss: 236.6775 0.0710 sec/batch\n",
      "Epoch (1/1) Batch (  800/4520 ) Iteration:      800 Avg. Training loss: 224.3931 0.0704 sec/batch\n",
      "Epoch (1/1) Batch (  900/4520 ) Iteration:      900 Avg. Training loss: 210.3310 0.0710 sec/batch\n",
      "Epoch (1/1) Batch ( 1000/4520 ) Iteration:     1000 Avg. Training loss: 192.5921 0.0710 sec/batch\n",
      "Epoch (1/1) Batch ( 1100/4520 ) Iteration:     1100 Avg. Training loss: 183.6749 0.0774 sec/batch\n",
      "Epoch (1/1) Batch ( 1200/4520 ) Iteration:     1200 Avg. Training loss: 174.1730 0.0709 sec/batch\n",
      "Epoch (1/1) Batch ( 1300/4520 ) Iteration:     1300 Avg. Training loss: 157.5479 0.0704 sec/batch\n",
      "Epoch (1/1) Batch ( 1400/4520 ) Iteration:     1400 Avg. Training loss: 155.7375 0.0711 sec/batch\n",
      "Epoch (1/1) Batch ( 1500/4520 ) Iteration:     1500 Avg. Training loss: 142.5464 0.0718 sec/batch\n",
      "Epoch (1/1) Batch ( 1600/4520 ) Iteration:     1600 Avg. Training loss: 140.7809 0.0720 sec/batch\n",
      "Epoch (1/1) Batch ( 1700/4520 ) Iteration:     1700 Avg. Training loss: 128.5473 0.0723 sec/batch\n",
      "Epoch (1/1) Batch ( 1800/4520 ) Iteration:     1800 Avg. Training loss: 124.0998 0.0737 sec/batch\n",
      "Epoch (1/1) Batch ( 1900/4520 ) Iteration:     1900 Avg. Training loss: 124.4296 0.0727 sec/batch\n",
      "Epoch (1/1) Batch ( 2000/4520 ) Iteration:     2000 Avg. Training loss: 114.0489 0.0712 sec/batch\n",
      "Epoch (1/1) Batch ( 2100/4520 ) Iteration:     2100 Avg. Training loss: 110.3582 0.0790 sec/batch\n",
      "Epoch (1/1) Batch ( 2200/4520 ) Iteration:     2200 Avg. Training loss: 104.8319 0.0729 sec/batch\n",
      "Epoch (1/1) Batch ( 2300/4520 ) Iteration:     2300 Avg. Training loss: 98.6611 0.0720 sec/batch\n",
      "Epoch (1/1) Batch ( 2400/4520 ) Iteration:     2400 Avg. Training loss: 100.9342 0.0718 sec/batch\n",
      "Epoch (1/1) Batch ( 2500/4520 ) Iteration:     2500 Avg. Training loss: 94.2075 0.0735 sec/batch\n",
      "Epoch (1/1) Batch ( 2600/4520 ) Iteration:     2600 Avg. Training loss: 88.5722 0.0717 sec/batch\n",
      "Epoch (1/1) Batch ( 2700/4520 ) Iteration:     2700 Avg. Training loss: 90.8537 0.0720 sec/batch\n",
      "Epoch (1/1) Batch ( 2800/4520 ) Iteration:     2800 Avg. Training loss: 86.9595 0.0711 sec/batch\n",
      "Epoch (1/1) Batch ( 2900/4520 ) Iteration:     2900 Avg. Training loss: 76.1409 0.0715 sec/batch\n",
      "Epoch (1/1) Batch ( 3000/4520 ) Iteration:     3000 Avg. Training loss: 81.7754 0.0714 sec/batch\n",
      "Epoch (1/1) Batch ( 3100/4520 ) Iteration:     3100 Avg. Training loss: 78.6951 0.0774 sec/batch\n",
      "Epoch (1/1) Batch ( 3200/4520 ) Iteration:     3200 Avg. Training loss: 71.7983 0.0726 sec/batch\n",
      "Epoch (1/1) Batch ( 3300/4520 ) Iteration:     3300 Avg. Training loss: 71.8450 0.0720 sec/batch\n",
      "Epoch (1/1) Batch ( 3400/4520 ) Iteration:     3400 Avg. Training loss: 72.9603 0.0721 sec/batch\n",
      "Epoch (1/1) Batch ( 3500/4520 ) Iteration:     3500 Avg. Training loss: 67.6524 0.0725 sec/batch\n",
      "Epoch (1/1) Batch ( 3600/4520 ) Iteration:     3600 Avg. Training loss: 63.7079 0.0720 sec/batch\n",
      "Epoch (1/1) Batch ( 3700/4520 ) Iteration:     3700 Avg. Training loss: 65.9114 0.0723 sec/batch\n",
      "Epoch (1/1) Batch ( 3800/4520 ) Iteration:     3800 Avg. Training loss: 67.0201 0.0710 sec/batch\n",
      "Epoch (1/1) Batch ( 3900/4520 ) Iteration:     3900 Avg. Training loss: 64.9610 0.0707 sec/batch\n",
      "Epoch (1/1) Batch ( 4000/4520 ) Iteration:     4000 Avg. Training loss: 53.7202 0.0713 sec/batch\n",
      "Epoch (1/1) Batch ( 4100/4520 ) Iteration:     4100 Avg. Training loss: 59.7622 0.0776 sec/batch\n",
      "Epoch (1/1) Batch ( 4200/4520 ) Iteration:     4200 Avg. Training loss: 55.6714 0.0717 sec/batch\n",
      "Epoch (1/1) Batch ( 4300/4520 ) Iteration:     4300 Avg. Training loss: 52.3057 0.0721 sec/batch\n",
      "Epoch (1/1) Batch ( 4400/4520 ) Iteration:     4400 Avg. Training loss: 57.7055 0.0718 sec/batch\n",
      "Epoch (1/1) Batch ( 4500/4520 ) Iteration:     4500 Avg. Training loss: 54.1081 0.0711 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 1024\n",
    "window_size = 5\n",
    "w2v_model.train(epochs, batch_size, window_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
