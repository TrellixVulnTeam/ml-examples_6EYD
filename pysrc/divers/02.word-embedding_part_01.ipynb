{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of numbers. However, natural language processing systems traditionally treat words a discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are very sparse and provide no useful information regarding the relationships that may exist between the individual symbols. \n",
    "\n",
    "Vector space models represent words in a continuous vector space where semantically similar words are mapped to nearby points (are embedded nearby each other). In this series of notebook, we look at few word embedding techniques and compare them:\n",
    "\n",
    "* Skip-gram with [Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* Glove: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) and more resource from [here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "# Skip-gram \n",
    "Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of traning words $w_1,w_2,\\ldots,w_T$, the objective of the Skip-gram model is to maximize the average log probability\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq 0}\\log p(w_{t+j}|w_t)\n",
    "$$\n",
    "The Skip-gram defines $p(w_{t+j}|w_t)$ using the softmax function\n",
    "$$\n",
    "p(w_{o}|w_{i}) = \\frac{\\exp\\left(u_{w_o}^Tv_{w_{i}}\\right)}{\\sum_{w=1}^V\\exp\\left(u_w^Tv_{w_{i}}\\right)}\n",
    "$$\n",
    "where $V$ is size of vocabulary and\n",
    "* $w_o$ is output word (outside word or surrounding word)\n",
    "* $w_i$ is input word (context word or center word)\n",
    "* $u_w$ is output vector representation\n",
    "* $v_w$ is input vector representation\n",
    "\n",
    "This formulation is impractical because the cost of computing the denominator is $O(V)$ where $V$ is often large ($10^5-10^7$).\n",
    "\n",
    "# Skip-gram with Negative sampling\n",
    "Mikolov et al. introduce one effecient technique so called Negative sampling (NEG). The NEG re-define the objective as\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}}\\right) + \\sum_{i=1}^k \\mathbb{E}_{j_i\\sim P_n(w)}\\log\\sigma\\left(-u_{j_i}^Tv_{w_{i}}\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "P_n(w) = U(w)^{3/4}/Z\n",
    "$$\n",
    "the unigram distribution $U(w)$ raised to the 3/4 power (then normalized by $Z$). The power 3/4 makes less frequent words be sampled more often.\n",
    "\n",
    "The idea here is to\n",
    "* maximize the probability that real outside word $w_o$ appears around center word $w_i$\n",
    "* minimize the probability that random words $j_i$ appears around center word $w_i$\n",
    "\n",
    "## Implementation planning\n",
    "Before doing the implementation, we list the required steps\n",
    "0. Choose dataset: \n",
    "    * which corpus to be used for training\n",
    "    * which test-set to be used for testing\n",
    "1. Pre-processing raw_tex:\n",
    "    * extract a set of all words (vocab)\n",
    "    * map vocab <-> integer id\n",
    "    * compute words-frequence (we might sub-sampling to remove some frequent words such as 'the,a,an,...e.t.c'), we also need the words-frequence to compute $P_n(w)$\n",
    "    * convert raw text to list of words-ids\n",
    "2. Ensemble a graph:\n",
    "    * Define inputs, targets: must take into account of mini-batches\n",
    "    * Define trainable variables\n",
    "    * Define a loss function with neg-sampling\n",
    "    * Define an optimizer (might need to apply some Gradient-Clipping technique)\n",
    "3. Training:\n",
    "    * How to feed inputs/targets data\n",
    "    * How to measure training performance\n",
    "    * How to tune hyper-parameters\n",
    "4. Evaluation:\n",
    "    * How to measure word2vec quality (hard)\n",
    "    \n",
    "## Choose dataset\n",
    "We use cleaned wiki-dataset from Matt Mahoney's [website](http://mattmahoney.net/dc/textdata.html):\n",
    "* [text8](http://mattmahoney.net/dc/text8.zip) is small dataset (100Mb) \n",
    "* [enwiki9](http://mattmahoney.net/dc/enwik9.zip) is bigger dataset (1Gb)\n",
    "\n",
    "We use the same script in Matt Mahoney to create text9 data from enwik9.\n",
    "\n",
    "First we load module for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, the text data is clean text (i.e no punctuation, no new line), let's view first 100 characters of our text-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "text_file = '/home/%s/workplaces/tf_datas/nltk/text8' % getpass.getuser()\n",
    "preprocess_file = '/home/%s/workplaces/tf_datas/nltk/text8.pkl' % getpass.getuser()\n",
    "with open(text_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    print (text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "We code pre-processing into **Word2VecInput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing took 20.42 seconds\n"
     ]
    }
   ],
   "source": [
    "from nlp.preprocess_input import Word2VecInput\n",
    "\n",
    "ts = time()\n",
    "w2v_input = Word2VecInput(text_file)\n",
    "print ('Pre-processing took {:.2f} seconds'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pre-processing took quite a long time, we dump pre-processed data into a pickled file which includes vocabs, word2id, id2word, word-frequences and trained_wordids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump pre-processing data to file\n",
    "w2v_input.dump(preprocess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble a graph\n",
    "We need to define inputs and targets, re-call that the Skip-gram model is to predict surrounding words given a center word so input will be center word and targets will be surrounding-words. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"750\"\n",
       "            src=\"./skipgram-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd19ebd1160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('./skipgram-demo/index.html', width=500, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the input/target can be defined by tf.placeholder of tf.int32 to represent word-id (integer), the tricky part is to \n",
    "* define embeding layer\n",
    "* define sampling procedure\n",
    "* define loss function\n",
    "\n",
    "### Embedding layer\n",
    "For each word $w$, we have two embedding layers $u_w$ and $v_w$ with embedding-dimension $D$, we can model it as follow\n",
    "* $v_w$ is input embedding-weight\n",
    "* $u_w$ is output softmax-weight\n",
    "\n",
    "We can define embedding-weight and softmax-weight as [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) with shape $[V,D]$. \n",
    "\n",
    "Note that for embedding-weight $u_w$ we often initialized by random-uniform between [-1,1], while $v_w$ is initialized by truncated-normal with $\\sigma=\\frac{1.0}{\\sqrt{D}}$.\n",
    "\n",
    "Note that since $V$ can be very large, we need a way to look-up $u_w, v_w$, this can be done via [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup).\n",
    "\n",
    "### Sampling procedure\n",
    "The sampling method is tricky since $V$ can be very large. Fortunately, Tensorflow has implemented various [candidate-sampling](https://www.tensorflow.org/api_guides/python/nn#Candidate_Sampling). Here we will use\n",
    "* [tf.nn.fixed_unigram_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/nn/fixed_unigram_candidate_sampler): to sample $P_n(w)$ as described above\n",
    "* [https://www.tensorflow.org/api_docs/python/tf/nn/log_uniform_candidate_sampler]: to sample log-uniform, note this should be used **only if our words is sorted with decreasing frequence**\n",
    "\n",
    "### Loss function\n",
    "Tensorflow has already implemented (see [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py) for implementation)\n",
    "* [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss): sampled softmax training loss\n",
    "* [tf.nn.nce_loss](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss): sampled logistic training loss\n",
    "\n",
    "Let's review the implementation of the two above loss function\n",
    "* Sampled-softmax compute\n",
    "$$\n",
    "-\\log\\left(\\frac{\\exp(u_{w_o}^Tv_{w_{i}})}{\\exp(u_{w_o}^Tv_{w_{i}}) + \\sum_{i=1}^k \\exp(u_{j_i}^Tv_{w_{i}})} \\right) \n",
    "$$\n",
    "via [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* Sampled-logistic compute\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}})\\right) + \\sum_{i=1}^k \\log \\sigma(-u_{j_i}^Tv_{w_{i}})\n",
    "$$\n",
    "via [tf.nn.sigmoid_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "We implement above steps inside **`Word2vecSamping`** object and add training to it.\n",
    "\n",
    "## Training\n",
    "Let build a word2vec model using **`Word2vecSampling`**\n",
    "and training it with pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocabs, word2id, id2word, freqs, train_wordids = pickle.load(open(preprocess_file, 'rb'))\n",
    "from nlp.word2vec import Word2vecSampling\n",
    "\n",
    "w2v_model = Word2vecSampling(vocabs, word2id, id2word, freqs, train_wordids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "settings = {'embed_dim'       : 200,\n",
    "            'nb_neg_sample'   : 100,\n",
    "            'learning_rate'   : 0.01,\n",
    "            'sampling_method' : 'fixed_unigram',\n",
    "            'loss_func'       : 'nce',\n",
    "            'subtract_log_q'  : True,\n",
    "            'use_tf_loss'     : False}\n",
    "\n",
    "w2v_model.build_graph(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create checkpoints to save training-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints\n",
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 1024\n",
    "window_size = 5\n",
    "w2v_model.train(epochs, batch_size, window_size, max_iters = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tunning\n",
    "In this section we summarize some result for training word2vec. We have the following method\n",
    "\n",
    "| Sampling method | Loss function   | \n",
    "| :-------------: |:---------------:| \n",
    "| fixed unigram   | sampled softmax | \n",
    "| log_uniform     | nce             |\n",
    "\n",
    "Note that, by-default `tf.nn.sampled_softmax_loss` and `tf.nn.nce` use sampling-method `log_uniform`. We want to make our test close with original papers so we will use `fixed_unigram` as default.\n",
    "\n",
    "We test with the following \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/5) Batch (  100/4518 ) Iteration:      100 Avg. Training loss: 479.1447 0.1214 sec/batch\n",
      "Epoch (1/5) Batch (  200/4518 ) Iteration:      200 Avg. Training loss: 413.2743 0.1208 sec/batch\n",
      "Epoch (1/5) Batch (  300/4518 ) Iteration:      300 Avg. Training loss: 373.8611 0.1212 sec/batch\n",
      "Epoch (1/5) Batch (  400/4518 ) Iteration:      400 Avg. Training loss: 323.8119 0.1213 sec/batch\n",
      "Epoch (1/5) Batch (  500/4518 ) Iteration:      500 Avg. Training loss: 297.6261 0.1209 sec/batch\n",
      "Epoch (1/5) Batch (  600/4518 ) Iteration:      600 Avg. Training loss: 272.2038 0.1210 sec/batch\n",
      "Epoch (1/5) Batch (  700/4518 ) Iteration:      700 Avg. Training loss: 261.1461 0.1212 sec/batch\n",
      "Epoch (1/5) Batch (  800/4518 ) Iteration:      800 Avg. Training loss: 247.5380 0.1209 sec/batch\n",
      "Epoch (1/5) Batch (  900/4518 ) Iteration:      900 Avg. Training loss: 223.1556 0.1211 sec/batch\n",
      "Epoch (1/5) Batch ( 1000/4518 ) Iteration:     1000 Avg. Training loss: 223.4641 0.1208 sec/batch\n",
      "Epoch (1/5) Batch ( 1100/4518 ) Iteration:     1100 Avg. Training loss: 202.7871 0.1265 sec/batch\n",
      "Epoch (1/5) Batch ( 1200/4518 ) Iteration:     1200 Avg. Training loss: 191.4545 0.1206 sec/batch\n",
      "Epoch (1/5) Batch ( 1300/4518 ) Iteration:     1300 Avg. Training loss: 186.1957 0.1207 sec/batch\n",
      "Epoch (1/5) Batch ( 1400/4518 ) Iteration:     1400 Avg. Training loss: 177.4607 0.1206 sec/batch\n",
      "Epoch (1/5) Batch ( 1500/4518 ) Iteration:     1500 Avg. Training loss: 171.2138 0.1220 sec/batch\n",
      "Epoch (1/5) Batch ( 1600/4518 ) Iteration:     1600 Avg. Training loss: 159.1106 0.1235 sec/batch\n",
      "Epoch (1/5) Batch ( 1700/4518 ) Iteration:     1700 Avg. Training loss: 155.7030 0.1213 sec/batch\n",
      "Epoch (1/5) Batch ( 1800/4518 ) Iteration:     1800 Avg. Training loss: 151.9165 0.1210 sec/batch\n",
      "Epoch (1/5) Batch ( 1900/4518 ) Iteration:     1900 Avg. Training loss: 144.1853 0.1210 sec/batch\n",
      "Epoch (1/5) Batch ( 2000/4518 ) Iteration:     2000 Avg. Training loss: 125.5168 0.1210 sec/batch\n",
      "Epoch (1/5) Batch ( 2100/4518 ) Iteration:     2100 Avg. Training loss: 126.1216 0.1264 sec/batch\n",
      "Epoch (1/5) Batch ( 2200/4518 ) Iteration:     2200 Avg. Training loss: 128.3887 0.1214 sec/batch\n",
      "Epoch (1/5) Batch ( 2300/4518 ) Iteration:     2300 Avg. Training loss: 123.8516 0.1212 sec/batch\n",
      "Epoch (1/5) Batch ( 2400/4518 ) Iteration:     2400 Avg. Training loss: 125.5389 0.1207 sec/batch\n",
      "Epoch (1/5) Batch ( 2500/4518 ) Iteration:     2500 Avg. Training loss: 118.7291 0.1205 sec/batch\n",
      "Epoch (1/5) Batch ( 2600/4518 ) Iteration:     2600 Avg. Training loss: 108.3280 0.1219 sec/batch\n",
      "Epoch (1/5) Batch ( 2700/4518 ) Iteration:     2700 Avg. Training loss: 107.8853 0.1214 sec/batch\n",
      "Epoch (1/5) Batch ( 2800/4518 ) Iteration:     2800 Avg. Training loss: 105.8960 0.1212 sec/batch\n",
      "Epoch (1/5) Batch ( 2900/4518 ) Iteration:     2900 Avg. Training loss: 107.5586 0.1222 sec/batch\n",
      "Epoch (1/5) Batch ( 3000/4518 ) Iteration:     3000 Avg. Training loss: 106.5531 0.1225 sec/batch\n",
      "Epoch (1/5) Batch ( 3100/4518 ) Iteration:     3100 Avg. Training loss: 98.4047 0.1300 sec/batch\n",
      "Epoch (1/5) Batch ( 3200/4518 ) Iteration:     3200 Avg. Training loss: 99.5885 0.1216 sec/batch\n",
      "Epoch (1/5) Batch ( 3300/4518 ) Iteration:     3300 Avg. Training loss: 92.7970 0.1212 sec/batch\n",
      "Epoch (1/5) Batch ( 3400/4518 ) Iteration:     3400 Avg. Training loss: 91.3805 0.1211 sec/batch\n",
      "Epoch (1/5) Batch ( 3500/4518 ) Iteration:     3500 Avg. Training loss: 93.2209 0.1218 sec/batch\n",
      "Epoch (1/5) Batch ( 3600/4518 ) Iteration:     3600 Avg. Training loss: 81.2216 0.1218 sec/batch\n",
      "Epoch (1/5) Batch ( 3700/4518 ) Iteration:     3700 Avg. Training loss: 86.9439 0.1224 sec/batch\n",
      "Epoch (1/5) Batch ( 3800/4518 ) Iteration:     3800 Avg. Training loss: 85.4419 0.1210 sec/batch\n",
      "Epoch (1/5) Batch ( 3900/4518 ) Iteration:     3900 Avg. Training loss: 82.0623 0.1221 sec/batch\n",
      "Epoch (1/5) Batch ( 4000/4518 ) Iteration:     4000 Avg. Training loss: 76.3866 0.1224 sec/batch\n",
      "Epoch (1/5) Batch ( 4100/4518 ) Iteration:     4100 Avg. Training loss: 80.7089 0.1276 sec/batch\n",
      "Epoch (1/5) Batch ( 4200/4518 ) Iteration:     4200 Avg. Training loss: 72.2589 0.1232 sec/batch\n",
      "Epoch (1/5) Batch ( 4300/4518 ) Iteration:     4300 Avg. Training loss: 69.7851 0.1230 sec/batch\n",
      "Epoch (1/5) Batch ( 4400/4518 ) Iteration:     4400 Avg. Training loss: 70.5414 0.1211 sec/batch\n",
      "Epoch (1/5) Batch ( 4500/4518 ) Iteration:     4500 Avg. Training loss: 74.1270 0.1209 sec/batch\n",
      "Epoch (2/5) Batch (   82/4518 ) Iteration:     4600 Avg. Training loss: 65.8998 0.1000 sec/batch\n",
      "Epoch (2/5) Batch (  182/4518 ) Iteration:     4700 Avg. Training loss: 66.0234 0.1214 sec/batch\n",
      "Epoch (2/5) Batch (  282/4518 ) Iteration:     4800 Avg. Training loss: 65.0605 0.1217 sec/batch\n",
      "Epoch (2/5) Batch (  382/4518 ) Iteration:     4900 Avg. Training loss: 58.3775 0.1212 sec/batch\n",
      "Epoch (2/5) Batch (  482/4518 ) Iteration:     5000 Avg. Training loss: 61.0502 0.1217 sec/batch\n",
      "Epoch (2/5) Batch (  582/4518 ) Iteration:     5100 Avg. Training loss: 58.8449 0.1265 sec/batch\n",
      "Epoch (2/5) Batch (  682/4518 ) Iteration:     5200 Avg. Training loss: 56.9186 0.1218 sec/batch\n",
      "Epoch (2/5) Batch (  782/4518 ) Iteration:     5300 Avg. Training loss: 56.1752 0.1214 sec/batch\n",
      "Epoch (2/5) Batch (  882/4518 ) Iteration:     5400 Avg. Training loss: 54.5037 0.1213 sec/batch\n",
      "Epoch (2/5) Batch (  982/4518 ) Iteration:     5500 Avg. Training loss: 61.7897 0.1217 sec/batch\n",
      "Epoch (2/5) Batch ( 1082/4518 ) Iteration:     5600 Avg. Training loss: 57.1359 0.1214 sec/batch\n",
      "Epoch (2/5) Batch ( 1182/4518 ) Iteration:     5700 Avg. Training loss: 52.4011 0.1213 sec/batch\n",
      "Epoch (2/5) Batch ( 1282/4518 ) Iteration:     5800 Avg. Training loss: 53.7829 0.1215 sec/batch\n",
      "Epoch (2/5) Batch ( 1382/4518 ) Iteration:     5900 Avg. Training loss: 56.3432 0.1210 sec/batch\n",
      "Epoch (2/5) Batch ( 1482/4518 ) Iteration:     6000 Avg. Training loss: 54.7101 0.1210 sec/batch\n",
      "Epoch (2/5) Batch ( 1582/4518 ) Iteration:     6100 Avg. Training loss: 51.7587 0.1279 sec/batch\n",
      "Epoch (2/5) Batch ( 1682/4518 ) Iteration:     6200 Avg. Training loss: 54.1519 0.1219 sec/batch\n",
      "Epoch (2/5) Batch ( 1782/4518 ) Iteration:     6300 Avg. Training loss: 51.6253 0.1210 sec/batch\n",
      "Epoch (2/5) Batch ( 1882/4518 ) Iteration:     6400 Avg. Training loss: 53.4852 0.1210 sec/batch\n",
      "Epoch (2/5) Batch ( 1982/4518 ) Iteration:     6500 Avg. Training loss: 49.0675 0.1212 sec/batch\n",
      "Epoch (2/5) Batch ( 2082/4518 ) Iteration:     6600 Avg. Training loss: 52.0985 0.1208 sec/batch\n",
      "Epoch (2/5) Batch ( 2182/4518 ) Iteration:     6700 Avg. Training loss: 48.9849 0.1210 sec/batch\n",
      "Epoch (2/5) Batch ( 2282/4518 ) Iteration:     6800 Avg. Training loss: 50.1724 0.1219 sec/batch\n",
      "Epoch (2/5) Batch ( 2382/4518 ) Iteration:     6900 Avg. Training loss: 47.2749 0.1214 sec/batch\n",
      "Epoch (2/5) Batch ( 2482/4518 ) Iteration:     7000 Avg. Training loss: 51.0452 0.1210 sec/batch\n",
      "Epoch (2/5) Batch ( 2582/4518 ) Iteration:     7100 Avg. Training loss: 45.2041 0.1266 sec/batch\n",
      "Epoch (2/5) Batch ( 2682/4518 ) Iteration:     7200 Avg. Training loss: 51.8045 0.1215 sec/batch\n",
      "Epoch (2/5) Batch ( 2782/4518 ) Iteration:     7300 Avg. Training loss: 51.9856 0.1210 sec/batch\n",
      "Epoch (2/5) Batch ( 2882/4518 ) Iteration:     7400 Avg. Training loss: 48.2955 0.1208 sec/batch\n",
      "Epoch (2/5) Batch ( 2982/4518 ) Iteration:     7500 Avg. Training loss: 48.7456 0.1209 sec/batch\n",
      "Epoch (2/5) Batch ( 3082/4518 ) Iteration:     7600 Avg. Training loss: 46.7604 0.1216 sec/batch\n",
      "Epoch (2/5) Batch ( 3182/4518 ) Iteration:     7700 Avg. Training loss: 46.6127 0.1212 sec/batch\n",
      "Epoch (2/5) Batch ( 3282/4518 ) Iteration:     7800 Avg. Training loss: 48.0601 0.1211 sec/batch\n",
      "Epoch (2/5) Batch ( 3382/4518 ) Iteration:     7900 Avg. Training loss: 46.0115 0.1207 sec/batch\n",
      "Epoch (2/5) Batch ( 3482/4518 ) Iteration:     8000 Avg. Training loss: 51.7175 0.1211 sec/batch\n",
      "Epoch (2/5) Batch ( 3582/4518 ) Iteration:     8100 Avg. Training loss: 45.9862 0.1272 sec/batch\n",
      "Epoch (2/5) Batch ( 3682/4518 ) Iteration:     8200 Avg. Training loss: 46.9765 0.1214 sec/batch\n",
      "Epoch (2/5) Batch ( 3782/4518 ) Iteration:     8300 Avg. Training loss: 47.3397 0.1212 sec/batch\n",
      "Epoch (2/5) Batch ( 3882/4518 ) Iteration:     8400 Avg. Training loss: 45.1801 0.1214 sec/batch\n",
      "Epoch (2/5) Batch ( 3982/4518 ) Iteration:     8500 Avg. Training loss: 46.2236 0.1206 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (2/5) Batch ( 4082/4518 ) Iteration:     8600 Avg. Training loss: 47.0055 0.1214 sec/batch\n",
      "Epoch (2/5) Batch ( 4182/4518 ) Iteration:     8700 Avg. Training loss: 45.4049 0.1211 sec/batch\n",
      "Epoch (2/5) Batch ( 4282/4518 ) Iteration:     8800 Avg. Training loss: 44.3155 0.1217 sec/batch\n",
      "Epoch (2/5) Batch ( 4382/4518 ) Iteration:     8900 Avg. Training loss: 43.6555 0.1213 sec/batch\n",
      "Epoch (2/5) Batch ( 4482/4518 ) Iteration:     9000 Avg. Training loss: 43.4577 0.1218 sec/batch\n",
      "Epoch (3/5) Batch (   64/4518 ) Iteration:     9100 Avg. Training loss: 43.7408 0.0780 sec/batch\n",
      "Epoch (3/5) Batch (  164/4518 ) Iteration:     9200 Avg. Training loss: 43.8538 0.1213 sec/batch\n",
      "Epoch (3/5) Batch (  264/4518 ) Iteration:     9300 Avg. Training loss: 44.3655 0.1211 sec/batch\n",
      "Epoch (3/5) Batch (  364/4518 ) Iteration:     9400 Avg. Training loss: 43.9025 0.1212 sec/batch\n",
      "Epoch (3/5) Batch (  464/4518 ) Iteration:     9500 Avg. Training loss: 43.3287 0.1212 sec/batch\n",
      "Epoch (3/5) Batch (  564/4518 ) Iteration:     9600 Avg. Training loss: 44.9821 0.1215 sec/batch\n",
      "Epoch (3/5) Batch (  664/4518 ) Iteration:     9700 Avg. Training loss: 44.6794 0.1214 sec/batch\n",
      "Epoch (3/5) Batch (  764/4518 ) Iteration:     9800 Avg. Training loss: 45.1427 0.1214 sec/batch\n",
      "Epoch (3/5) Batch (  864/4518 ) Iteration:     9900 Avg. Training loss: 46.4412 0.1216 sec/batch\n",
      "Epoch (3/5) Batch (  964/4518 ) Iteration:    10000 Avg. Training loss: 47.0261 0.1217 sec/batch\n",
      "Epoch (3/5) Batch ( 1064/4518 ) Iteration:    10100 Avg. Training loss: 43.3294 0.1275 sec/batch\n",
      "Epoch (3/5) Batch ( 1164/4518 ) Iteration:    10200 Avg. Training loss: 42.9523 0.1213 sec/batch\n",
      "Epoch (3/5) Batch ( 1264/4518 ) Iteration:    10300 Avg. Training loss: 47.5868 0.1216 sec/batch\n",
      "Epoch (3/5) Batch ( 1364/4518 ) Iteration:    10400 Avg. Training loss: 44.0489 0.1214 sec/batch\n",
      "Epoch (3/5) Batch ( 1464/4518 ) Iteration:    10500 Avg. Training loss: 42.0840 0.1217 sec/batch\n",
      "Epoch (3/5) Batch ( 1564/4518 ) Iteration:    10600 Avg. Training loss: 47.4011 0.1215 sec/batch\n",
      "Epoch (3/5) Batch ( 1664/4518 ) Iteration:    10700 Avg. Training loss: 43.1103 0.1212 sec/batch\n",
      "Epoch (3/5) Batch ( 1764/4518 ) Iteration:    10800 Avg. Training loss: 47.8879 0.1219 sec/batch\n",
      "Epoch (3/5) Batch ( 1864/4518 ) Iteration:    10900 Avg. Training loss: 46.5871 0.1214 sec/batch\n",
      "Epoch (3/5) Batch ( 1964/4518 ) Iteration:    11000 Avg. Training loss: 42.7513 0.1215 sec/batch\n",
      "Epoch (3/5) Batch ( 2064/4518 ) Iteration:    11100 Avg. Training loss: 44.9730 0.1281 sec/batch\n",
      "Epoch (3/5) Batch ( 2164/4518 ) Iteration:    11200 Avg. Training loss: 45.3608 0.1228 sec/batch\n",
      "Epoch (3/5) Batch ( 2264/4518 ) Iteration:    11300 Avg. Training loss: 44.0188 0.1225 sec/batch\n",
      "Epoch (3/5) Batch ( 2364/4518 ) Iteration:    11400 Avg. Training loss: 45.9223 0.1229 sec/batch\n",
      "Epoch (3/5) Batch ( 2464/4518 ) Iteration:    11500 Avg. Training loss: 46.6274 0.1272 sec/batch\n",
      "Epoch (3/5) Batch ( 2564/4518 ) Iteration:    11600 Avg. Training loss: 44.7537 0.1299 sec/batch\n",
      "Epoch (3/5) Batch ( 2664/4518 ) Iteration:    11700 Avg. Training loss: 46.1080 0.1297 sec/batch\n",
      "Epoch (3/5) Batch ( 2764/4518 ) Iteration:    11800 Avg. Training loss: 43.0942 0.1265 sec/batch\n",
      "Epoch (3/5) Batch ( 2864/4518 ) Iteration:    11900 Avg. Training loss: 47.3182 0.1248 sec/batch\n",
      "Epoch (3/5) Batch ( 2964/4518 ) Iteration:    12000 Avg. Training loss: 44.9743 0.1275 sec/batch\n",
      "Epoch (3/5) Batch ( 3064/4518 ) Iteration:    12100 Avg. Training loss: 44.8924 0.1347 sec/batch\n",
      "Epoch (3/5) Batch ( 3164/4518 ) Iteration:    12200 Avg. Training loss: 45.8450 0.1327 sec/batch\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for testing\n",
    "test_lr     = [0.02] #[0.1, 0.01, 0.001]\n",
    "test_lf     = ['nce']\n",
    "test_use_tf = [True]#[True, False]\n",
    "\n",
    "settings = {'embed_dim'       : 200,\n",
    "            'nb_neg_sample'   : 100,\n",
    "            'learning_rate'   : 0.01,\n",
    "            'sampling_method' : 'fixed_unigram',\n",
    "            'loss_func'       : 'nce',\n",
    "            'subtract_log_q'  : True,\n",
    "            'use_tf_loss'     : False}\n",
    "\n",
    "epochs      = 5\n",
    "batch_size  = 1024\n",
    "window_size = 10\n",
    "max_iters   = None\n",
    "\n",
    "for lr in test_lr:\n",
    "    settings['learning_rate'] = lr\n",
    "    for lf in test_lf:\n",
    "        settings['loss_func'] = lf\n",
    "        for use_tf in test_use_tf:\n",
    "            settings['use_tf_loss'] = use_tf\n",
    "            \n",
    "            ## rebuild with new-setting\n",
    "            w2v_model.build_graph(settings)\n",
    "            \n",
    "            ## train and logs\n",
    "            w2v_model.train(epochs, batch_size, window_size, summary_path='nce_1', max_iters = max_iters)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at above result, we can see that learning_rate 0.1 doesn't work and learning_rate=0.001 is too low. Let change the learning_rate=0.02 and use 5 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "It's not trival to test the quality of word-vector. As in the introduction, we use 'dense-representation' of word to allow us to model the similarity of words via distance between points in embedding-space. So to evaluate the quality of word-vector one can use the folloing intrinsic tasks \n",
    "* `Nearest neighbors`: is to find closest word (in Euclidean distance or cosine similarity) for a given word \n",
    "* `Word analogy`: is to answer the question of the form `a` is to `b` as `c` is to `__` for example:\n",
    "<center>\n",
    "good:better rough:__ (expect rougher)\n",
    "<center>\n",
    "\n",
    "Or one can use the extrinsic tasks \n",
    "* `Sentiment classification`: for example we want to classify movie review\n",
    "\n",
    "Let load one checkpoint and try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build eval model and load a check-point\n",
    "w2v_eval = Word2vecSampling(vocabs, word2id, id2word, freqs, train_wordids)\n",
    "w2v_eval.build_graph()\n",
    "w2v_eval.build_eval_graph()\n",
    "sess = w2v_eval.load_checkpoint('./checkpoints/sg_lr=(0.02,),lf=nce,sampling=fixed_unigram,use_tf=True-45000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict man-king as woman-?\n",
      "answer: leader\n",
      "\n",
      "predict big-bigger as smart-?\n",
      "answer: poultry\n",
      "\n",
      "\n",
      "Nearest neighbours of [london]\n",
      "=====================================\n",
      "london               1.0000\n",
      "other                0.5790\n",
      "places               0.5779\n",
      "a                    0.5779\n",
      "meditations          0.5769\n",
      "actually             0.5764\n",
      "and                  0.5735\n",
      "others               0.5699\n",
      "but                  0.5634\n",
      "different            0.5632\n"
     ]
    }
   ],
   "source": [
    "# test some analogy\n",
    "w2v_eval.analogy(sess, 'man', 'king', 'woman')\n",
    "\n",
    "w2v_eval.analogy(sess, 'good', 'better', 'rough')\n",
    "\n",
    "# test nearest nearby word\n",
    "w2v_eval.nearby(sess, 'london')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As we can see the evaluation doesn't work very well since we train on a limited corpus and Skip-Gram with Neg-Sampling need bigger datas. In the next series, we look at Glove word2vec then look how to use pre-trained word2vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
