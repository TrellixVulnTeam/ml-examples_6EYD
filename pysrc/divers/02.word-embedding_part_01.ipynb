{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of numbers. However, natural language processing systems traditionally treat words a discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are very sparse and provide no useful information regarding the relationships that may exist between the individual symbols. \n",
    "\n",
    "Vector space models represent words in a continuous vector space where semantically similar words are mapped to nearby points (are embedded nearby each other). In this series of notebook, we look at few word embedding techniques and compare them:\n",
    "\n",
    "* Skip-gram with [Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* Glove: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) and more resource from [here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "# Skip-gram \n",
    "Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of traning words $w_1,w_2,\\ldots,w_T$, the objective of the Skip-gram model is to maximize the average log probability\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq 0}\\log p(w_{t+j}|w_t)\n",
    "$$\n",
    "The Skip-gram defines $p(w_{t+j}|w_t)$ using the softmax function\n",
    "$$\n",
    "p(w_{o}|w_{i}) = \\frac{\\exp\\left(u_{w_o}^Tv_{w_{i}}\\right)}{\\sum_{w=1}^V\\exp\\left(u_w^Tv_{w_{i}}\\right)}\n",
    "$$\n",
    "where $V$ is size of vocabulary and\n",
    "* $w_o$ is output word (outside word or surrounding word)\n",
    "* $w_i$ is input word (context word or center word)\n",
    "* $u_w$ is output vector representation\n",
    "* $v_w$ is input vector representation\n",
    "\n",
    "This formulation is impractical because the cost of computing the denominator is $O(V)$ where $V$ is often large ($10^5-10^7$).\n",
    "\n",
    "# Skip-gram with Negative sampling\n",
    "Mikolov et al. introduce one effecient technique so called Negative sampling (NEG). The NEG re-define the objective as\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}}\\right) + \\sum_{i=1}^k \\mathbb{E}_{j_i\\sim P_n(w)}\\log\\sigma\\left(-u_{j_i}^Tv_{w_{i}}\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "P_n(w) = U(w)^{3/4}/Z\n",
    "$$\n",
    "the unigram distribution $U(w)$ raised to the 3/4 power (then normalized by $Z$). The power 3/4 makes less frequent words be sampled more often.\n",
    "\n",
    "The idea here is to\n",
    "* maximize the probability that real outside word $w_o$ appears around center word $w_i$\n",
    "* minimize the probability that random words $j_i$ appears around center word $w_i$\n",
    "\n",
    "## Implementation planning\n",
    "Before doing the implementation, we list the required steps\n",
    "0. Choose dataset: \n",
    "    * which corpus to be used for training\n",
    "    * which test-set to be used for testing\n",
    "1. Pre-processing raw_tex:\n",
    "    * extract a set of all words (vocab)\n",
    "    * map vocab <-> integer id\n",
    "    * compute words-frequence (we might sub-sampling to remove some frequent words such as 'the,a,an,...e.t.c'), we also need the words-frequence to compute $P_n(w)$\n",
    "    * convert raw text to list of words-ids\n",
    "2. Ensemble a graph:\n",
    "    * Define inputs, targets: must take into account of mini-batches\n",
    "    * Define trainable variables\n",
    "    * Define a loss function with neg-sampling\n",
    "    * Define an optimizer (might need to apply some Gradient-Clipping technique)\n",
    "3. Training:\n",
    "    * How to feed inputs/targets data\n",
    "    * How to measure training performance\n",
    "    * How to tune hyper-parameters\n",
    "4. Evaluation:\n",
    "    * How to measure word2vec quality (hard)\n",
    "    \n",
    "## Choose dataset\n",
    "We use cleaned wiki-dataset from Matt Mahoney's [website](http://mattmahoney.net/dc/textdata.html):\n",
    "* [text8](http://mattmahoney.net/dc/text8.zip) is small dataset (100Mb) \n",
    "* [enwiki9](http://mattmahoney.net/dc/enwik9.zip) is bigger dataset (1Gb)\n",
    "\n",
    "We use the same script in Matt Mahoney to create text9 data from enwik9.\n",
    "\n",
    "First we load module for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, the text data is clean text (i.e no punctuation, no new line), let's view first 100 characters of our text-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "text_file = '/home/minhvu/workplaces/tf_datas/nltk/text8'\n",
    "preprocess_file = '/home/minhvu/workplaces/tf_datas/nltk/text8.pkl'\n",
    "with open(text_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    print (text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "We code pre-processing into **Word2VecInput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing took 21.26 seconds\n"
     ]
    }
   ],
   "source": [
    "from nlp.preprocess_input import Word2VecInput\n",
    "\n",
    "ts = time()\n",
    "w2v_input = Word2VecInput(text_file)\n",
    "print ('Pre-processing took {:.2f} seconds'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pre-processing took quite a long time, we dump pre-processed data into a pickled file which includes vocabs, word2id, id2word, word-frequences and trained_wordids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump pre-processing data to file\n",
    "w2v_input.dump(preprocess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble a graph\n",
    "We need to define inputs and targets, re-call that the Skip-gram model is to predict surrounding words given a center word so input will be center word and targets will be surrounding-words. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"750\"\n",
       "            src=\"./skipgram-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc0e253208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('./skipgram-demo/index.html', width=500, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the input/target can be defined by tf.placeholder of tf.int32 to represent word-id (integer), the tricky part is to \n",
    "* define embeding layer\n",
    "* define sampling procedure\n",
    "* define loss function\n",
    "\n",
    "### Embedding layer\n",
    "For each word $w$, we have two embedding layers $u_w$ and $v_w$ with embedding-dimension $D$, we can model it as follow\n",
    "* $v_w$ is input embedding-weight\n",
    "* $u_w$ is output softmax-weight\n",
    "\n",
    "We can define embedding-weight and softmax-weight as [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) with shape $[V,D]$. \n",
    "\n",
    "Note that for embedding-weight $u_w$ we often initialized by random-uniform between [-1,1], while $v_w$ is initialized by truncated-normal with $\\sigma=\\frac{1.0}{\\sqrt{D}}$.\n",
    "\n",
    "Note that since $V$ can be very large, we need a way to look-up $u_w, v_w$, this can be done via [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup).\n",
    "\n",
    "### Sampling procedure\n",
    "The sampling method is tricky since $V$ can be very large. Fortunately, Tensorflow has implemented various [candidate-sampling](https://www.tensorflow.org/api_guides/python/nn#Candidate_Sampling). Here we will use\n",
    "* [tf.nn.fixed_unigram_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/nn/fixed_unigram_candidate_sampler): to sample $P_n(w)$ as described above\n",
    "* [https://www.tensorflow.org/api_docs/python/tf/nn/log_uniform_candidate_sampler]: to sample log-uniform, note this should be used only our words is sorted with decreasing frequence\n",
    "\n",
    "### Loss function\n",
    "Tensorflow has already implemented \n",
    "* [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss): sampled softmax training loss\n",
    "* [tf.nn.nce_loss](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss): sampled logistic training loss\n",
    "\n",
    "Let's review the implementation of the two above loss function\n",
    "* Sampled-softmax compute\n",
    "$$\n",
    "-\\log\\left(\\frac{\\exp(u_{w_o}^Tv_{w_{i}})}{\\exp(u_{w_o}^Tv_{w_{i}}) + \\sum_{i=1}^k \\exp(u_{j_i}^Tv_{w_{i}})} \\right) \n",
    "$$\n",
    "via [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* Sampled-logistic compute\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}})\\right) + \\sum_{i=1}^k \\log \\sigma(-u_{j_i}^Tv_{w_{i}})\n",
    "$$\n",
    "via [tf.nn.sigmoid_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "We implement above steps inside **`Word2vecSamping`** object and add training to it.\n",
    "\n",
    "## Training\n",
    "Let build a word2vec model using **`Word2vecSampling`**\n",
    "and training it with pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocabs, word2id, id2word, freqs, train_wordids = pickle.load(open(preprocess_file, 'rb'))\n",
    "from nlp.word2vec import Word2vecSampling\n",
    "\n",
    "w2v_model = Word2vecSampling(vocabs, word2id, id2word, freqs, train_wordids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 200]\n",
      "[None, 1, 200]\n",
      "[None, 1]\n",
      "[None, 200]\n"
     ]
    }
   ],
   "source": [
    "settings = {'embed_dim'       : 200,\n",
    "            'nb_neg_sample'   : 100,\n",
    "            'learning_rate'   : 0.01,\n",
    "            'sampling_method' : 'neg',\n",
    "            'loss_func'       : 'nce'}\n",
    "\n",
    "w2v_model.build_graph(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create checkpoints to save training-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) Batch (  100/18081) Iteration:      100 Avg. Training loss: 1.4814 0.1002 sec/batch\n",
      "Epoch (1/1) Batch (  200/18081) Iteration:      200 Avg. Training loss: 1.6759 0.0995 sec/batch\n",
      "Epoch (1/1) Batch (  300/18081) Iteration:      300 Avg. Training loss: 1.7622 0.0987 sec/batch\n",
      "Epoch (1/1) Batch (  400/18081) Iteration:      400 Avg. Training loss: 1.8012 0.0992 sec/batch\n",
      "Epoch (1/1) Batch (  500/18081) Iteration:      500 Avg. Training loss: 1.8751 0.0996 sec/batch\n",
      "Epoch (1/1) Batch (  600/18081) Iteration:      600 Avg. Training loss: 2.1220 0.0998 sec/batch\n",
      "Epoch (1/1) Batch (  700/18081) Iteration:      700 Avg. Training loss: 2.0015 0.0992 sec/batch\n",
      "Epoch (1/1) Batch (  800/18081) Iteration:      800 Avg. Training loss: 2.1337 0.0994 sec/batch\n",
      "Epoch (1/1) Batch (  900/18081) Iteration:      900 Avg. Training loss: 2.2160 0.0996 sec/batch\n",
      "Epoch (1/1) Batch ( 1000/18081) Iteration:     1000 Avg. Training loss: 2.2054 0.0993 sec/batch\n",
      "Epoch (1/1) Batch ( 1100/18081) Iteration:     1100 Avg. Training loss: 2.2302 0.1182 sec/batch\n",
      "Epoch (1/1) Batch ( 1200/18081) Iteration:     1200 Avg. Training loss: 2.1878 0.1005 sec/batch\n",
      "Epoch (1/1) Batch ( 1300/18081) Iteration:     1300 Avg. Training loss: 2.2409 0.0991 sec/batch\n",
      "Epoch (1/1) Batch ( 1400/18081) Iteration:     1400 Avg. Training loss: 2.3038 0.0998 sec/batch\n",
      "Epoch (1/1) Batch ( 1500/18081) Iteration:     1500 Avg. Training loss: 2.3538 0.0986 sec/batch\n",
      "Epoch (1/1) Batch ( 1600/18081) Iteration:     1600 Avg. Training loss: 2.4579 0.0999 sec/batch\n",
      "Epoch (1/1) Batch ( 1700/18081) Iteration:     1700 Avg. Training loss: 2.4993 0.0995 sec/batch\n",
      "Epoch (1/1) Batch ( 1800/18081) Iteration:     1800 Avg. Training loss: 2.4847 0.0984 sec/batch\n",
      "Epoch (1/1) Batch ( 1900/18081) Iteration:     1900 Avg. Training loss: 2.6041 0.0997 sec/batch\n",
      "Epoch (1/1) Batch ( 2000/18081) Iteration:     2000 Avg. Training loss: 2.4362 0.0994 sec/batch\n",
      "Epoch (1/1) Batch ( 2100/18081) Iteration:     2100 Avg. Training loss: 2.5838 0.1219 sec/batch\n",
      "Epoch (1/1) Batch ( 2200/18081) Iteration:     2200 Avg. Training loss: 2.4155 0.0986 sec/batch\n",
      "Epoch (1/1) Batch ( 2300/18081) Iteration:     2300 Avg. Training loss: 2.4463 0.0996 sec/batch\n",
      "Epoch (1/1) Batch ( 2400/18081) Iteration:     2400 Avg. Training loss: 2.5067 0.1002 sec/batch\n",
      "Epoch (1/1) Batch ( 2500/18081) Iteration:     2500 Avg. Training loss: 2.4743 0.0994 sec/batch\n",
      "Epoch (1/1) Batch ( 2600/18081) Iteration:     2600 Avg. Training loss: 2.6375 0.0988 sec/batch\n",
      "Epoch (1/1) Batch ( 2700/18081) Iteration:     2700 Avg. Training loss: 2.3794 0.1006 sec/batch\n",
      "Epoch (1/1) Batch ( 2800/18081) Iteration:     2800 Avg. Training loss: 2.7151 0.0996 sec/batch\n",
      "Epoch (1/1) Batch ( 2900/18081) Iteration:     2900 Avg. Training loss: 2.6523 0.0983 sec/batch\n",
      "Epoch (1/1) Batch ( 3000/18081) Iteration:     3000 Avg. Training loss: 2.6017 0.0999 sec/batch\n",
      "Epoch (1/1) Batch ( 3100/18081) Iteration:     3100 Avg. Training loss: 2.6023 0.1145 sec/batch\n",
      "Epoch (1/1) Batch ( 3200/18081) Iteration:     3200 Avg. Training loss: 2.5852 0.0995 sec/batch\n",
      "Epoch (1/1) Batch ( 3300/18081) Iteration:     3300 Avg. Training loss: 2.4970 0.1000 sec/batch\n",
      "Epoch (1/1) Batch ( 3400/18081) Iteration:     3400 Avg. Training loss: 2.6408 0.1000 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6677f5c5b1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/minhvu/workplaces/vudev/ml-examples/pysrc/common/nlp/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, window_size, print_every, save_every)\u001b[0m\n\u001b[1;32m    190\u001b[0m                             self._target_words : np.array(y)[:, None]}\n\u001b[1;32m    191\u001b[0m                     train_loss, _ = sess.run([self._cost,\n\u001b[0;32m--> 192\u001b[0;31m                                               self._optimizer], feed_dict=feed)\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 256\n",
    "window_size = 5\n",
    "w2v_model.train(epochs, batch_size, window_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
