{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of numbers. However, natural language processing systems traditionally treat words a discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are very sparse and provide no useful information regarding the relationships that may exist between the individual symbols. \n",
    "\n",
    "Vector space models represent words in a continuous vector space where semantically similar words are mapped to nearby points (are embedded nearby each other). In this series of notebook, we look at few word embedding techniques and compare them:\n",
    "\n",
    "* Skip-gram with [Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* Glove: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) and more resource from [here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "# Skip-gram \n",
    "Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of traning words $w_1,w_2,\\ldots,w_T$, the objective of the Skip-gram model is to maximize the average log probability\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq 0}\\log p(w_{t+j}|w_t)\n",
    "$$\n",
    "The Skip-gram defines $p(w_{t+j}|w_t)$ using the softmax function\n",
    "$$\n",
    "p(w_{o}|w_{i}) = \\frac{\\exp\\left(u_{w_o}^Tv_{w_{i}}\\right)}{\\sum_{w=1}^V\\exp\\left(u_w^Tv_{w_{i}}\\right)}\n",
    "$$\n",
    "where $V$ is size of vocabulary and\n",
    "* $w_o$ is output word (outside word or surrounding word)\n",
    "* $w_i$ is input word (context word or center word)\n",
    "* $u_w$ is output vector representation\n",
    "* $v_w$ is input vector representation\n",
    "\n",
    "This formulation is impractical because the cost of computing the denominator is $O(V)$ where $V$ is often large ($10^5-10^7$).\n",
    "\n",
    "# Skip-gram with Negative sampling\n",
    "Mikolov et al. introduce one effecient technique so called Negative sampling (NEG). The NEG re-define the objective as\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}}\\right) + \\sum_{i=1}^k \\mathbb{E}_{j_i\\sim P_n(w)}\\log\\sigma\\left(-u_{j_i}^Tv_{w_{i}}\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "P_n(w) = U(w)^{3/4}/Z\n",
    "$$\n",
    "the unigram distribution $U(w)$ raised to the 3/4 power (then normalized by $Z$). The power 3/4 makes less frequent words be sampled more often.\n",
    "\n",
    "The idea here is to\n",
    "* maximize the probability that real outside word $w_o$ appears around center word $w_i$\n",
    "* minimize the probability that random words $j_i$ appears around center word $w_i$\n",
    "\n",
    "## Implementation planning\n",
    "Before doing the implementation, we list the required steps\n",
    "0. Choose dataset: \n",
    "    * which corpus to be used for training\n",
    "    * which test-set to be used for testing\n",
    "1. Pre-processing raw_tex:\n",
    "    * extract a set of all words (vocab)\n",
    "    * map vocab <-> integer id\n",
    "    * compute words-frequence (we might sub-sampling to remove some frequent words such as 'the,a,an,...e.t.c'), we also need the words-frequence to compute $P_n(w)$\n",
    "    * convert raw text to list of words-ids\n",
    "2. Ensemble a graph:\n",
    "    * Define inputs, targets: must take into account of mini-batches\n",
    "    * Define trainable variables\n",
    "    * Define a loss function with neg-sampling\n",
    "    * Define an optimizer (might need to apply some Gradient-Clipping technique)\n",
    "3. Training:\n",
    "    * How to feed inputs/targets data\n",
    "    * How to measure training performance\n",
    "    * How to tune hyper-parameters\n",
    "4. Evaluation:\n",
    "    * How to measure word2vec quality (hard)\n",
    "    \n",
    "## Choose dataset\n",
    "We use cleaned wiki-dataset from Matt Mahoney's [website](http://mattmahoney.net/dc/textdata.html):\n",
    "* [text8](http://mattmahoney.net/dc/text8.zip) is small dataset (100Mb) \n",
    "* [enwiki9](http://mattmahoney.net/dc/enwik9.zip) is bigger dataset (1Gb)\n",
    "\n",
    "We use the same script in Matt Mahoney to create text9 data from enwik9.\n",
    "\n",
    "First we load module for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, the text data is clean text (i.e no punctuation, no new line), let's view first 100 characters of our text-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "text_file = '/home/%s/workplaces/tf_datas/nltk/text8' % getpass.getuser()\n",
    "preprocess_file = '/home/%s/workplaces/tf_datas/nltk/text8.pkl' % getpass.getuser()\n",
    "with open(text_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    print (text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "We code pre-processing into **Word2VecInput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing took 20.42 seconds\n"
     ]
    }
   ],
   "source": [
    "from nlp.preprocess_input import Word2VecInput\n",
    "\n",
    "ts = time()\n",
    "w2v_input = Word2VecInput(text_file)\n",
    "print ('Pre-processing took {:.2f} seconds'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pre-processing took quite a long time, we dump pre-processed data into a pickled file which includes vocabs, word2id, id2word, word-frequences and trained_wordids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump pre-processing data to file\n",
    "w2v_input.dump(preprocess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble a graph\n",
    "We need to define inputs and targets, re-call that the Skip-gram model is to predict surrounding words given a center word so input will be center word and targets will be surrounding-words. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"750\"\n",
       "            src=\"./skipgram-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd19ebd1160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('./skipgram-demo/index.html', width=500, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the input/target can be defined by tf.placeholder of tf.int32 to represent word-id (integer), the tricky part is to \n",
    "* define embeding layer\n",
    "* define sampling procedure\n",
    "* define loss function\n",
    "\n",
    "### Embedding layer\n",
    "For each word $w$, we have two embedding layers $u_w$ and $v_w$ with embedding-dimension $D$, we can model it as follow\n",
    "* $v_w$ is input embedding-weight\n",
    "* $u_w$ is output softmax-weight\n",
    "\n",
    "We can define embedding-weight and softmax-weight as [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) with shape $[V,D]$. \n",
    "\n",
    "Note that for embedding-weight $u_w$ we often initialized by random-uniform between [-1,1], while $v_w$ is initialized by truncated-normal with $\\sigma=\\frac{1.0}{\\sqrt{D}}$.\n",
    "\n",
    "Note that since $V$ can be very large, we need a way to look-up $u_w, v_w$, this can be done via [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup).\n",
    "\n",
    "### Sampling procedure\n",
    "The sampling method is tricky since $V$ can be very large. Fortunately, Tensorflow has implemented various [candidate-sampling](https://www.tensorflow.org/api_guides/python/nn#Candidate_Sampling). Here we will use\n",
    "* [tf.nn.fixed_unigram_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/nn/fixed_unigram_candidate_sampler): to sample $P_n(w)$ as described above\n",
    "* [https://www.tensorflow.org/api_docs/python/tf/nn/log_uniform_candidate_sampler]: to sample log-uniform, note this should be used **only if our words is sorted with decreasing frequence**\n",
    "\n",
    "### Loss function\n",
    "Tensorflow has already implemented (see [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py) for implementation)\n",
    "* [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss): sampled softmax training loss\n",
    "* [tf.nn.nce_loss](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss): sampled logistic training loss\n",
    "\n",
    "Let's review the implementation of the two above loss function\n",
    "* Sampled-softmax compute\n",
    "$$\n",
    "-\\log\\left(\\frac{\\exp(u_{w_o}^Tv_{w_{i}})}{\\exp(u_{w_o}^Tv_{w_{i}}) + \\sum_{i=1}^k \\exp(u_{j_i}^Tv_{w_{i}})} \\right) \n",
    "$$\n",
    "via [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* Sampled-logistic compute\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}})\\right) + \\sum_{i=1}^k \\log \\sigma(-u_{j_i}^Tv_{w_{i}})\n",
    "$$\n",
    "via [tf.nn.sigmoid_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "We implement above steps inside **`Word2vecSamping`** object and add training to it.\n",
    "\n",
    "## Training\n",
    "Let build a word2vec model using **`Word2vecSampling`**\n",
    "and training it with pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocabs, word2id, id2word, freqs, train_wordids = pickle.load(open(preprocess_file, 'rb'))\n",
    "from nlp.word2vec import Word2vecSampling\n",
    "\n",
    "w2v_model = Word2vecSampling(vocabs, word2id, id2word, freqs, train_wordids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "settings = {'embed_dim'       : 200,\n",
    "            'nb_neg_sample'   : 100,\n",
    "            'learning_rate'   : 0.01,\n",
    "            'sampling_method' : 'fixed_unigram',\n",
    "            'loss_func'       : 'nce',\n",
    "            'subtract_log_q'  : True,\n",
    "            'use_tf_loss'     : False}\n",
    "\n",
    "w2v_model.build_graph(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create checkpoints to save training-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints\n",
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) Batch (  100/4520 ) Iteration:      100 Avg. Training loss: 489.8901 0.1228 sec/batch\n",
      "Epoch (1/1) Batch (  200/4520 ) Iteration:      200 Avg. Training loss: 423.4808 0.1210 sec/batch\n",
      "Epoch (1/1) Batch (  300/4520 ) Iteration:      300 Avg. Training loss: 360.3843 0.1206 sec/batch\n",
      "Epoch (1/1) Batch (  400/4520 ) Iteration:      400 Avg. Training loss: 328.1710 0.1209 sec/batch\n",
      "Epoch (1/1) Batch (  500/4520 ) Iteration:      500 Avg. Training loss: 295.6182 0.1210 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c8d56c6b8730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/minhvu/workplaces/vudev/ml-examples/pysrc/common/nlp/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, window_size, max_iters, print_every, save_every, summary_every, summary_path)\u001b[0m\n\u001b[1;32m    250\u001b[0m                             self._target_words : np.array(y)[:, None]}\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/tf_gpu_head/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 1024\n",
    "window_size = 5\n",
    "w2v_model.train(epochs, batch_size, window_size, summary_path='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tunning\n",
    "In this section we summarize some result for training word2vec. We have the following method\n",
    "\n",
    "| Sampling method | Loss function   | \n",
    "| :-------------: |:---------------:| \n",
    "| fixed unigram   | sampled softmax | \n",
    "| log_uniform     | nce             |\n",
    "\n",
    "Note that, by-default `tf.nn.sampled_softmax_loss` and `tf.nn.nce` use sampling-method `log_uniform`. We want to make our test close with original papers so we will use `fixed_unigram` as default.\n",
    "\n",
    "We test with the following \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) Batch (  100/4518 ) Iteration:      100 Avg. Training loss: 476.0511 0.1334 sec/batch\n",
      "Epoch (1/1) Batch (  200/4518 ) Iteration:      200 Avg. Training loss: 412.1777 0.1330 sec/batch\n",
      "Epoch (1/1) Batch (  300/4518 ) Iteration:      300 Avg. Training loss: 365.3296 0.1342 sec/batch\n",
      "Epoch (1/1) Batch (  400/4518 ) Iteration:      400 Avg. Training loss: 323.3641 0.1350 sec/batch\n",
      "Epoch (1/1) Batch (  500/4518 ) Iteration:      500 Avg. Training loss: 294.5101 0.1347 sec/batch\n",
      "Epoch (1/1) Batch (  600/4518 ) Iteration:      600 Avg. Training loss: 277.0850 0.1348 sec/batch\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for testing\n",
    "test_lr     = [0.02] #[0.1, 0.01, 0.001]\n",
    "test_lf     = ['nce']\n",
    "test_use_tf = [True]#[True, False]\n",
    "\n",
    "settings = {'embed_dim'       : 200,\n",
    "            'nb_neg_sample'   : 100,\n",
    "            'learning_rate'   : 0.01,\n",
    "            'sampling_method' : 'fixed_unigram',\n",
    "            'loss_func'       : 'nce',\n",
    "            'subtract_log_q'  : True,\n",
    "            'use_tf_loss'     : False}\n",
    "\n",
    "epochs      = 1\n",
    "batch_size  = 1024\n",
    "window_size = 5\n",
    "max_iters   = 2000\n",
    "\n",
    "for lr in test_lr:\n",
    "    settings['learning_rate'] = lr\n",
    "    for lf in test_lf:\n",
    "        settings['loss_func'] = lf\n",
    "        for use_tf in test_use_tf:\n",
    "            settings['use_tf_loss'] = use_tf\n",
    "            \n",
    "            ## rebuild with new-setting\n",
    "            w2v_model.build_graph(settings)\n",
    "            \n",
    "            ## train and logs\n",
    "            w2v_model.train(epochs, batch_size, window_size, summary_path='nce_1', max_iters = max_iters)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at above result, we can see that learning_rate 0.1 doesn't work and learning_rate=0.001 is too low. Let change the learning_rate=0.01 and 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "It's not trival to test the quality of word-vector. As in the introduction, we use 'dense-representation' of word to allow us to model the similarity of words via distance between points in embedding-space. So to evaluate the quality of word-vector one can use the folloing intrinsic tasks \n",
    "* `Nearest neighbors`: is to find closest word (in Euclidean distance or cosine similarity) for a given word \n",
    "* `Word analogy`: is to answer the question of the form `a` is to `b` as `c` is to `__` for example:\n",
    "<center>\n",
    "good:better rough:__ (expect rougher)\n",
    "<center>\n",
    "\n",
    "Or one can use the extrinsic tasks \n",
    "* `Sentiment classification`: for example we want to classify movie review\n",
    "\n",
    "Let load one checkpoint and try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build eval model and load a check-point\n",
    "w2v_eval = Word2vecSampling(vocabs, word2id, id2word, freqs, train_wordids)\n",
    "w2v_eval.build_graph()\n",
    "w2v_eval.build_eval_graph()\n",
    "sess = w2v_eval.load_checkpoint('./checkpoints/sg_lr=(0.02,),lf=nce,sampling=fixed_unigram,use_tf=False-2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict man-king as woman-?\n",
      "answer: vassal\n",
      "\n",
      "predict go-went as learn-?\n",
      "answer: bioprospecting\n",
      "\n",
      "\n",
      "Nearest neighbor of [good]\n",
      "=====================================\n",
      "good                 1.0000\n",
      "adventure            0.4218\n",
      "on                   0.4200\n",
      "in                   0.4122\n",
      "romandy              0.3985\n",
      "audiences            0.3961\n",
      "curving              0.3886\n",
      "improv               0.3869\n",
      "bitmaps              0.3837\n",
      "argument             0.3805\n"
     ]
    }
   ],
   "source": [
    "# test some analogy\n",
    "w2v_eval.analogy(sess, 'man', 'king', 'woman')\n",
    "\n",
    "w2v_eval.analogy(sess, 'go', 'went', 'learn')\n",
    "\n",
    "# test nearest nearby word\n",
    "w2v_eval.nearby(sess, 'good')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
