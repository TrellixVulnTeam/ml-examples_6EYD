{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of numbers. However, natural language processing systems traditionally treat words a discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are very sparse and provide no useful information regarding the relationships that may exist between the individual symbols. \n",
    "\n",
    "Vector space models represent words in a continuous vector space where semantically similar words are mapped to nearby points (are embedded nearby each other). In this series of notebook, we look at few word embedding techniques and compare them:\n",
    "\n",
    "* Skip-gram with [Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* Glove: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) and more resource from [here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "# Skip-gram \n",
    "Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of traning words $w_1,w_2,\\ldots,w_T$, the objective of the Skip-gram model is to maximize the average log probability\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq 0}\\log p(w_{t+j}|w_t)\n",
    "$$\n",
    "The Skip-gram defines $p(w_{t+j}|w_t)$ using the softmax function\n",
    "$$\n",
    "p(w_{o}|w_{i}) = \\frac{\\exp\\left(u_{w_o}^Tv_{w_{i}}\\right)}{\\sum_{w=1}^V\\exp\\left(u_w^Tv_{w_{i}}\\right)}\n",
    "$$\n",
    "where $V$ is size of vocabulary and\n",
    "* $w_o$ is output word (outside word or surrounding word)\n",
    "* $w_i$ is input word (context word or center word)\n",
    "* $u_w$ is output vector representation\n",
    "* $v_w$ is input vector representation\n",
    "\n",
    "This formulation is impractical because the cost of computing the denominator is $O(V)$ where $V$ is often large ($10^5-10^7$).\n",
    "\n",
    "# Skip-gram with Negative sampling\n",
    "Mikolov et al. introduce one effecient technique so called Negative sampling (NEG). The NEG re-define the objective as\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}}\\right) + \\sum_{i=1}^k \\mathbb{E}_{j_i\\sim P_n(w)}\\log\\sigma\\left(-u_{j_i}^Tv_{w_{i}}\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "P_n(w) = U(w)^{3/4}/Z\n",
    "$$\n",
    "the unigram distribution $U(w)$ raised to the 3/4 power (then normalized by $Z$). The power 3/4 makes less frequent words be sampled more often.\n",
    "\n",
    "The idea here is to\n",
    "* maximize the probability that real outside word $w_o$ appears around center word $w_i$\n",
    "* minimize the probability that random words $j_i$ appears around center word $w_i$\n",
    "\n",
    "## Implementation planning\n",
    "Before doing the implementation, we list the required steps\n",
    "0. Choose dataset: \n",
    "    * which corpus to be used for training\n",
    "    * which test-set to be used for testing\n",
    "1. Pre-processing raw_tex:\n",
    "    * extract a set of all words (vocab)\n",
    "    * map vocab <-> integer id\n",
    "    * compute words-frequence (we might sub-sampling to remove some frequent words such as 'the,a,an,...e.t.c'), we also need the words-frequence to compute $P_n(w)$\n",
    "    * convert raw text to list of words-ids\n",
    "2. Ensemble a graph:\n",
    "    * Define inputs, targets: must take into account of mini-batches\n",
    "    * Define trainable variables\n",
    "    * Define a loss function with neg-sampling\n",
    "    * Define an optimizer (might need to apply some Gradient-Clipping technique)\n",
    "3. Training:\n",
    "    * How to feed inputs/targets data\n",
    "    * How to measure training performance\n",
    "    * How to tune hyper-parameters\n",
    "4. Evaluation:\n",
    "    * How to measure word2vec quality (hard)\n",
    "    \n",
    "## Choose dataset\n",
    "We use cleaned wiki-dataset from Matt Mahoney's [website](http://mattmahoney.net/dc/textdata.html):\n",
    "* [text8](http://mattmahoney.net/dc/text8.zip) is small dataset (100Mb) \n",
    "* [enwiki9](http://mattmahoney.net/dc/enwik9.zip) is bigger dataset (1Gb)\n",
    "\n",
    "We use the same script in Matt Mahoney to create text9 data from enwik9.\n",
    "\n",
    "First we load module for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, the text data is clean text (i.e no punctuation, no new line), let's view first 100 characters of our text-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "text_file = '/home/%s/workplaces/tf_datas/nltk/text8' % getpass.getuser()\n",
    "preprocess_file = '/home/%s/workplaces/tf_datas/nltk/text8.pkl' % getpass.getuser()\n",
    "with open(text_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    print (text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "We code pre-processing into **Word2VecInput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing took 17.26 seconds\n"
     ]
    }
   ],
   "source": [
    "from nlp.preprocess_input import Word2VecInput\n",
    "\n",
    "ts = time()\n",
    "w2v_input = Word2VecInput(text_file)\n",
    "print ('Pre-processing took {:.2f} seconds'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pre-processing took quite a long time, we dump pre-processed data into a pickled file which includes vocabs, word2id, id2word, word-frequences and trained_wordids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump pre-processing data to file\n",
    "w2v_input.dump(preprocess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble a graph\n",
    "We need to define inputs and targets, re-call that the Skip-gram model is to predict surrounding words given a center word so input will be center word and targets will be surrounding-words. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"750\"\n",
       "            src=\"./skipgram-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd19ebd1160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('./skipgram-demo/index.html', width=500, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the input/target can be defined by tf.placeholder of tf.int32 to represent word-id (integer), the tricky part is to \n",
    "* define embeding layer\n",
    "* define sampling procedure\n",
    "* define loss function\n",
    "\n",
    "### Embedding layer\n",
    "For each word $w$, we have two embedding layers $u_w$ and $v_w$ with embedding-dimension $D$, we can model it as follow\n",
    "* $v_w$ is input embedding-weight\n",
    "* $u_w$ is output softmax-weight\n",
    "\n",
    "We can define embedding-weight and softmax-weight as [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) with shape $[V,D]$. \n",
    "\n",
    "Note that for embedding-weight $u_w$ we often initialized by random-uniform between [-1,1], while $v_w$ is initialized by truncated-normal with $\\sigma=\\frac{1.0}{\\sqrt{D}}$.\n",
    "\n",
    "Note that since $V$ can be very large, we need a way to look-up $u_w, v_w$, this can be done via [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup).\n",
    "\n",
    "### Sampling procedure\n",
    "The sampling method is tricky since $V$ can be very large. Fortunately, Tensorflow has implemented various [candidate-sampling](https://www.tensorflow.org/api_guides/python/nn#Candidate_Sampling). Here we will use\n",
    "* [tf.nn.fixed_unigram_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/nn/fixed_unigram_candidate_sampler): to sample $P_n(w)$ as described above\n",
    "* [https://www.tensorflow.org/api_docs/python/tf/nn/log_uniform_candidate_sampler]: to sample log-uniform, note this should be used only our words is sorted with decreasing frequence\n",
    "\n",
    "### Loss function\n",
    "Tensorflow has already implemented (see [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py) for implementation)\n",
    "* [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss): sampled softmax training loss\n",
    "* [tf.nn.nce_loss](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss): sampled logistic training loss\n",
    "\n",
    "Let's review the implementation of the two above loss function\n",
    "* Sampled-softmax compute\n",
    "$$\n",
    "-\\log\\left(\\frac{\\exp(u_{w_o}^Tv_{w_{i}})}{\\exp(u_{w_o}^Tv_{w_{i}}) + \\sum_{i=1}^k \\exp(u_{j_i}^Tv_{w_{i}})} \\right) \n",
    "$$\n",
    "via [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* Sampled-logistic compute\n",
    "$$\n",
    "\\log \\sigma\\left(u_{w_o}^Tv_{w_{i}})\\right) + \\sum_{i=1}^k \\log \\sigma(-u_{j_i}^Tv_{w_{i}})\n",
    "$$\n",
    "via [tf.nn.sigmoid_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "We implement above steps inside **`Word2vecSamping`** object and add training to it.\n",
    "\n",
    "## Training\n",
    "Let build a word2vec model using **`Word2vecSampling`**\n",
    "and training it with pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocabs, word2id, id2word, freqs, train_wordids = pickle.load(open(preprocess_file, 'rb'))\n",
    "from nlp.word2vec import Word2vecSampling\n",
    "\n",
    "w2v_model = Word2vecSampling(vocabs, word2id, id2word, freqs, train_wordids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {'embed_dim'       : 200,\n",
    "            'nb_neg_sample'   : 100,\n",
    "            'learning_rate'   : 0.01,\n",
    "            'sampling_method' : 'neg',\n",
    "            'loss_func'       : 'sampled_softmax',\n",
    "            'subtract_log_q'  : True}\n",
    "\n",
    "w2v_model.build_graph(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create checkpoints to save training-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1) Batch (  100/4521 ) Iteration:      100 Avg. Training loss: 5.6810 0.2212 sec/batch\n",
      "Epoch (1/1) Batch (  200/4521 ) Iteration:      200 Avg. Training loss: 5.5808 0.2210 sec/batch\n",
      "Epoch (1/1) Batch (  300/4521 ) Iteration:      300 Avg. Training loss: 5.4242 0.2206 sec/batch\n",
      "Epoch (1/1) Batch (  400/4521 ) Iteration:      400 Avg. Training loss: 5.6244 0.2200 sec/batch\n",
      "Epoch (1/1) Batch (  500/4521 ) Iteration:      500 Avg. Training loss: 5.6160 0.2259 sec/batch\n",
      "Epoch (1/1) Batch (  600/4521 ) Iteration:      600 Avg. Training loss: 5.6623 0.2326 sec/batch\n",
      "Epoch (1/1) Batch (  700/4521 ) Iteration:      700 Avg. Training loss: 5.7017 0.2319 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 1024\n",
    "window_size = 5\n",
    "w2v_model.train(epochs, batch_size, window_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
