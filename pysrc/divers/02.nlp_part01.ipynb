{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with TensorFlow part01\n",
    "In this notebook we learn how to use NLP with TensorFlow via the following steps\n",
    "* word embeddings\n",
    "* language model with rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy, sys, time\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')\n",
    "\n",
    "import helper\n",
    "from gradient_check import rel_error\n",
    "source_path = '../common/data/small_vocab_en'\n",
    "target_path = '../common/data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "The first step is to create lookup tables word to integer-id and vice-versa, note that we always add some special word into the dictionary e.g\n",
    "~~~~\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text, special_codes):\n",
    "    vocab_to_int = copy.copy(special_codes)\n",
    "    vocab = set(text.split())\n",
    "    \n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "src_vocab_to_int, src_int_to_vocab = create_lookup_tables(source_text, CODES)\n",
    "des_vocab_to_int, des_int_to_vocab = create_lookup_tables(target_text, CODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given lookup tables, we need convert text into ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len 17 at 1\n",
      "min len  3 at 5057\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(text, vocab_to_int, append_eos = False):\n",
    "    eos = []\n",
    "    if append_eos:\n",
    "        eos = [vocab_to_int['<EOS>']]\n",
    "    \n",
    "    sequence_ids = []\n",
    "    for sent in text.split('\\n'):\n",
    "        sent_ids = [vocab_to_int[w] for w in sent.split()]\n",
    "        if len(sent_ids) > 0:\n",
    "            sequence_ids.append(sent_ids + eos)\n",
    "    return sequence_ids\n",
    "\n",
    "src_seq_ids = text_to_ids(source_text, src_vocab_to_int)\n",
    "des_seq_ids = text_to_ids(target_text, des_vocab_to_int, append_eos=True)\n",
    "\n",
    "i_max = np.argmax([len(s) for s in src_seq_ids])\n",
    "i_min = np.argmin([len(s) for s in src_seq_ids])\n",
    "print ('max len {:2d} at {}'.format(len(src_seq_ids[i_max]), i_max))\n",
    "print ('min len {:2d} at {}'.format(len(src_seq_ids[i_min]), i_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try word embedding with RNN\n",
    "In this section, we want to implement the encoder part of the following schema\n",
    "<img src=\"images/encoder_decoder.png\" width=\"600\"/>\n",
    "\n",
    "We will use the following helper functions\n",
    "* helper.pad_sentence_batch: we want all sentence in one batch has same length\n",
    "* [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence) to embed a sequence (run rnn for all sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab-size: 231\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# create interactive session \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create data\n",
    "input_data = tf.placeholder(tf.int32, shape = [None, None])\n",
    "src_vocab_size = len(src_vocab_to_int)\n",
    "src_embed_dim = 2\n",
    "\n",
    "print ('source vocab-size: {}'.format(src_vocab_size))\n",
    "\n",
    "# we create initilizer so we can control embedding-weights init\n",
    "embed_weights = np.linspace(0.0, 1.0, src_vocab_size * src_embed_dim, dtype=np.float32).reshape(src_vocab_size, \n",
    "                                                                                                src_embed_dim)\n",
    "\n",
    "\n",
    "embed_init = tf.constant_initializer(embed_weights)\n",
    "\n",
    "# we create embedding\n",
    "embed_input = tf.contrib.layers.embed_sequence(input_data, src_vocab_size, src_embed_dim, initializer=embed_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check embed layer\n",
    "We will run embed-layer, we should expect **embed-outputs** match with **embed_weights**, we only test for two batches with different seq-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 17)\n",
      "word[0,1] = 201\n",
      "embed_vals[0,1] = [ 0.87201732  0.87418658]\n",
      "embed_weight[201] = [ 0.87201732  0.87418658]\n",
      "rel-err 0.000000e+00\n",
      "(2, 9)\n",
      "word[1,7] = 73\n",
      "embed_vals[1,7] = [ 0.31670281  0.318872  ]\n",
      "embed_weight[73] = [ 0.31670281  0.318872  ]\n",
      "rel-err 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 2\n",
    "indices = [1, 5057]\n",
    "for idx in indices:\n",
    "    test_batch = np.array(helper.pad_sentence_batch(src_seq_ids[idx:idx+batch_size]))\n",
    "    print (test_batch.shape)\n",
    "    embed_vals = sess.run(embed_input, feed_dict={input_data:test_batch})\n",
    "    seq_len = test_batch.shape[1]\n",
    "    w = 0\n",
    "    while (w==0): \n",
    "        i = np.random.randint(batch_size)\n",
    "        j = np.random.randint(seq_len)\n",
    "        w = test_batch[i,j]\n",
    "    print ('word[{},{}] = {}'.format(i, j, test_batch[i,j]))\n",
    "    print ('embed_vals[{},{}] = {}'.format(i, j, embed_vals[i,j]))\n",
    "    print ('embed_weight[{}] = {}'.format(test_batch[i,j], embed_weights[test_batch[i,j]]))\n",
    "    print ('rel-err {:e}'.format(rel_error(embed_vals[i,j], embed_weights[test_batch[i,j]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement encoder layer  \n",
    "Given embed_input ($w_1,...,w_n$), we are ready to make it passed through a RNN encoder. Since the seq-len is variable, we will use \n",
    "\n",
    "* [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to perform un-roll rnn encoder\n",
    "* [`tf.contrib.rnn.BasicRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell) or [`tf.contrib.rnn.BasicLSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) to model a cell in our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbedSequence/embeddings:0\n",
      "rnn/basic_rnn_cell/weights:0\n",
      "rnn/basic_rnn_cell/biases:0\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 3\n",
    "\n",
    "enc_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "_, enc_state = tf.nn.dynamic_rnn(enc_cell, embed_input, dtype=tf.float32)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for var in tvars:\n",
    "    print(var.name)  # Prints the name of the variable alongside its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09072495  0.36186045  0.51128739]\n",
      " [ 0.09670514  0.46966249  0.58714062]\n",
      " [ 0.1749981   0.34577197  0.19017464]\n",
      " [-0.38420555 -0.26265156 -0.20659775]\n",
      " [-0.42786956  0.74334031  0.68960029]]\n",
      "[ 0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "rnn_w = [var for var in tvars if var.name == 'rnn/basic_rnn_cell/weights:0']\n",
    "rnn_b = [var for var in tvars if var.name == 'rnn/basic_rnn_cell/biases:0']\n",
    "print (sess.run(rnn_w[0]))\n",
    "print (sess.run(rnn_b[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
