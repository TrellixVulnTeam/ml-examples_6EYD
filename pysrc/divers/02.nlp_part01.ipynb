{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with TensorFlow part01\n",
    "In this notebook we learn how to use NLP with TensorFlow via the following steps\n",
    "* word embeddings\n",
    "* language model with rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy, sys, time\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')\n",
    "\n",
    "import helper\n",
    "from gradient_check import rel_error\n",
    "source_path = '../common/data/small_vocab_en'\n",
    "target_path = '../common/data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "The first step is to create lookup tables word to integer-id and vice-versa, note that we always add some special word into the dictionary e.g\n",
    "~~~~\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text, special_codes):\n",
    "    vocab_to_int = copy.copy(special_codes)\n",
    "    vocab = set(text.split())\n",
    "    \n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "src_vocab_to_int, src_int_to_vocab = create_lookup_tables(source_text, CODES)\n",
    "des_vocab_to_int, des_int_to_vocab = create_lookup_tables(target_text, CODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given lookup tables, we need convert text into ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len 17 at 1\n",
      "min len  3 at 5057\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(text, vocab_to_int, append_eos = False):\n",
    "    eos = []\n",
    "    if append_eos:\n",
    "        eos = [vocab_to_int['<EOS>']]\n",
    "    \n",
    "    sequence_ids = []\n",
    "    for sent in text.split('\\n'):\n",
    "        sent_ids = [vocab_to_int[w] for w in sent.split()]\n",
    "        if len(sent_ids) > 0:\n",
    "            sequence_ids.append(sent_ids + eos)\n",
    "    return sequence_ids\n",
    "\n",
    "src_seq_ids = text_to_ids(source_text, src_vocab_to_int)\n",
    "des_seq_ids = text_to_ids(target_text, des_vocab_to_int, append_eos=True)\n",
    "\n",
    "i_max = np.argmax([len(s) for s in src_seq_ids])\n",
    "i_min = np.argmin([len(s) for s in src_seq_ids])\n",
    "print ('max len {:2d} at {}'.format(len(src_seq_ids[i_max]), i_max))\n",
    "print ('min len {:2d} at {}'.format(len(src_seq_ids[i_min]), i_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try word embedding with RNN\n",
    "In this section, we want to implement the encoder part of the following schema\n",
    "<img src=\"images/encoder_decoder.png\" width=\"600\"/>\n",
    "\n",
    "We will use the following helper functions\n",
    "* helper.pad_sentence_batch: we want all sentence in one batch has same length\n",
    "* [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence) to embed a sequence (run rnn for all sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab-size: 231\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# create interactive session \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# create data\n",
    "input_data = tf.placeholder(tf.int32, shape = [None, None])\n",
    "src_vocab_size = len(src_vocab_to_int)\n",
    "src_embed_dim = 2\n",
    "\n",
    "print ('source vocab-size: {}'.format(src_vocab_size))\n",
    "\n",
    "# we create initilizer so we can control embedding-weights init\n",
    "embed_weights = np.linspace(0.0, 1.0, src_vocab_size * src_embed_dim, dtype=np.float32).reshape(src_vocab_size, \n",
    "                                                                                                src_embed_dim)\n",
    "\n",
    "\n",
    "embed_init = tf.constant_initializer(embed_weights)\n",
    "\n",
    "# we create embedding\n",
    "embed_input = tf.contrib.layers.embed_sequence(input_data, src_vocab_size, src_embed_dim, initializer=embed_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check embed layer\n",
    "We will run embed-layer, we should expect **embed-outputs** match with **embed_weights**, we only test for two batches with different seq-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 17)\n",
      "word[1,7] = 127\n",
      "embed_vals[1,7] = [ 0.55097616  0.55314535]\n",
      "embed_weight[127] = [ 0.55097616  0.55314535]\n",
      "rel-err 0.000000e+00\n",
      "\n",
      "(2, 9)\n",
      "word[1,5] = 113\n",
      "embed_vals[1,5] = [ 0.49023861  0.4924078 ]\n",
      "embed_weight[113] = [ 0.49023861  0.4924078 ]\n",
      "rel-err 0.000000e+00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we initilize our variable, another way is to use tf.assign\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 2\n",
    "indices = [1, 5057]\n",
    "batch_datas = []\n",
    "for idx in indices:\n",
    "    test_batch = np.array(helper.pad_sentence_batch(src_seq_ids[idx:idx+batch_size]))\n",
    "    print (test_batch.shape)\n",
    "    batch_datas.append(test_batch)\n",
    "    embed_vals = sess.run(embed_input, feed_dict={input_data:test_batch})\n",
    "    seq_len = test_batch.shape[1]\n",
    "    w = 0\n",
    "    while (w==0): \n",
    "        i = np.random.randint(batch_size)\n",
    "        j = np.random.randint(seq_len)\n",
    "        w = test_batch[i,j]\n",
    "    print ('word[{},{}] = {}'.format(i, j, test_batch[i,j]))\n",
    "    print ('embed_vals[{},{}] = {}'.format(i, j, embed_vals[i,j]))\n",
    "    print ('embed_weight[{}] = {}'.format(test_batch[i,j], embed_weights[test_batch[i,j]]))\n",
    "    print ('rel-err {:e}\\n'.format(rel_error(embed_vals[i,j], embed_weights[test_batch[i,j]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement encoder layer  \n",
    "Given embed_input ($w_1,...,w_n$), we are ready to make it passed through a RNN encoder. Since the seq-len is variable, we will use \n",
    "\n",
    "* [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to perform un-roll rnn encoder\n",
    "* [`tf.contrib.rnn.BasicRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell) or [`tf.contrib.rnn.BasicLSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) to model a cell in our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_size = 4\n",
    "\n",
    "enc_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "_, enc_state = tf.nn.dynamic_rnn(enc_cell, embed_input, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbedSequence/embeddings:0\n",
      "rnn/basic_rnn_cell/weights:0\n",
      "rnn/basic_rnn_cell/biases:0\n"
     ]
    }
   ],
   "source": [
    "# print all variable\n",
    "tvars = tf.global_variables()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for var in tvars:\n",
    "    print(var.name)  # Prints the name of the variable alongside its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect variables\n",
    "We look at our trainable variables:\n",
    "* embedding-weights variable: **EmbedSequence/embeddings:0**\n",
    "* rnn-weights variable: **rnn/basic_rnn_cell/weights:0**\n",
    "* rnn-biases variable: **rnn/basic_rnn_cell/biases:0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_ew has shape [231, 2]\n",
      "[6, 4]\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "rnn_ew = [var for var in tvars if var.name == 'EmbedSequence/embeddings:0'][0]\n",
    "rnn_w  = [var for var in tvars if var.name == 'rnn/basic_rnn_cell/weights:0'][0]\n",
    "rnn_b  = [var for var in tvars if var.name == 'rnn/basic_rnn_cell/biases:0'][0]\n",
    "\n",
    "# we should expect rnn_ew.shape = (vocab_size = 231, embed_dim = 2)\n",
    "print ('rnn_ew has shape {}'.format(rnn_ew.get_shape().as_list()))\n",
    "\n",
    "# we should expect rnn_w.shape = (embed_dim + rnn_size, rnn_size)\n",
    "print (rnn_w.get_shape().as_list())\n",
    "\n",
    "# we should expect rnn_b.shape = (rnn_size)\n",
    "print (rnn_b.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN encoder\n",
    "Let's run RNN encoder with an input data to verify if it follows the following dynamics\n",
    "$$\n",
    "h_0 = (0,\\ldots,0) \\in \\mathbb{R}^H, x_t \\in \\mathbb{R}^D, W \\in \\mathbb{R}^{(D+H)\\times H}, b \\in \\mathbb{R}^H\n",
    "$$\n",
    "with update rule\n",
    "$$\n",
    "h_t = \\tanh\\left( x_{t} \\times W[0:D,:] +  h_{t-1}\\times W[D:,:] + b\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input:  (2, 2, 2)\n",
      "encoder output: (2, 4)\n",
      "(6, 4)\n",
      "[ 0.29934925  0.30151844]\n",
      "[[-0.2107414  -0.20193748  0.57699555  0.60662717]\n",
      " [-0.29763207 -0.18281864  0.62321222  0.66580057]]\n",
      "[[-0.21074142 -0.2019375   0.57699555  0.60662705]\n",
      " [-0.29763207 -0.18281864  0.62321222  0.66580051]]\n",
      "\n",
      "rel-error: 9.825582e-08\n"
     ]
    }
   ],
   "source": [
    "# let run rnn now, we reduce the dimension to verify it easier\n",
    "seq_in = batch_datas[0][:,0:2]\n",
    "enc_in  = sess.run(embed_input, feed_dict={input_data : seq_in})\n",
    "enc_out = sess.run(enc_state, feed_dict={input_data : seq_in})\n",
    "print ('encoder input:  {}'.format(enc_in.shape))\n",
    "print ('encoder output: {}'.format(enc_out.shape))\n",
    "\n",
    "w_v = rnn_w.eval()\n",
    "b_v = rnn_b.eval()\n",
    "print (w_v.shape)\n",
    "print (enc_in[0,0,:]) \n",
    "print (enc_out)\n",
    "h0 = np.zeros((2,4), dtype=np.float32)\n",
    "h1 = np.tanh(enc_in[:,0,:].dot(w_v[0:2,:]) + h0.dot(w_v[2:,:]) + b_v)\n",
    "h2 = np.tanh(enc_in[:,1,:].dot(w_v[0:2,:]) + h1.dot(w_v[2:,:]) + b_v)\n",
    "print (h2)\n",
    "print ('\\nrel-error: {:e}'.format(rel_error(enc_out, h2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that RNN works as epected, we do see some error since Tensorflow uses different math-backend (Eigen) than Numpy with MKL.\n",
    "\n",
    "Now let's look at LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
