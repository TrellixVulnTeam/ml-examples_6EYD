{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Language Models\n",
    "As seen in previous example, RNNs works well for sequential dataset. In this notebook, we want to train RNNs character-level language models i.e we'll give the RNNs a huge trunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.\n",
    "\n",
    "We recall the vanilla-RNNs dynamics\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "h_t &= \\tanh\\left(x_t\\times W_{xh} + h_{t-1}\\times W_{hh} + b_{h}\\right)\\\\\n",
    "o_t &= \\mathrm{softmax}\\left(h_t\\times W_{ho} + b_{o}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "where \n",
    "* $x_t$ is one-hot encoding of an input character\n",
    "* $W_{xh}$ is the input-to-hidden weight matrix\n",
    "* $W_{hh}$ is the hidden-to-hidden weight matrix\n",
    "* $W_{ho}$ is the hidden-to-output weight matrix\n",
    "* $b_h$ and $b_o$ are the biases\n",
    "\n",
    "Here we use $o_t$ to model the  conditional distribution\n",
    "$$\n",
    "P(x_{t+1}=j| x_{\\leq t}) = o_t[j]\n",
    "$$\n",
    "\n",
    "First we import the libraries we need and define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if '../common' not in sys.path:\n",
    "    sys.path.insert(0, '../common')\n",
    "\n",
    "from rnn.mrnn import BasicMRNNCell                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training dataset\n",
    "We will train RNNs model on Anna Karenina (~2Mb).\n",
    "\n",
    "## Pre-processing\n",
    "First we need to do the following pre-processing\n",
    "* get the set of all characters\n",
    "* get the map character to ids and vice-versa\n",
    "* convert text to ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../common/data/anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "\n",
    "# get all unique characters\n",
    "vocabs = set(text)\n",
    "\n",
    "# get the map char-to-id and vice-versa\n",
    "vocab_to_id = {c: i for i, c in enumerate(vocabs)}\n",
    "id_to_vocab = dict(enumerate(vocabs))\n",
    "\n",
    "# convert text-input into ids\n",
    "char_ids = np.array([vocab_to_id[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 50 characters in text & ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n",
      "[33 80 10 55  1 51 69 56 14 76 76 76 41 10 55 55 22 56 62 10 65 66 81 66 51\n",
      " 18 56 10 69 51 56 10 81 81 56 10 81 66 48 51 52 56 51 61 51 69 22 56 39  7]\n"
     ]
    }
   ],
   "source": [
    "print(text[:50])\n",
    "print(chars[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batches\n",
    "Now we want to split data into mini-batches and into training and validation sets. We implement it in following helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(char_ids, batch_size, seq_len, split_frac = 0.9):\n",
    "    slice_size = batch_size*seq_len\n",
    "    nb_batches = (len(char_ids) - 1) // slice_size\n",
    "    \n",
    "    # get input/target\n",
    "    x = char_ids[  : nb_batches*slice_size]\n",
    "    y = char_ids[1 : nb_batches*slice_size+1]\n",
    "    \n",
    "    # split them to batches\n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # split into train/validation set\n",
    "    split_idx = int(nb_batches*split_frac) * seq_len\n",
    "    \n",
    "    train_x, train_y = x[:, :split_idx], y[:, :split_idx]\n",
    "    val_x, val_y = x[:, split_idx:], y[:, split_idx:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "def get_batches(train_inputs, train_targets, seq_len):\n",
    "    nb_batches = train_inputs.shape[1]//seq_len\n",
    "    idx = 0\n",
    "    for i in range(nb_batches):\n",
    "        idx += seq_len\n",
    "        yield train_inputs[:, idx-seq_len : idx], train_targets[:, idx-seq_len : idx]\n",
    "        \n",
    "def pick_top_idx(top_prob, top_idx):\n",
    "    c = np.random.choice(len(top_prob), 1, p = top_prob/np.sum(top_prob))[0]\n",
    "    return top_idx[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble a RNNs model\n",
    "As in previous post, we will use Tensorflow to create a RNNs model using the following functions\n",
    "* [`tf.one_hot`](https://www.tensorflow.org/api_docs/python/tf/one_hot) to convert target into one-hot representation\n",
    "* [`tf.contrib.rnn.BasicRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell) to model a basic RNN cell\n",
    "* [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to perform fully dynamic unrolling of our rnn i.e we compute the final state of our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rnn character-lever language model\n",
    "class CharRnn(object):\n",
    "    def __init__(self, vocabs, vocab_to_id, id_to_voca\n",
    "                     , cell_type, rnn_size, batch_size, seq_len\n",
    "                     , num_factors = 3, num_layers = 2, learning_rate = 0.001):\n",
    "        # set input\n",
    "        self._vocabs = vocabs\n",
    "        self._vocabs_size = len(vocabs)\n",
    "        \n",
    "        self._vocab_to_id = vocab_to_id\n",
    "        self._id_to_vocab = id_to_vocab\n",
    "        self._rnn_size = rnn_size\n",
    "        self._batch_size = batch_size\n",
    "        self._seq_len = seq_len\n",
    "        self._cell_type = cell_type\n",
    "        self._num_factors = num_factors\n",
    "        self._num_layers = num_layers\n",
    "        self._lr = learning_rate        \n",
    "        \n",
    "        # check input\n",
    "        assert (self._cell_type in ['rnn', 'mrnn', 'lstm', 'gru'])\n",
    "        assert (self._num_layers >= 1)\n",
    "        \n",
    "        # build graph\n",
    "        self.build_graph()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        # create placeholder for input/target\n",
    "        self._create_placeholder()\n",
    "        \n",
    "        # create rnn layers\n",
    "        self._create_rnn()\n",
    "        \n",
    "        # create loss/cost layers\n",
    "        self._create_loss()\n",
    "        \n",
    "        # create train-op & saver\n",
    "        self._create_train_op_saver()\n",
    "        \n",
    "        # create sample\n",
    "        self._create_sample()\n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        with self._graph.as_default():\n",
    "            # input & target has shape [batch_size, seq_len] \n",
    "            self._inputs  = tf.placeholder(tf.int32, [self._batch_size, None], name = 'inputs')\n",
    "            self._targets = tf.placeholder(tf.int32, [self._batch_size, None], name = 'targets')\n",
    "            \n",
    "            # convert to one-hot encoding\n",
    "            self._inputs_one_hot  = tf.one_hot(self._inputs,  self._vocabs_size)\n",
    "            self._targets_one_hot = tf.one_hot(self._targets, self._vocabs_size)\n",
    "            \n",
    "            # Keep probability placeholder for drop out layers\n",
    "            self._keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            \n",
    "    def _create_rnn(self):\n",
    "        with self._graph.as_default():\n",
    "            with tf.variable_scope('rnn_scopes') as vs:\n",
    "                # create rnn-cell\n",
    "                if self._cell_type == 'rnn':\n",
    "                    cell = tf.contrib.rnn.BasicRNNCell(self._rnn_size)\n",
    "                elif self._cell_type == 'mrnn':\n",
    "                    cell = BasicMRNNCell(self._rnn_size, self._num_factors)\n",
    "                elif self._cell_type == 'lstm':\n",
    "                    cell = tf.contrib.rnn.BasicLSTMCell(self._rnn_size)\n",
    "                elif self._cell_type == 'gru':\n",
    "                    cell = tf.contrib.rnn.GRUCell(self._rnn_size)\n",
    "                \n",
    "                if (self._num_layers == 1):\n",
    "                    self._cell = cell\n",
    "                else:\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self._keep_prob)\n",
    "                    self._cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "                \n",
    "                # get initial_state\n",
    "                self._initial_state = self._cell.zero_state(self._batch_size, dtype = tf.float32)\n",
    "                \n",
    "                # run rnn through inputs to create outputs & final-state                \n",
    "                self._outputs, self._final_state = tf.nn.dynamic_rnn(self._cell,\n",
    "                                                                     self._inputs_one_hot,\n",
    "                                                                     initial_state = self._initial_state)\n",
    "                \n",
    "                # Retrieve just the RNNs variables.\n",
    "                self._rnn_variables = [v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        with self._graph.as_default():\n",
    "            # create softmax-weight & biases\n",
    "            init_stddev = 1.0 / np.sqrt(self._vocabs_size)\n",
    "            self._softmax_weights = tf.Variable(tf.truncated_normal([self._rnn_size, self._vocabs_size],\n",
    "                                                                    stddev = init_stddev), name = 'softmax_w')\n",
    "            self._softmax_biases  = tf.Variable(tf.zeros(self._vocabs_size), name = 'softmax_b')\n",
    "            \n",
    "            # reshape outputs/targets so we can use tf.matmul/tf.nn.softmax_cross_entropy_with_logits\n",
    "            outputs_flat = tf.reshape(self._outputs, [-1, self._rnn_size])\n",
    "            targets_flat = tf.reshape(self._targets_one_hot, [-1, self._vocabs_size])\n",
    "            \n",
    "            # compute logits (input to softmax)        \n",
    "            self._logits = tf.matmul(outputs_flat, self._softmax_weights) + self._softmax_biases\n",
    "            \n",
    "            # compute the cross-entropy loss at each time-step\n",
    "            self._loss = tf.nn.softmax_cross_entropy_with_logits(logits=self._logits, \n",
    "                                                                 labels=targets_flat)\n",
    "            \n",
    "            # cost is the reduce_mean of loss at all time-step\n",
    "            self._cost = tf.reduce_mean(self._loss)\n",
    "    \n",
    "    def _create_train_op_saver(self):\n",
    "        with self._graph.as_default():\n",
    "            # apply gradient clipping to control exploiding gradient\n",
    "            tvars    = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), 5.0)\n",
    "            \n",
    "            # create train-op with gradient clipping\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self._lr)\n",
    "            self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            \n",
    "            # create saver\n",
    "            self._saver = tf.train.Saver(max_to_keep=100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, train_inputs, train_targets, \n",
    "              val_inputs, val_targets,\n",
    "              epochs, save_every=50, \n",
    "              save_dir = 'checkpoints', keep_prob = 0.5):\n",
    "        with tf.Session(graph=self._graph) as sess:\n",
    "            # initialize variable\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # compute nb of iterations\n",
    "            nb_batches = train_inputs.shape[1]//self._seq_len            \n",
    "            nb_iters   = epochs * nb_batches\n",
    "            \n",
    "            # number of batches in validation set\n",
    "            val_nb_batches = val_inputs.shape[1]//self._seq_len\n",
    "            \n",
    "            iteration  = 0\n",
    "            for e in range(epochs):\n",
    "                # reset initial-state to 0\n",
    "                new_state  = sess.run(self._initial_state)\n",
    "                train_loss = 0.\n",
    "                b = 0\n",
    "                for inputs, targets in get_batches(train_inputs, train_targets, self._seq_len):\n",
    "                    # run the training-op\n",
    "                    # note that the final state of one batch shoud be used as initial-state of next batch\n",
    "                    start = time.time()\n",
    "                    batch_loss, new_state, _ = sess.run([self._cost, self._final_state, self._train_op],\n",
    "                                                        feed_dict = {self._inputs  : inputs,\n",
    "                                                                     self._targets : targets,\n",
    "                                                                     self._keep_prob : keep_prob,\n",
    "                                                                     self._initial_state : new_state})\n",
    "                    \n",
    "                    end = time.time()\n",
    "                    train_loss += batch_loss\n",
    "                    b          += 1\n",
    "                    iteration  +=1\n",
    "                    \n",
    "                    sys.stdout.write('\\rEpoch {}/{}'.format(e+1, epochs) + \n",
    "                                     ' Iteration {}/{}'.format(iteration, nb_iters) +\n",
    "                                     ' Training loss: {:.4f}'.format(train_loss/b) +\n",
    "                                     ' Running {:.4f} sec/batch'.format((end-start)))\n",
    "                        \n",
    "                    if (   (iteration%save_every == 0) \n",
    "                        or (iteration == nb_iters)):\n",
    "                        \n",
    "                        # reset state for validation set\n",
    "                        val_state = sess.run(self._initial_state)\n",
    "                        val_loss  = 0.\n",
    "                        \n",
    "                        # run rnn and measure the loss on validation set\n",
    "                        for val_x, val_y in get_batches(val_inputs, val_targets, self._seq_len):\n",
    "                            batch_loss, val_state = sess.run([self._cost, self._final_state],\n",
    "                                                             feed_dict = {self._inputs  : val_x,\n",
    "                                                                          self._targets : val_y,\n",
    "                                                                          self._keep_prob : 1.0,\n",
    "                                                                          self._initial_state : val_state})\n",
    "                            val_loss += batch_loss\n",
    "                        \n",
    "                        val_loss /= val_nb_batches\n",
    "                        # report validation loss & save down checkpoints\n",
    "                        print('\\nValidation loss: {:.4f}'.format(val_loss), 'Saving checkpoint!\\n')\n",
    "                        save_path = '{}/cell_{}_i{}_l{}_v{:.4f}.ckpt'.format(save_dir,\n",
    "                                                                             self._cell_type,\n",
    "                                                                             iteration, \n",
    "                                                                             self._rnn_size, \n",
    "                                                                             val_loss)\n",
    "                        self._saver.save(sess, save_path)\n",
    "\n",
    "    def _create_sample(self):\n",
    "        with self._graph.as_default():\n",
    "            dist = tf.nn.softmax(self._logits)\n",
    "            top_probs, top_indices = tf.nn.top_k(dist, k = 3)\n",
    "            self._top_probs   = tf.reshape(top_probs, [-1])\n",
    "            self._top_indices = tf.reshape(top_indices, [-1])\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        sess = tf.Session(graph = self._graph)\n",
    "        self._saver.restore(sess, checkpoint)\n",
    "        return sess\n",
    "    \n",
    "    def sample_text(self, sess, sample_len, prime = 'The '):\n",
    "        '''\n",
    "        We generate new text that given current text (prime)\n",
    "        '''\n",
    "        new_state = sess.run(self._initial_state)\n",
    "        \n",
    "        for c in prime:\n",
    "            c_id = self._vocab_to_id[c]\n",
    "            inputs = np.array([c_id]).reshape([1,1])\n",
    "            \n",
    "            # forward a single time-step \n",
    "            new_state, top_prob, top_idx = sess.run([self._final_state, self._top_probs, self._top_indices], \n",
    "                                                    feed_dict = {self._inputs : inputs, \n",
    "                                                                 self._keep_prob : 1.0,\n",
    "                                                                 self._initial_state : new_state})\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        # pick next most probable character\n",
    "        c_id  = pick_top_idx(top_prob, top_idx)\n",
    "        samples.append(self._id_to_vocab[c_id])\n",
    "        \n",
    "        for i in range(sample_len-1):\n",
    "            inputs = np.array([c_id]).reshape([1,1])\n",
    "            \n",
    "            # forward a single time-step \n",
    "            new_state, top_prob, top_idx = sess.run([self._final_state, self._top_probs, self._top_indices], \n",
    "                                                    feed_dict = {self._inputs : inputs, \n",
    "                                                                 self._keep_prob : 1.0,\n",
    "                                                                 self._initial_state : new_state})\n",
    "            c_id  = pick_top_idx(top_prob, top_idx)\n",
    "            samples.append(self._id_to_vocab[c_id])\n",
    "        \n",
    "        return ''.join(samples)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RNNs\n",
    "In this section, we will train our RNNs with various cell-type\n",
    "* [`BasicRNNCell`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicRNNCell)\n",
    "* [`BasicLSTMCell`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "* [`BasicGRUCell`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicGRUCell)\n",
    "* [`BasicMRNNCell`](http://www.icml-2011.org/papers/524_icmlpaper.pdf)\n",
    "\n",
    "First create checkpoint directory so we can store trained-model's checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/crnn’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# create dir to store checkpoints\n",
    "!mkdir checkpoints/crnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with BasicRNNCell\n",
    "Before train with BasicRNNCell, we inspect the variables' shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input-shape [128, 83]\n",
      "input-shape [128, 256]\n",
      "initial_state is a tuple of len 2 each has shape \n",
      "\t(128, 256) i.e (batch_size, rnn_size)\n",
      "\n",
      "rnn weights and biases:\n",
      "\trnn_scopes/rnn/multi_rnn_cell/cell_0/basic_rnn_cell/weights:0     rank 2 shape [339, 256]\n",
      "\trnn_scopes/rnn/multi_rnn_cell/cell_0/basic_rnn_cell/biases:0      rank 1 shape [256]\n",
      "\trnn_scopes/rnn/multi_rnn_cell/cell_1/basic_rnn_cell/weights:0     rank 2 shape [512, 256]\n",
      "\trnn_scopes/rnn/multi_rnn_cell/cell_1/basic_rnn_cell/biases:0      rank 1 shape [256]\n",
      "at each layer:\n",
      "\tweights should has shape [input_dim + hidden_dim, hidden_dim] \n",
      "\tbiases should has shape  [hidden_dim]\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 256\n",
    "batch_size = 128\n",
    "seq_len = 64\n",
    "num_layers = 2\n",
    "cell_type = 'rnn'\n",
    "\n",
    "# rnn models\n",
    "crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "               rnn_size = rnn_size, batch_size = batch_size, \n",
    "               seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "# view shape\n",
    "if num_layers > 1:\n",
    "    print ('initial_state is a tuple of len {} each has shape \\n\\t{} i.e (batch_size, rnn_size)\\n'.format(\n",
    "                                                                        len(crnn._initial_state),\n",
    "                                                                        crnn._initial_state[0].get_shape()))\n",
    "    \n",
    "    print ('rnn weights and biases:')\n",
    "    for v in crnn._rnn_variables:\n",
    "        print ('\\t{:<65} rank {} shape {}'.format(v.name, v.get_shape().ndims, v.get_shape().as_list()))\n",
    "    \n",
    "    print ('at each layer:\\n\\tweights should has shape [input_dim + hidden_dim, hidden_dim]',\n",
    "                         '\\n\\tbiases should has shape  [hidden_dim]')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with MRNN cell\n",
    "We try out the MRNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "rnn_size = 256\n",
    "batch_size = 128\n",
    "seq_len = 64\n",
    "num_factors= 3\n",
    "num_layers = 1\n",
    "cell_type = 'mrnn'\n",
    "\n",
    "# rnn models\n",
    "crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "               num_factors = num_factors, rnn_size = rnn_size, \n",
    "               batch_size = batch_size, seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "# create train/validation dataset\n",
    "train_x, train_y, val_x, val_y = split_data(char_ids, batch_size, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train MRNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 Iteration 500/10850 Training loss: 2.2269 Running 0.0389 sec/batch\n",
      "Validation loss: 2.2319 Saving checkpoint!\n",
      "\n",
      "Epoch 5/50 Iteration 1000/10850 Training loss: 2.1534 Running 0.0255 sec/batch\n",
      "Validation loss: 2.1602 Saving checkpoint!\n",
      "\n",
      "Epoch 7/50 Iteration 1500/10850 Training loss: 2.1125 Running 0.0284 sec/batch\n",
      "Validation loss: 2.1161 Saving checkpoint!\n",
      "\n",
      "Epoch 10/50 Iteration 2000/10850 Training loss: 2.0608 Running 0.0292 sec/batch\n",
      "Validation loss: 2.0785 Saving checkpoint!\n",
      "\n",
      "Epoch 12/50 Iteration 2500/10850 Training loss: 2.0302 Running 0.0326 sec/batch\n",
      "Validation loss: 2.0435 Saving checkpoint!\n",
      "\n",
      "Epoch 14/50 Iteration 3000/10850 Training loss: 2.0030 Running 0.0280 sec/batch\n",
      "Validation loss: 2.0101 Saving checkpoint!\n",
      "\n",
      "Epoch 17/50 Iteration 3500/10850 Training loss: 1.9665 Running 0.0292 sec/batch\n",
      "Validation loss: 1.9803 Saving checkpoint!\n",
      "\n",
      "Epoch 19/50 Iteration 4000/10850 Training loss: 1.9373 Running 0.0257 sec/batch\n",
      "Validation loss: 1.9518 Saving checkpoint!\n",
      "\n",
      "Epoch 21/50 Iteration 4500/10850 Training loss: 1.9152 Running 0.0282 sec/batch\n",
      "Validation loss: 1.9271 Saving checkpoint!\n",
      "\n",
      "Epoch 24/50 Iteration 5000/10850 Training loss: 1.8869 Running 0.0263 sec/batch\n",
      "Validation loss: 1.9028 Saving checkpoint!\n",
      "\n",
      "Epoch 26/50 Iteration 5500/10850 Training loss: 1.8679 Running 0.0263 sec/batch\n",
      "Validation loss: 1.8848 Saving checkpoint!\n",
      "\n",
      "Epoch 28/50 Iteration 6000/10850 Training loss: 1.8529 Running 0.0264 sec/batch\n",
      "Validation loss: 1.8667 Saving checkpoint!\n",
      "\n",
      "Epoch 30/50 Iteration 6500/10850 Training loss: 1.8413 Running 0.0267 sec/batch\n",
      "Validation loss: 1.8504 Saving checkpoint!\n",
      "\n",
      "Epoch 33/50 Iteration 7000/10850 Training loss: 1.8182 Running 0.0268 sec/batch\n",
      "Validation loss: 1.8376 Saving checkpoint!\n",
      "\n",
      "Epoch 35/50 Iteration 7500/10850 Training loss: 1.8090 Running 0.0306 sec/batch\n",
      "Validation loss: 1.8274 Saving checkpoint!\n",
      "\n",
      "Epoch 37/50 Iteration 8000/10850 Training loss: 1.8014 Running 0.0270 sec/batch\n",
      "Validation loss: 1.8151 Saving checkpoint!\n",
      "\n",
      "Epoch 40/50 Iteration 8500/10850 Training loss: 1.7859 Running 0.0287 sec/batch\n",
      "Validation loss: 1.8039 Saving checkpoint!\n",
      "\n",
      "Epoch 42/50 Iteration 9000/10850 Training loss: 1.7766 Running 0.0275 sec/batch\n",
      "Validation loss: 1.7945 Saving checkpoint!\n",
      "\n",
      "Epoch 44/50 Iteration 9500/10850 Training loss: 1.7715 Running 0.0284 sec/batch\n",
      "Validation loss: 1.7867 Saving checkpoint!\n",
      "\n",
      "Epoch 47/50 Iteration 10000/10850 Training loss: 1.7655 Running 0.0280 sec/batch\n",
      "Validation loss: 1.7783 Saving checkpoint!\n",
      "\n",
      "Epoch 49/50 Iteration 10500/10850 Training loss: 1.7519 Running 0.0265 sec/batch\n",
      "Validation loss: 1.7717 Saving checkpoint!\n",
      "\n",
      "Epoch 50/50 Iteration 10850/10850 Training loss: 1.7519 Running 0.0273 sec/batch\n",
      "Validation loss: 1.7655 Saving checkpoint!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "keep_prob = 0.5\n",
    "crnn.train(train_x, train_y, val_x, val_y, \n",
    "           epochs=epochs,\n",
    "           save_every=500, \n",
    "           keep_prob=keep_prob,\n",
    "           save_dir= 'checkpoints/crnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to generate some new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime:  Happy families are \n",
      "Sample: take was the sair.\"\n",
      "\n",
      "\"I'm\n",
      "his been the conversan and a she with the said, will he had breating that and\n",
      "was\n",
      "sayices that and to he seent the seen a come were had he said the were,\" salting him, a come\n"
     ]
    }
   ],
   "source": [
    "val_crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "                   rnn_size = rnn_size, batch_size = 1, \n",
    "                   seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "sess = val_crnn.load_checkpoint('checkpoints/crnn/crnn_mrnn_i10850_l256_v1.7655.ckpt')\n",
    "\n",
    "prime = 'Happy families are '\n",
    "new_text = val_crnn.sample_text(sess, 200, prime=prime)\n",
    "print ('Prime:  {}\\nSample: {}'.format(prime, new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with LSTM cell\n",
    "Let's create a RNNs model so that we can train it with given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/crnn’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "rnn_size = 256\n",
    "batch_size = 128\n",
    "seq_len = 64\n",
    "num_layers = 2\n",
    "cell_type = 'lstm'\n",
    "\n",
    "# rnn models\n",
    "crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "               rnn_size = rnn_size, batch_size = batch_size, \n",
    "               seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "# create train/validation dataset\n",
    "train_x, train_y, val_x, val_y = split_data(char_ids, batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for training, we pass train/validation dataset to the `train` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 Iteration 500/10850 Training loss: 2.0730 Running 0.0841 sec/batch\n",
      "Validation loss: 1.9345 Saving checkpoint!\n",
      "\n",
      "Epoch 5/50 Iteration 1000/10850 Training loss: 1.7867 Running 0.0967 sec/batch\n",
      "Validation loss: 1.6416 Saving checkpoint!\n",
      "\n",
      "Epoch 7/50 Iteration 1500/10850 Training loss: 1.6519 Running 0.0830 sec/batch\n",
      "Validation loss: 1.5002 Saving checkpoint!\n",
      "\n",
      "Epoch 10/50 Iteration 2000/10850 Training loss: 1.5484 Running 0.0928 sec/batch\n",
      "Validation loss: 1.4180 Saving checkpoint!\n",
      "\n",
      "Epoch 12/50 Iteration 2500/10850 Training loss: 1.4964 Running 0.0886 sec/batch\n",
      "Validation loss: 1.3674 Saving checkpoint!\n",
      "\n",
      "Epoch 14/50 Iteration 3000/10850 Training loss: 1.4573 Running 0.1189 sec/batch\n",
      "Validation loss: 1.3287 Saving checkpoint!\n",
      "\n",
      "Epoch 17/50 Iteration 3500/10850 Training loss: 1.4232 Running 0.0827 sec/batch\n",
      "Validation loss: 1.3010 Saving checkpoint!\n",
      "\n",
      "Epoch 19/50 Iteration 4000/10850 Training loss: 1.3974 Running 0.0842 sec/batch\n",
      "Validation loss: 1.2790 Saving checkpoint!\n",
      "\n",
      "Epoch 21/50 Iteration 4500/10850 Training loss: 1.3790 Running 0.0870 sec/batch\n",
      "Validation loss: 1.2634 Saving checkpoint!\n",
      "\n",
      "Epoch 24/50 Iteration 5000/10850 Training loss: 1.3617 Running 0.0926 sec/batch\n",
      "Validation loss: 1.2523 Saving checkpoint!\n",
      "\n",
      "Epoch 26/50 Iteration 5500/10850 Training loss: 1.3438 Running 0.0967 sec/batch\n",
      "Validation loss: 1.2378 Saving checkpoint!\n",
      "\n",
      "Epoch 28/50 Iteration 6000/10850 Training loss: 1.3302 Running 0.0915 sec/batch\n",
      "Validation loss: 1.2245 Saving checkpoint!\n",
      "\n",
      "Epoch 30/50 Iteration 6500/10850 Training loss: 1.3159 Running 0.0839 sec/batch\n",
      "Validation loss: 1.2119 Saving checkpoint!\n",
      "\n",
      "Epoch 33/50 Iteration 7000/10850 Training loss: 1.2995 Running 0.1000 sec/batch\n",
      "Validation loss: 1.1988 Saving checkpoint!\n",
      "\n",
      "Epoch 35/50 Iteration 7500/10850 Training loss: 1.2895 Running 0.0794 sec/batch\n",
      "Validation loss: 1.1935 Saving checkpoint!\n",
      "\n",
      "Epoch 37/50 Iteration 8000/10850 Training loss: 1.2843 Running 0.0850 sec/batch\n",
      "Validation loss: 1.1863 Saving checkpoint!\n",
      "\n",
      "Epoch 40/50 Iteration 8500/10850 Training loss: 1.2739 Running 0.0861 sec/batch\n",
      "Validation loss: 1.1800 Saving checkpoint!\n",
      "\n",
      "Epoch 42/50 Iteration 9000/10850 Training loss: 1.2645 Running 0.1141 sec/batch\n",
      "Validation loss: 1.1751 Saving checkpoint!\n",
      "\n",
      "Epoch 44/50 Iteration 9500/10850 Training loss: 1.2612 Running 0.0805 sec/batch\n",
      "Validation loss: 1.1738 Saving checkpoint!\n",
      "\n",
      "Epoch 47/50 Iteration 10000/10850 Training loss: 1.2661 Running 0.0790 sec/batch\n",
      "Validation loss: 1.1680 Saving checkpoint!\n",
      "\n",
      "Epoch 49/50 Iteration 10500/10850 Training loss: 1.2470 Running 0.0786 sec/batch\n",
      "Validation loss: 1.1655 Saving checkpoint!\n",
      "\n",
      "Epoch 50/50 Iteration 10850/10850 Training loss: 1.2463 Running 0.0805 sec/batch\n",
      "Validation loss: 1.1623 Saving checkpoint!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "keep_prob = 0.5\n",
    "crnn.train(train_x, train_y, val_x, val_y, \n",
    "           epochs=epochs,\n",
    "           save_every=500, \n",
    "           keep_prob=keep_prob,\n",
    "           save_dir= 'checkpoints/crnn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuation\n",
    "Now that the RNNs is trained, we want to use it to generate some new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime:  Happy families are \n",
      "Sample: all the cannot on the point and the carriage that he had no decain as he would\n",
      "be talking and had\n",
      "been seening to say,\n",
      "as he had\n",
      "seen her and the country, and had been to be saying in the position, as\n"
     ]
    }
   ],
   "source": [
    "val_crnn = CharRnn(vocabs, vocab_to_id, id_to_vocab, cell_type,\n",
    "                   rnn_size = rnn_size, batch_size = 1, \n",
    "                   seq_len = seq_len, num_layers=num_layers)\n",
    "\n",
    "sess = val_crnn.load_checkpoint('checkpoints/crnn/crnn_i4340_l256_v1.2683.ckpt')\n",
    "\n",
    "prime = 'Happy families are '\n",
    "new_text = val_crnn.sample_text(sess, 200, prime=prime)\n",
    "print ('Prime:  {}\\nSample: {}'.format(prime, new_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
