<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning Examples on Machine Learning Examples</title>
    <link>https://minh84.github.io/ml-examples/index.xml</link>
    <description>Recent content in Machine Learning Examples on Machine Learning Examples</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Mar 2017 22:18:54 +0000</lastBuildDate>
    <atom:link href="/ml-examples/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title></title>
      <link>https://minh84.github.io/ml-examples/demos/learn_tf/02-linear-classifier-softmax/</link>
      <pubDate>Fri, 10 Mar 2017 22:18:54 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/demos/learn_tf/02-linear-classifier-softmax/</guid>
      <description>Linear classifier with softmax&amp;#182;We continue from the previous notebook linear classifier with svm, in this note book we consider a different loss function the Softmax function $$ L(y, s(x)) = -\log\left(\frac{e^{s_y(x)}}{\sum_je^{s_j(x)}}\right) $$ we recall some notation $\newcommand{\real}{\mathbb{R}} \newcommand{\vi}[1]{#1^{(i)}} \newcommand{\vik}[1]{#1^{(i)}_k} \newcommand{\vij}[2]{#1^{(i)}_{#2}} $
 $x\in\real^{D\times 1}$ is input features  $s(x)$ is linear score of given by $$ s(x) = W^T\times x $$ with $W\in \real^{C\times D}$ with $C$ is number of classes.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://minh84.github.io/ml-examples/demos/learn_tf/01-linear-classifier-svm/</link>
      <pubDate>Thu, 09 Mar 2017 16:00:03 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/demos/learn_tf/01-linear-classifier-svm/</guid>
      <description>Classification Problem&amp;#182;In this notebook we work on classification of CIFAR-10 dataset. We define some notation to use later on
 $x^{(i)}$ are input-images each has shape 32x32x3 (RGB) $y^{(i)}$ are labels of above images and can take values $0,\ldots,9$ corresponding to 10 classes  To solve this classification, we try to find a function $h$ that maps from image $x$ to scores i.e $$ h: x \mapsto \left(\begin{array}{c}s_0(x)\\ \ldots\\ s_9(x)\end{array}\right) $$ where $s_i(x)$ is score of $x$ in $i-$th class.</description>
    </item>
    
    <item>
      <title>Learn TensorFlow P01</title>
      <link>https://minh84.github.io/ml-examples/post/learn-tensorflow-p01/</link>
      <pubDate>Thu, 09 Mar 2017 15:42:16 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/post/learn-tensorflow-p01/</guid>
      <description>In this blog series we will learn TensorFlow by test it v.s Numpy implementation. We start by implement a Linear Classifier on CIFAR10 dataset. We look into the following topics
 Linear classifier with Multiclass SVM Linear classifier with Softmax  Linear classifier with Multiclass SVM In this notebook, we introduce to you
 Notation of a linear classifier Multiclass SVM loss function SGD optimizer  Go though this ipython notebook Linear Classifer with Multiclass SVM you will know</description>
    </item>
    
    <item>
      <title></title>
      <link>https://minh84.github.io/ml-examples/demos/linear_models/01-Linear-Regression/</link>
      <pubDate>Sun, 05 Mar 2017 21:36:46 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/demos/linear_models/01-Linear-Regression/</guid>
      <description>Linear basis function regression&amp;#182;In this note, we consider the simplest form of linear regression models where $h(\pmb{\mathrm{x}})$ is a linear function of the input variables $$ y(\pmb{\mathrm{x}},\pmb{\mathrm{w}}) = w_0 + w_1 x_1 + \ldots + w_D x_D $$ To make our problem more concrete, we consider the boston housing data. Go through this exercise, we will learn the following
 get the raw data and preprocess it to a convenient form visualize the data via plot formulate the maximum likelihood =&amp;gt; least squares problem solve least square problem with Gradient Descend/Stochastic Gradient Descend try it in TensorFlow  Let&#39;s get started</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>https://minh84.github.io/ml-examples/post/mlintro-linear-models/</link>
      <pubDate>Sun, 05 Mar 2017 13:43:22 +0000</pubDate>
      
      <guid>https://minh84.github.io/ml-examples/post/mlintro-linear-models/</guid>
      <description>We start by looking at Supervised Learning with Linear Models inluding following topics
 Linear Regression
 Logistic Regression
  Although Linear Models is a simple approach to supervised learning, it has a nice analytical form and it&amp;rsquo;s easy to interpret its results.
Notation We define some notation to be used later
 $\pmb{\mathrm{x}}^{(i)}\in\mathbb{R}^D,\ i=1,\ldots,N$ called input features $t^{(i)},\ i=1,\ldots,N$ called target variable  Our goal is to train/find a hypothesis function $h$ that allows us to predict a new target variable $t$ given a new input $\pmb{\mathrm{x}}=(x_1,\ldots,x_D)$ i.</description>
    </item>
    
  </channel>
</rss>
